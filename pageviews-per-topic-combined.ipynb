{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ed30ee-ecba-493a-a0e5-4773b70b8581",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bz2\n",
    "import csv\n",
    "import io\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import List, Dict\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import lsde2021.csv as csvutils\n",
    "import lsde2021.utils as utils\n",
    "import lsde2021.download as dl\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45dc79b-e533-4506-b3a6-6dc96e1451c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_MEMORY = \"30G\"\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"parse-wikipedia-sql-dumps\") \\\n",
    "    .config(\"spark.executor.memory\", MAX_MEMORY) \\\n",
    "    .config(\"spark.driver.memory\", MAX_MEMORY) \\\n",
    "    .config('spark.driver.maxResultSize', MAX_MEMORY) \\\n",
    "    .config('spark.ui.showConsoleProgress', 'false') \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "csv_loader = spark.read.format(\"csv\").options(header='True', inferSchema='True')\n",
    "parquet_reader = spark.read.format(\"parquet\").options(inferSchema='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a4b198-3e84-438e-ad81-8314fc7fd070",
   "metadata": {},
   "outputs": [],
   "source": [
    "pageview_complete_processed_src = Path(\"../nvme/pageview_complete_processed\")\n",
    "# pageview_complete_per_topic_dest = Path(\"../nvme/pageview_complete_per_topic\")\n",
    "end_date = datetime.date(2021, 10, 1)\n",
    "\n",
    "daily_pageview_files = []\n",
    "for year in [2019]: # 2019, 2020, 2021]:\n",
    "    daily_range = list(dl.date_range(\n",
    "        datetime.date(year, 1, 1),\n",
    "        datetime.date(year, 12, 31),\n",
    "    ))\n",
    "    \n",
    "    daily_range = [d for d in daily_range if (end_date - d).total_seconds() > 0]\n",
    "    daily_pageview_files += daily_range\n",
    "    \n",
    "daily_pageview_files = [\n",
    "    (\n",
    "        pageview_complete_processed_src / Path(\"/\".join(dl.wikimedia_pageview_complete_local_file(date, monthly=False))).with_suffix(\".parquet\"),\n",
    "        date,\n",
    "        # pageview_complete_per_topic_dest / Path(\"/\".join(dl.wikimedia_pageview_complete_local_file(date, monthly=False))).with_suffix(\".parquet\"),\n",
    "    )\n",
    "    for date in daily_pageview_files\n",
    "]\n",
    "pprint(daily_pageview_files[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0175d7fa-8fe9-4a4a-8035-ff171cc5c1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_cols = [\"topic\", \"dbname\", \"wiki_code\", \"group\", \"language\"]\n",
    "\n",
    "def prepare(daily_processed_file, date):\n",
    "    d = date.timetuple()\n",
    "    day = f\"{d.tm_year}-{d.tm_mon}-{d.tm_mday}\"\n",
    "    # print(day)\n",
    "    \n",
    "    df = spark.read.format(\"parquet\").load(str(daily_processed_file))\n",
    "    \n",
    "    topic1_counts = df.select(*group_cols, \"daily_total\", F.explode(\"topics1\").alias(\"topic\"))\n",
    "    topic1_counts = topic1_counts.groupBy(group_cols).agg(F.sum(\"daily_total\").alias(day))\n",
    "    topic1_counts = topic1_counts.withColumn(\"level\", F.lit(1))\n",
    "    # topic1_counts.limit(10).show()\n",
    "    \n",
    "    topic2_counts = df.select(*group_cols, \"daily_total\", F.explode(\"topics2\").alias(\"topic\"))\n",
    "    topic2_counts = topic2_counts.groupBy(group_cols).agg(F.sum(\"daily_total\").alias(day))\n",
    "    topic2_counts = topic2_counts.withColumn(\"level\", F.lit(2))\n",
    "    \n",
    "    topic3_counts = df.select(*group_cols, \"daily_total\", F.explode(\"topics3\").alias(\"topic\"))\n",
    "    topic3_counts = topic3_counts.groupBy(group_cols).agg(F.sum(\"daily_total\").alias(day))\n",
    "    topic3_counts = topic3_counts.withColumn(\"level\", F.lit(3))\n",
    "    \n",
    "    topic4_counts = df.select(*group_cols, \"daily_total\", F.explode(\"topics4\").alias(\"topic\"))\n",
    "    topic4_counts = topic4_counts.groupBy(group_cols).agg(F.sum(\"daily_total\").alias(day))\n",
    "    topic4_counts = topic4_counts.withColumn(\"level\", F.lit(4))\n",
    "    \n",
    "    topic_counts = topic1_counts\n",
    "    topic_counts = topic_counts.union(topic2_counts)\n",
    "    topic_counts = topic_counts.union(topic3_counts)\n",
    "    topic_counts = topic_counts.union(topic4_counts)\n",
    "    \n",
    "    topic_counts = topic_counts.filter((F.col(\"group\").isNotNull()))\n",
    "    # topic_counts.limit(10).show()\n",
    "    \n",
    "    # test for pizza\n",
    "    # topic_counts.filter(F.lower(\"topic\") == \"pizza\").limit(100).show()\n",
    "    \n",
    "    return topic_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe560eb-e8fa-43ca-a870-1ca5f94dcc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with the first date\n",
    "total_start = time.time()\n",
    "daily_pageview_file, date = daily_pageview_files[0]\n",
    "daily_pageview_combined = prepare(daily_pageview_file, date)\n",
    "\n",
    "# iteratively join other days\n",
    "for daily_pageview_file, date in daily_pageview_files[1:]:\n",
    "    start = time.time()\n",
    "    daily_pageview_combined = daily_pageview_combined.join(prepare(daily_pageview_file, date).repartition(\"group\", \"level\"), on=group_cols + [\"level\"], how=\"outer\")\n",
    "    daily_pageview_combined = daily_pageview_combined.repartition(\"group\", \"level\")\n",
    "    print(\"processed %s (%d rows) in %.2f minutes\" % (daily_pageview_file, daily_pageview_combined.count(), (time.time() - start) / (60)))\n",
    "\n",
    "# 6M * 4 * langs X 365\n",
    "    \n",
    "# daily_pageview_combined.limit(10).show()\n",
    "daily_pageview_combined.write.format(\"parquet\").mode(\"overwrite\").partitionBy(\"group\", \"level\").save(\"../nvme/pageview_complete_per_topic_combined/2019.parquet\")\n",
    "print(\"done in %.2f hours\" % ((time.time() - total_start) / (60**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7fbbc9-c586-4e33-a028-aa0313eb0acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a65e3c-d349-4265-b7b7-4db72383c68b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
