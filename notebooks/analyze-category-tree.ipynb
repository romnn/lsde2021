{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cffa67e-678f-4bc0-99d6-69cd54258145",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import bz2\n",
    "import csv\n",
    "import io\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "import concurrent.futures\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from functools import partial, reduce\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import List, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "import lsde2021.csv as csvutil\n",
    "import lsde2021.utils as utils\n",
    "from lsde2021.lang import singularize, pluralize\n",
    "import lsde2021.download as dl\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50d6c52-0d85-4b72-936d-530a84dc2877",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_MEMORY = \"10G\"\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"parse-wikipedia-sql-dumps\") \\\n",
    "    .config(\"spark.executor.memory\", MAX_MEMORY) \\\n",
    "    .config(\"spark.driver.memory\", MAX_MEMORY) \\\n",
    "    .config('spark.driver.maxResultSize', MAX_MEMORY) \\\n",
    "    .config('spark.ui.showConsoleProgress', 'false') \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "csv_loader = spark.read.format(\"csv\").options(header='True', inferSchema='True')\n",
    "parquet_reader = spark.read.format(\"parquet\").options(inferSchema='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f39ab0-7032-4143-9a8b-998666e831a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join categories with english wiki page table\n",
    "wiki = \"enwiki\"\n",
    "raw_pages = parquet_reader.load(str(f\"../nvme/wikipedia_sql_dumps/{wiki}/20211001/{wiki}-20211001-page-category-count.sql.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f448c1-a9c8-4a1a-bd5b-b78a9e73f240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first have a closer look at some of the categories and how they look like so we can split them eventually\n",
    "example_categories = raw_pages.select(\"category_name\").limit(1_000).rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62445b16-fbaf-42f2-8a89-ad3546fc7f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(example_categories[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61347da9-6b06-4c64-9647-aca4a8138161",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = nx.read_gpickle(f\"../nvme/en-category-tree.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac33d1c-86a3-4e56-b2a1-00dbce59d86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out all hidden categories by removing nodes that have an edge to the hidden category\n",
    "hidden_category = raw_pages \\\n",
    "    .filter((F.col(\"category_name\") == \"Hidden_categories\") & (F.col(\"page_namespace\") == 14))\n",
    "hidden_category.limit(100).show()\n",
    "\n",
    "hidden_category = hidden_category \\\n",
    "    .groupBy(\"category_page_id\") \\\n",
    "    .count()\n",
    "hidden_category.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93882e21-ba20-4972-98f9-9df0a85a45bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_category_node = 15961454\n",
    "\n",
    "hidden_sub_categories = list(nx.bfs_tree(graph, reverse=True, source=hidden_category_node, depth_limit=1))\n",
    "\n",
    "# 30259 for depth_limit=1\n",
    "# 6_838_612 for depth_limit=2\n",
    "print(len(hidden_sub_categories))\n",
    "pprint([graph.nodes[n] for n in hidden_sub_categories[-25:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3772bd-e77b-4973-a5e7-ac006600517f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove hidden topics and their edges from the graph\n",
    "print(\"edges before: %d | nodes before: %d\" % (len(graph.edges), len(graph.nodes)))\n",
    "graph.remove_nodes_from(hidden_sub_categories)\n",
    "print(\"edges after: %d | nodes after: %d\" % (len(graph.edges), len(graph.nodes)))\n",
    "\n",
    "# edges before: 74832061 | nodes before: 7987708\n",
    "# edges after: 38634343 | nodes after: 7957449"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea6b141-baef-446f-afd0-13cb5083f719",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"edges reduced\", 38634343/74832061)\n",
    "print(\"nodes reduced\", 7957449/7987708)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfc1eab-6f7e-4a7d-b8a9-eca6762a9531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the graph for reuse\n",
    "# nx.write_gpickle(graph, f\"../nvme/en-category-tree-without-hidden.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252f723d-adb2-4cc0-a5aa-4e810e0319b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example case: find the COVID 19 wikipedia article\n",
    "covid_article = raw_pages.filter(F.col(\"page_title\") == \"COVID-19\").limit(100)\n",
    "covid_article.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ae395d-4418-40e8-9159-bb9e4bbfba44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the content category\n",
    "root_category = raw_pages.filter((F.col(\"category_name\") == \"Content\") & (F.col(\"page_namespace\") == 14)).limit(100)\n",
    "root_category.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cc9557-7c37-4c98-9ba4-6cef92a1e5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find sinks in the graph (there should only be one)\n",
    "sinks = [node for node in graph.nodes if graph.out_degree(node) == 0 and graph.in_degree(node) > 0]\n",
    "print(len(sinks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fd7c75-2162-450a-a891-bd3d04259d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint([graph.nodes[n][\"title\"] for n in sinks[:20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857df395-aec4-4a11-9aca-72c059c83376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the average node degree of the graph\n",
    "leafs = [node for node in graph.nodes if graph.in_degree(node) < 1]\n",
    "inners = [node for node in graph.nodes if graph.in_degree(node) > 0]\n",
    "\n",
    "def avg_out_degree(nodes):\n",
    "    sum_of_edges = sum([graph.out_degree(node) for node in nodes])\n",
    "    return sum_of_edges / len(nodes)\n",
    "\n",
    "print(\"average node degree of leafs\", avg_out_degree(leafs))\n",
    "print(\"average node degree of inners\", avg_out_degree(inners))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874c2d3a-3e5b-470a-bc01-960fddd83904",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric = re.compile(r'^([\\s\\d]+)$')\n",
    "\n",
    "patterns = [\n",
    "    (re.compile(r\"^\\d+th-century_(\\w+)_in_the_(\\w+)$\"), []),\n",
    "    (re.compile(r\"^\\d+th-century_(\\w+)_in_(\\w+)$\"), []),\n",
    "    \n",
    "    (re.compile(r\"^\\d+s_in_the_(\\w+)$\"), []),\n",
    "    (re.compile(r\"^\\d+s_in_(\\w+)$\"), []),\n",
    "    (re.compile(r\"^\\d+_in_the_(\\w+)$\"), []),\n",
    "    (re.compile(r\"^\\d+_in_(\\w+)$\"), []),\n",
    "    \n",
    "    (re.compile(r\"^(\\w+)_based_in_(\\w+)_by_subject$\"), []),\n",
    "    \n",
    "    (re.compile(r\"^(\\w+)_established_in_the_(\\w+)$\"), []),\n",
    "    (re.compile(r\"^(\\w+)_established_in_(\\w+)$\"), []),\n",
    "    \n",
    "    (re.compile(r\"^(\\w+)_in_the_(\\w+)$\"), []),\n",
    "    (re.compile(r\"^(\\w+)_in_(\\w+)$\"), []),\n",
    "    \n",
    "    (re.compile(r\"^(\\w+)_and_the_(\\w+)$\"), []),\n",
    "    (re.compile(r\"^(\\w+)_and_(\\w+)$\"), []),\n",
    "    \n",
    "    (re.compile(r\"^(\\w+)_of_the_(\\w+)_by_country$\"), []),\n",
    "    (re.compile(r\"^(\\w+)_of_(\\w+)_by_country$\"), []),\n",
    "    (re.compile(r\"^(\\w+)_of_the_(\\w+)$\"), []),\n",
    "    (re.compile(r\"^(\\w+)_of_(\\w+)$\"), []),\n",
    "    \n",
    "    (re.compile(r\"^(\\w+)_by_country$\"), []),\n",
    "    (re.compile(r\"^(\\w+)_by_region$\"), []),\n",
    "    (re.compile(r\"^(\\w+)_by_location$\"), []),\n",
    "    (re.compile(r\"^(\\w+)_by_field$\"), []),\n",
    "    (re.compile(r\"^(\\w+)_by_location$\"), []),\n",
    "    (re.compile(r\"^(\\w+)_by_type$\"), []),\n",
    "    \n",
    "    (re.compile(r\"^\\d+_(\\w+)_by_legal_status$\"), []),\n",
    "    (re.compile(r\"^\\d+_(\\w+)_by_year$\"), []),\n",
    "    (re.compile(r\"^\\d+_(\\w+)_by_date$\"), []),\n",
    "    (re.compile(r\"^\\d+_(\\w+)_by_year_and_country$\"), []),\n",
    "    (re.compile(r\"^\\d+_(\\w+)_by_country_and_year$\"), []),\n",
    "    (re.compile(r\"^\\d+_(\\w+)_by_country$\"), []),\n",
    "    (re.compile(r\"^\\d+_(\\w+)_by_continent$\"), []),\n",
    "    (re.compile(r\"^\\d+_(\\w+)_by_decade$\"), []),\n",
    "    (re.compile(r\"^\\d+_(\\w+)_by_date$\"), []),\n",
    "    (re.compile(r\"^\\d+_(\\w+)_by_(\\w+)$\"), []),\n",
    "    \n",
    "    \n",
    "    (re.compile(r\"^(\\w+)_by_legal_status$\"), []),\n",
    "    (re.compile(r\"^(\\w+)_by_year$\"), []),\n",
    "    (re.compile(r\"^(\\w+)_by_date$\"), []),\n",
    "    (re.compile(r\"^(\\w+)_by_year_and_country$\"), []),\n",
    "    (re.compile(r\"^(\\w+)_by_country_and_year$\"), []),\n",
    "    (re.compile(r\"^(\\w+)_by_country$\"), []),\n",
    "    (re.compile(r\"^(\\w+)_by_continent$\"), []),\n",
    "    (re.compile(r\"^(\\w+)_by_decade$\"), []),\n",
    "    (re.compile(r\"^(\\w+)_by_date$\"), []),\n",
    "    (re.compile(r\"^(\\w+)_by_(\\w+)$\"), []),\n",
    "    \n",
    "    (re.compile(r\"^\\d+_(\\w+)$\"), []),\n",
    "]\n",
    "print(len(patterns))\n",
    "\n",
    "test_str = 'Companies_by_date'\n",
    "for pattern, extra_words in patterns:\n",
    "    match = pattern.fullmatch(test_str)\n",
    "    if match:\n",
    "        print(list(match.groups()))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5203763-0a87-4837-8ab1-0bf4fe22a65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['a', 'about', 'above', 'across', 'after', 'afterwards']\n",
    "stopwords += ['again', 'against', 'all', 'almost', 'alone', 'along']\n",
    "stopwords += ['already', 'also', 'although', 'always', 'am', 'among']\n",
    "stopwords += ['amongst', 'amoungst', 'amount', 'an', 'and', 'another']\n",
    "stopwords += ['any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere']\n",
    "stopwords += ['are', 'around', 'as', 'at', 'back', 'be', 'became']\n",
    "stopwords += ['because', 'become', 'becomes', 'becoming', 'been']\n",
    "stopwords += ['before', 'beforehand', 'behind', 'being', 'below']\n",
    "stopwords += ['beside', 'besides', 'between', 'beyond', 'bill', 'both']\n",
    "stopwords += ['bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant']\n",
    "stopwords += ['co', 'computer', 'con', 'could', 'couldnt', 'cry', 'de']\n",
    "stopwords += ['describe', 'detail', 'did', 'do', 'done', 'down', 'due']\n",
    "stopwords += ['during', 'each', 'eg', 'eight', 'either', 'eleven', 'else']\n",
    "stopwords += ['elsewhere', 'empty', 'enough', 'etc', 'even', 'ever']\n",
    "stopwords += ['every', 'everyone', 'everything', 'everywhere', 'except']\n",
    "stopwords += ['few', 'fifteen', 'fifty', 'fill', 'find', 'fire', 'first']\n",
    "stopwords += ['five', 'for', 'former', 'formerly', 'forty', 'found']\n",
    "stopwords += ['four', 'from', 'front', 'full', 'further', 'get', 'give']\n",
    "stopwords += ['go', 'had', 'has', 'hasnt', 'have', 'he', 'hence', 'her']\n",
    "stopwords += ['here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers']\n",
    "stopwords += ['herself', 'him', 'himself', 'his', 'how', 'however']\n",
    "stopwords += ['hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed']\n",
    "stopwords += ['interest', 'into', 'is', 'it', 'its', 'itself', 'keep']\n",
    "stopwords += ['last', 'latter', 'latterly', 'least', 'less', 'ltd', 'made']\n",
    "stopwords += ['many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine']\n",
    "stopwords += ['more', 'moreover', 'most', 'mostly', 'move', 'much']\n",
    "stopwords += ['must', 'my', 'myself', 'name', 'namely', 'neither', 'never']\n",
    "stopwords += ['nevertheless', 'next', 'nine', 'no', 'nobody', 'none']\n",
    "stopwords += ['noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of']\n",
    "stopwords += ['off', 'often', 'on','once', 'one', 'only', 'onto', 'or']\n",
    "stopwords += ['other', 'others', 'otherwise', 'our', 'ours', 'ourselves']\n",
    "stopwords += ['out', 'over', 'own', 'part', 'per', 'perhaps', 'please']\n",
    "stopwords += ['put', 'rather', 're', 's', 'same', 'see', 'seem', 'seemed']\n",
    "stopwords += ['seeming', 'seems', 'serious', 'several', 'she', 'should']\n",
    "stopwords += ['show', 'side', 'since', 'sincere', 'six', 'sixty', 'so']\n",
    "stopwords += ['some', 'somehow', 'someone', 'something', 'sometime']\n",
    "stopwords += ['sometimes', 'somewhere', 'still', 'such', 'system', 'take']\n",
    "stopwords += ['ten', 'than', 'that', 'the', 'their', 'them', 'themselves']\n",
    "stopwords += ['then', 'thence', 'there', 'thereafter', 'thereby']\n",
    "stopwords += ['therefore', 'therein', 'thereupon', 'these', 'they']\n",
    "stopwords += ['thick', 'thin', 'third', 'this', 'those', 'though', 'three']\n",
    "stopwords += ['three', 'through', 'throughout', 'thru', 'thus', 'to']\n",
    "stopwords += ['together', 'too', 'top', 'toward', 'towards', 'twelve']\n",
    "stopwords += ['twenty', 'two', 'un', 'under', 'until', 'up', 'upon']\n",
    "stopwords += ['us', 'very', 'via', 'was', 'we', 'well', 'were', 'what']\n",
    "stopwords += ['whatever', 'when', 'whence', 'whenever', 'where']\n",
    "stopwords += ['whereafter', 'whereas', 'whereby', 'wherein', 'whereupon']\n",
    "stopwords += ['wherever', 'whether', 'which', 'while', 'whither', 'who']\n",
    "stopwords += ['whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with']\n",
    "stopwords += ['within', 'without', 'would', 'yet', 'you', 'your']\n",
    "stopwords += ['yours', 'yourself', 'yourselves']\n",
    "\n",
    "EXCLUDE = set(stopwords).union({\"by\",\"or\",\"and\",\"with\",\"the\",\"of\",\"in\",\"without\",\"a\",\"on\"})\n",
    "print(len(EXCLUDE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91341a60-e20c-4155-ab58-085902624d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def unique(l, key):\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    return [x for x in l if not (key(x) in seen or seen_add(key(x)))]\n",
    "\n",
    "def union(dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "def is_uppercase(s: str):\n",
    "    return s[0].isupper()\n",
    "\n",
    "def split_by_pattern(s: str) -> List[str]:\n",
    "    for pattern, extra_words in patterns:\n",
    "        match = re.fullmatch(pattern, s)\n",
    "        if match:\n",
    "            return list(match.groups()), True\n",
    "    return [s], False\n",
    "\n",
    "def split(s: str, split_unmatched=False, singularize=False, pluralize=False, recursive=False):\n",
    "    # first, test for common patterns\n",
    "    splitted, matched = split_by_pattern(s)\n",
    "    \n",
    "    # split recursively\n",
    "    rec_splitted = flatten([split_by_pattern(ss)[0] for ss in splitted])\n",
    "    while recursive and set(splitted) != set(rec_splitted):\n",
    "        splitted = rec_splitted[:]\n",
    "        rec_splitted = flatten([split_by_pattern(ss)[0] for ss in splitted])\n",
    "            \n",
    "    if not matched:\n",
    "        if split_unmatched:\n",
    "            # if no pattern is found, split and remove stopwords\n",
    "            splitted += re.split(' |,|_', s)\n",
    "        else:\n",
    "            splitted = [s]\n",
    "    \n",
    "    splitted = set([sp.replace(\"_\", \" \") for sp in splitted if numeric.match(sp) is None])\n",
    "    \n",
    "    if singularize and pluralize:\n",
    "        splitted = set([singularize(sp) for sp in splitted]).union(set([pluralize(sp) for sp in splitted]))\n",
    "    elif singularize:\n",
    "        splitted = set([singularize(sp) for sp in splitted])\n",
    "    elif pluralize:\n",
    "        splitted = set([pluralize(sp) for sp in splitted])\n",
    "    splitted = splitted - EXCLUDE\n",
    "    return splitted\n",
    "\n",
    "def split_all(s: str):\n",
    "    return split(s, split_unmatched=True)\n",
    "\n",
    "def bfs_tree(g, node, depth_limit=None):\n",
    "    ans = []\n",
    "    visited = set()\n",
    "    level = [(node, 0)]\n",
    "    while len(level) > 0:\n",
    "        for v, depth in level:\n",
    "            ans.append((v, depth))\n",
    "            visited.add(v)\n",
    "        next_level = set()\n",
    "        for v, depth in level:\n",
    "            for w in g.neighbors(v):\n",
    "                if w not in visited:\n",
    "                    next_level.add((w, depth + 1))\n",
    "        level = next_level\n",
    "    return ans\n",
    "\n",
    "def freq_bfs_tree(g, node, depth_limit=None):\n",
    "    ans = []\n",
    "    counts = dict()\n",
    "    visited = set()\n",
    "    level = [(node, 0)]\n",
    "    while len(level) > 0:\n",
    "        for v, depth in level:\n",
    "            ans.append((v, depth))\n",
    "            visited.add(v)\n",
    "            counts[v] = 1\n",
    "        next_level = set()\n",
    "        for v, depth in level:\n",
    "            for w in g.neighbors(v):\n",
    "                if w in visited:\n",
    "                    counts[v] += 1\n",
    "                elif depth_limit is None or depth + 1 <= depth_limit:\n",
    "                    next_level.add((w, depth + 1))\n",
    "        level = next_level\n",
    "    \n",
    "    levels = dict()\n",
    "    for n, depth in ans:\n",
    "        if depth not in levels:\n",
    "            levels[depth] = []\n",
    "        levels[depth].append((n, counts[n]))\n",
    "    \n",
    "    levels = {depth: sorted(nodes, key=lambda x: x[1], reverse=True) for depth, nodes in levels.items()}\n",
    "    return levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85e013f-29ff-47e2-a6ff-24f873799116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_topics(node, g, depth_limit: int = 4, max_categories: int = 5) -> Dict[int, List[int]]:\n",
    "    categories = freq_bfs_tree(g, node, depth_limit=depth_limit)\n",
    "    if False:\n",
    "        pprint({\n",
    "            depth: [(g.nodes[n][\"title\"], n, count) for n, count in nodes]\n",
    "            for depth, nodes in categories.items() if depth > 0\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        depth: unique(flatten([\n",
    "            [w.capitalize() for w in split(g.nodes[n][\"title\"], recursive=True)]\n",
    "            for n, count in nodes\n",
    "        ]), key=lambda x: x[0])[:max_categories] for depth, nodes in categories.items() if depth > 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bff8e22-7c0e-486b-8248-93f2d257aa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "depth_limit = 4\n",
    "n_categories = 5\n",
    "page_id = 63030231 # covid 19\n",
    "# page_id = 11867 # germany\n",
    "# page_id = 24365 # porsche\n",
    "\n",
    "pprint(find_topics(page_id, g=graph, depth_limit=4, max_categories=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344d2567-52dd-4ec5-bf10-c5642fe65f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_page_ids =  list(enumerate(sorted(raw_pages.filter(F.col(\"page_namespace\") == 0).select(\"page_id\").distinct().rdd.flatMap(lambda x: x).collect())))\n",
    "print(len(all_page_ids))\n",
    "with open(\"../nvme/en_topics/all_page_ids.pkl\", 'wb') as f:\n",
    "    pkl.dump(all_page_ids, f, protocol=pkl.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
