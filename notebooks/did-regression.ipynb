{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9f3f13-fe27-4e6a-b1cb-c63cc84635ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pip install colour ruptures\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bz2\n",
    "import csv\n",
    "import io\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from matplotlib import ticker\n",
    "from colour import Color\n",
    "import ruptures as rpt\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import List, Dict\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import lsde2021.csv as csvutils\n",
    "import lsde2021.utils as utils\n",
    "import lsde2021.download as dl\n",
    "import lsde2021.changepoints as cp\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6372447b-1db1-467d-8c3e-f3856400812c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_MEMORY = \"30G\"\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"parse-wikipedia-sql-dumps\") \\\n",
    "    .config(\"spark.executor.memory\", MAX_MEMORY) \\\n",
    "    .config(\"spark.driver.memory\", MAX_MEMORY) \\\n",
    "    .config('spark.driver.maxResultSize', MAX_MEMORY) \\\n",
    "    .config('spark.ui.showConsoleProgress', 'false') \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7204bb-4e73-41c2-b3cb-9c4a9f9dbebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stringency = spark.read.format(\"parquet\").load(f\"../nvme/oxcgrt-covid-policy-tracker/OxCGRT_withnotes.parquet\")\n",
    "languages = spark.read.format(\"parquet\").load(f\"./data/languages.parquet\")\n",
    "# languages.show()\n",
    "# stringency.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68f7029-4421-4da9-99f5-b89c40f739c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_groups = list(reversed(languages.select(\"group\").distinct().rdd.flatMap(lambda x: x).collect()))\n",
    "print(language_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ef0b37-d0fb-4707-9435-2b1582f0e7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_stringency, changepoints = cp.get_change_points(stringency, \"Germany\")\n",
    "country_stringency.show()\n",
    "cp.plot_changepoints(\n",
    "    country_stringency.select(\"Date\", \"StringencyIndex\").toPandas().set_index(\"Date\"),\n",
    "    changepoints,\n",
    "    title=\"Changepoints based on stringency index for {Germany}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e69fa32-7bae-4d8c-8832-ea4c74cac5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_stringency(\n",
    "    s,\n",
    "    pageviews,\n",
    "    label=\"Page views\",\n",
    "    xlabel=\"time\",\n",
    "    ylabel1=\"stringency index\",\n",
    "    ylabel2=\"absolute page views\",\n",
    "    title=None,\n",
    "    smoothing = 50,\n",
    "    fontsize=15,\n",
    "    figsize=(12, 7),\n",
    "    savefig=None\n",
    "):\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax2 = ax.twinx()\n",
    "    \n",
    "    ax.tick_params(axis='both', which='major', labelsize=fontsize * 3/4)\n",
    "    ax.tick_params(axis='both', which='minor', labelsize=fontsize * 3/4)\n",
    "    \n",
    "    ax.set_xlabel(xlabel, labelpad=20, size=fontsize)\n",
    "    ax.set_ylabel(ylabel1, labelpad=20, size=fontsize)\n",
    "    ax2.set_ylabel(ylabel2, labelpad=20, size=fontsize)\n",
    "    \n",
    "    assert len(pageviews) > smoothing\n",
    "    \n",
    "    ax.plot(s.index[:len(pageviews)-smoothing], s[\"StringencyIndex\"][:len(pageviews)-smoothing], label=\"Stringency Index\", color=\"gray\", linestyle='--', linewidth=2)\n",
    "    ax2.plot(pageviews.index[:-smoothing], utils.smoothing_window(pageviews[\"page_views\"], radius=smoothing), label=label, color=\"blue\", linestyle='-', linewidth=3)\n",
    "\n",
    "    plt.legend(fontsize=fontsize)\n",
    "    if title:\n",
    "        plt.title(title, fontsize=fontsize, pad=20)\n",
    "    plt.tight_layout()\n",
    "    if savefig:\n",
    "        Path(savefig).parent.mkdir(parents=True, exist_ok=True)\n",
    "        plt.savefig(savefig)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec33467-eb25-4123-826e-6782fe9d7423",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = [\"disambiguation\", \"wikipedia\", \"main topic articles\", \"stubs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a9f5ec-c074-4fde-9687-aadc4f2a1e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def did(year_df, changepoint, control_changepoint, topic_level=4, window_size=relativedelta(days=10), relative=False):\n",
    "    control_start, control_end = control_changepoint - window_size, control_changepoint + window_size\n",
    "    target_start, target_end = changepoint - window_size, changepoint + window_size\n",
    "    \n",
    "    # exlude known bad topics\n",
    "    year_df = year_df.filter(~F.lower(\"topic\").rlike(\"|\".join([\"(\" + pat + \")\" for pat in exclude])))\n",
    "    \n",
    "    pre_target_mean = year_df \\\n",
    "        .filter((F.lit(target_start) <= F.col(\"date\")) & (F.col(\"date\") <= F.lit(changepoint))) \\\n",
    "        .fillna(0) \\\n",
    "        .groupBy(\"topic\", \"dbname\", \"wiki_code\", \"language\") \\\n",
    "        .agg(F.mean(f\"level{topic_level}_daily_total\").alias(\"pre_target_mean\"))\n",
    "\n",
    "    pre_control_mean = year_df \\\n",
    "        .filter((F.lit(control_start) <= F.col(\"date\")) & (F.col(\"date\") <= F.lit(control_changepoint))) \\\n",
    "        .fillna(0) \\\n",
    "        .groupBy(\"topic\", \"dbname\", \"wiki_code\", \"language\") \\\n",
    "        .agg(F.mean(f\"level{topic_level}_daily_total\").alias(\"pre_control_mean\"))\n",
    "\n",
    "    post_target_mean = year_df \\\n",
    "        .filter((F.lit(changepoint) <= F.col(\"date\")) & (F.col(\"date\") <= F.lit(target_end))) \\\n",
    "        .fillna(0) \\\n",
    "        .groupBy(f\"topic\", \"dbname\", \"wiki_code\", \"language\") \\\n",
    "        .agg(F.mean(f\"level{topic_level}_daily_total\").alias(\"post_target_mean\"))\n",
    "\n",
    "    post_control_mean = year_df \\\n",
    "        .filter((F.lit(control_changepoint) <= F.col(\"date\")) & (F.col(\"date\") <= F.lit(control_end))) \\\n",
    "        .fillna(0) \\\n",
    "        .groupBy(f\"topic\", \"dbname\", \"wiki_code\", \"language\") \\\n",
    "        .agg(F.mean(f\"level{level}_daily_total\").alias(\"post_control_mean\")) \\\n",
    "\n",
    "    diff = pre_target_mean.join(pre_control_mean, on=[\"topic\", \"dbname\", \"wiki_code\", \"language\"], how=\"inner\")\n",
    "    diff = diff.join(post_target_mean, on=[\"topic\", \"dbname\", \"wiki_code\", \"language\"], how=\"inner\")\n",
    "    diff = diff.join(post_control_mean, on=[\"topic\", \"dbname\", \"wiki_code\", \"language\"], how=\"inner\")\n",
    "\n",
    "    diff = diff.withColumn('control_diff', ( F.col(\"pre_control_mean\") - F.col(\"post_control_mean\") ) )\n",
    "    diff = diff.withColumn('target_diff', ( F.col(\"post_target_mean\") - (F.col(\"pre_target_mean\") + F.col(\"control_diff\")) ) )\n",
    "    diff = diff.withColumn('rel_diff', ( F.col(\"post_target_mean\") / (F.col(\"pre_target_mean\") + F.col(\"control_diff\")) ) )\n",
    "    \n",
    "    round_digits = 10 if relative else 0\n",
    "    diff_key = \"rel_diff\" if relative else \"target_diff\"\n",
    "    diff = diff.select(\n",
    "        \"topic\",\n",
    "        F.round(diff_key, round_digits).alias(\"diff\"),\n",
    "        F.round(\"pre_target_mean\", round_digits).alias(\"pre_target_mean\"),\n",
    "        F.round(\"pre_control_mean\", round_digits).alias(\"pre_control_mean\"),\n",
    "        F.round(\"post_target_mean\", round_digits).alias(\"post_target_mean\"),\n",
    "        F.round(\"post_control_mean\", round_digits).alias(\"post_control_mean\"),\n",
    "    )\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637b14e1-08c2-4874-9c91-0f102648cd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff(year_df, changepoint, control_changepoint, topic_level=4, window_size=relativedelta(days=10), relative=False):\n",
    "    pre_start, pre_end = changepoint - window_size, changepoint\n",
    "    post_start, post_end = changepoint, changepoint + window_size\n",
    "    \n",
    "    # exlude known bad topics\n",
    "    year_df = year_df.filter(~F.lower(\"topic\").rlike(\"|\".join([\"(\" + pat + \")\" for pat in exclude])))\n",
    "    \n",
    "    pre_mean = year_df \\\n",
    "        .filter((F.lit(pre_start) <= F.col(\"date\")) & (F.col(\"date\") <= F.lit(pre_end))) \\\n",
    "        .fillna(0) \\\n",
    "        .groupBy(\"topic\", \"dbname\", \"wiki_code\", \"language\") \\\n",
    "        .agg(F.mean(f\"level{topic_level}_daily_total\").alias(\"pre_mean\"))\n",
    "\n",
    "    post_mean = year_df \\\n",
    "        .filter((F.lit(post_start) <= F.col(\"date\")) & (F.col(\"date\") <= F.lit(post_end))) \\\n",
    "        .fillna(0) \\\n",
    "        .groupBy(\"topic\", \"dbname\", \"wiki_code\", \"language\") \\\n",
    "        .agg(F.mean(f\"level{topic_level}_daily_total\").alias(\"post_mean\"))\n",
    "\n",
    "    diff = pre_mean.join(post_mean, on=[\"topic\", \"dbname\", \"wiki_code\", \"language\"], how=\"inner\")\n",
    "    \n",
    "    diff = diff.withColumn('rel_diff', ( F.col(\"post_mean\") / F.col(\"pre_mean\") ) )\n",
    "    diff = diff.withColumn('abs_diff', ( F.col(\"post_mean\") - F.col(\"pre_mean\") ) )\n",
    "    \n",
    "    round_digits = 10 if relative else 0\n",
    "    diff_key = \"rel_diff\" if relative else \"abs_diff\"\n",
    "    diff = diff.select(\n",
    "        \"topic\",\n",
    "        F.round(diff_key, round_digits).alias(\"diff\"),\n",
    "        F.round(\"rel_diff\", round_digits).alias(\"rel_diff\"),\n",
    "        F.round(\"abs_diff\", round_digits).alias(\"abs_diff\"),\n",
    "    )\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1e7987-ae3c-46f4-9113-6ea3dcbbfccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_topics(topics, fontsize=17, figsize=(12, 7), relative=False, risers=True, title=None, savefig=None):\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    \n",
    "    (pre_start, pre_end) = (Color(\"blue\"), Color(\"purple\"))\n",
    "    (post_start, post_end) = (Color(\"red\"), Color(\"orange\"))\n",
    "\n",
    "    y_pos = np.arange(len(topics))\n",
    "    \n",
    "    colors = [c.hex for c in pre_start.range_to(pre_end, len(topics))]\n",
    "    if risers:\n",
    "        colors = [c.hex for c in post_start.range_to(post_end,len(topics))]\n",
    "\n",
    "    ax.barh(0 + 2 * y_pos, topics[\"diff\"], color=colors, align='center')\n",
    "\n",
    "    ax.tick_params(axis=\"both\", labelsize=fontsize, which=\"both\", bottom=\"off\", top=\"off\", labelbottom=\"on\", left=\"off\", right=\"off\", labelleft=\"on\")\n",
    "\n",
    "    vals = ax.get_xticks()\n",
    "    for tick in vals:\n",
    "        ax.axvline(x=tick, linestyle='dashed', alpha=0.4, color='#eeeeee', zorder=1)\n",
    "\n",
    "    ax.set_xlabel(\"%s difference page views\" % (\"Relative\" if relative else \"Absolute\"), labelpad=20, size=fontsize)\n",
    "\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    ax.yaxis.set_major_locator(ticker.FixedLocator(0.0 + 2 * y_pos))\n",
    "    ax.yaxis.set_major_formatter(ticker.FixedFormatter(topics[\"topic\"]))\n",
    "    ax.set_yticklabels(topics[\"topic\"])\n",
    "    \n",
    "    if title:\n",
    "        plt.title(title, fontsize=fontsize, pad=20)\n",
    "    plt.tight_layout()\n",
    "    if savefig:\n",
    "        Path(savefig).parent.mkdir(parents=True, exist_ok=True)\n",
    "        plt.savefig(savefig)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185ed393-fb3f-4b14-a222-3a79e26f5947",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 15\n",
    "level = 4\n",
    "relative = False\n",
    "\n",
    "def to_date2(d):\n",
    "    d = d.timetuple()\n",
    "    return f\"{d.tm_year}-{d.tm_mon}-{d.tm_mday}\"\n",
    "\n",
    "def to_date(d):\n",
    "    d = d.timetuple()\n",
    "    return f\"{d.tm_mday}/{d.tm_mon}/{d.tm_year}\"\n",
    "\n",
    "for country_group in language_groups:\n",
    "    for country in cp.COUNTRIES[country_group]:\n",
    "        try:\n",
    "            country_stringency, changepoints = cp.get_change_points(stringency, country)\n",
    "        except rpt.exceptions.BadSegmentationParameters as e:\n",
    "            print(e)\n",
    "            changepoints = []\n",
    "            \n",
    "        # include set of known global changepoints\n",
    "        changepoints = [\n",
    "            datetime.date(2020, 1, 11) # first death in china\n",
    "        ] + changepoints\n",
    "        \n",
    "        print(f\"found {len(changepoints)} changepoints for {country} ({country_group})\")\n",
    "        \n",
    "        country_desc = dict(\n",
    "            group=country_group,\n",
    "            name=country_stringency.select(\"CountryName\").first().CountryName,\n",
    "            code=country_stringency.select(\"CountryCode\").first().CountryCode,\n",
    "        )\n",
    "        \n",
    "        stringency_dataset = dict(\n",
    "            country = country_desc,\n",
    "            stringency = [json.loads(s) for s in country_stringency.select(\"Date\", \"StringencyIndex\", \"Notes\").toJSON().collect()],\n",
    "            changepoints = changepoints,\n",
    "        )\n",
    "        \n",
    "        out_path = Path(\"./website/public/data\")\n",
    "        out_src_path = Path(\"./website/src/data\")\n",
    "        \n",
    "        stringency_out_path = out_path / country_group.lower() / country.lower() / \"stringency_changepoints.json\"\n",
    "        stringency_out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        with open(stringency_out_path, \"w\") as f:\n",
    "            json.dump(stringency_dataset, f, indent=2, sort_keys=True, default=str)\n",
    "        \n",
    "        cp.plot_changepoints(\n",
    "            country_stringency.select(\"Date\", \"StringencyIndex\").toPandas().set_index(\"Date\"),\n",
    "            changepoints,\n",
    "            title=f\"Change-points based on stringency index for {country}\",\n",
    "            savefig=f\"./figs/results/{country}/changepoints.pdf\")\n",
    "        plt.show()\n",
    "\n",
    "        country_stringency_2020 = country_stringency.filter(\n",
    "            (F.lit(datetime.date(2020,1,1)) <= F.col(\"Date\"))\n",
    "            & (F.col(\"Date\") <= F.lit(datetime.date(2021,1,1)))\n",
    "        ).toPandas().set_index(\"Date\")\n",
    "\n",
    "        print(f\"collecting samples for {country}\")\n",
    "        year_df = spark.read.format(\"parquet\").load(f\"../nvme/country_pageviews/pageviews_{country_group}.parquet\")\n",
    "        year_df = year_df.filter(F.col(f\"level{level}_daily_total\").isNotNull())\n",
    "\n",
    "        total_traffic = year_df \\\n",
    "            .select(\"date\", f\"level{level}_daily_total\") \\\n",
    "            .groupBy(\"date\") \\\n",
    "            .agg(F.sum(f\"level{level}_daily_total\").alias(\"page_views\")) \\\n",
    "            .sort(F.col(\"date\").asc())\n",
    "        \n",
    "        plot_with_stringency(\n",
    "            country_stringency.toPandas().set_index(\"Date\"),\n",
    "            total_traffic.toPandas().set_index(\"date\"),\n",
    "            label=\"Total\",\n",
    "            title=f\"Total page views for {country}\",\n",
    "            savefig=f\"./figs/results/{country}/total_pageviews.pdf\"\n",
    "        )\n",
    "        plt.show()\n",
    "        \n",
    "        total_traffic_dataset = dict(\n",
    "            country = country_desc,\n",
    "            traffic = [json.loads(s) for s in total_traffic.select(\"date\", F.col(\"page_views\").alias(\"views\")).toJSON().collect()],\n",
    "        )\n",
    "        total_traffic_out_path = out_path / country_group.lower() / country.lower() / \"total.json\"\n",
    "        total_traffic_out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        with open(total_traffic_out_path, \"w\") as f:\n",
    "            json.dump(total_traffic_dataset, f, indent=2, sort_keys=True, default=str)\n",
    "        \n",
    "        selected_topics = [\"medicine\", \"music\", \"entertainment\", \"sports\", \"television\", \"gaming\", \"culture\", \"party\", \"science\", \"books\"]\n",
    "        selected_topics_out_path = out_src_path / \"selected_topics.json\"\n",
    "        selected_topics_out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        with open(selected_topics_out_path, \"w\") as f:\n",
    "            json.dump(selected_topics, f, indent=2, sort_keys=True, default=str)\n",
    "        \n",
    "        \n",
    "        for topic in selected_topics:\n",
    "            # absolute\n",
    "            topic_page_views = year_df \\\n",
    "                .filter(F.lower(F.col(\"topic\")) == topic.lower()) \\\n",
    "                .select(\"date\", f\"level{level}_daily_total\") \\\n",
    "                .groupBy(\"date\") \\\n",
    "                .agg(F.sum(f\"level{level}_daily_total\").alias(\"page_views\"))\n",
    "            \n",
    "            try:\n",
    "                plot_with_stringency(\n",
    "                    country_stringency_2020,\n",
    "                    # topic_page_views.filter((F.lit(datetime.date(2020,1,1)) <= F.col(\"date\"))).sort(F.col(\"date\").asc()).toPandas().set_index(\"date\"),\n",
    "                    topic_page_views.sort(F.col(\"date\").asc()).toPandas().set_index(\"date\"),\n",
    "                    label=f\"{topic.capitalize()}\",\n",
    "                    title=f\"Page views for topic \\\"{topic.capitalize()}\\\" in {country}\",\n",
    "                    savefig=f\"./figs/results/{country}/topics/{country}_{topic}.pdf\"\n",
    "                )\n",
    "                plt.show()\n",
    "                \n",
    "                topic_dataset = dict(\n",
    "                    country = country_desc,\n",
    "                    traffic = [json.loads(s) for s in topic_page_views.select(\"date\", F.col(\"page_views\").alias(\"views\")).toJSON().collect()],\n",
    "                )\n",
    "                topic_out_path = out_path / country_group.lower() / country.lower() / \"topics\" / f\"{topic.lower()}.json\"\n",
    "                topic_out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                with open(topic_out_path, \"w\") as f:\n",
    "                    json.dump(topic_dataset, f, indent=2, sort_keys=True, default=str)\n",
    "                \n",
    "            except AssertionError:\n",
    "                print(\"topic\", topic, \"failed\")\n",
    "            \n",
    "            # normalized (relative)\n",
    "            topic_page_views_normalized = topic_page_views.join(\n",
    "                    total_traffic.select(\"date\", F.col(\"page_views\").alias(\"total_page_views\")),\n",
    "                    on=\"date\",\n",
    "                    how=\"inner\"\n",
    "                ) \\\n",
    "                .withColumn(\"normalized_page_views\", F.col(\"page_views\") / F.col(\"total_page_views\")) \\\n",
    "                .select(\"date\", F.col(\"normalized_page_views\").alias(\"page_views\"))\n",
    "            \n",
    "            try:\n",
    "                plot_with_stringency(\n",
    "                    country_stringency_2020,\n",
    "                    # topic_page_views.filter((F.lit(datetime.date(2020,1,1)) <= F.col(\"date\"))).sort(F.col(\"date\").asc()).toPandas().set_index(\"date\"),\n",
    "                    topic_page_views_normalized.sort(F.col(\"date\").asc()).toPandas().set_index(\"date\"),\n",
    "                    label=f\"{topic.capitalize()}\",\n",
    "                    ylabel2=\"page views / total page views\",\n",
    "                    title=f\"Normalized page views for topic \\\"{topic.capitalize()}\\\" in {country}\",\n",
    "                    savefig=f\"./figs/results/{country}/topics/{country}_{topic}_normalized.pdf\"\n",
    "                )\n",
    "                plt.show()\n",
    "                \n",
    "                topic_normalized_dataset = dict(\n",
    "                    country = country_desc,\n",
    "                    traffic = [json.loads(s) for s in topic_page_views_normalized.select(\"date\", F.col(\"page_views\").alias(\"views\")).toJSON().collect()],\n",
    "                )\n",
    "                topic_normalized_out_path = out_path / country_group.lower() / country.lower() / \"topics\" / f\"{topic.lower()}_normalized.json\"\n",
    "                topic_normalized_out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                with open(topic_normalized_out_path, \"w\") as f:\n",
    "                    json.dump(topic_normalized_dataset, f, indent=2, sort_keys=True, default=str)\n",
    "            \n",
    "            except AssertionError:\n",
    "                print(\"topic\", topic, \"failed\")\n",
    "\n",
    "        # for each changepoint, get risers and decreasers\n",
    "        for changepoint in changepoints:\n",
    "            for (method_name, method, method_desc) in [(\"did\", did, \" using DiD regression\"), (\"diff\", diff, \" using absolute difference\")]:\n",
    "                start = time.time()\n",
    "                control_changepoint = changepoint\n",
    "                day_diff = 0\n",
    "                while True:\n",
    "                    try:\n",
    "                        control_changepoint = changepoint.replace(year=2019, day=changepoint.day - day_diff)\n",
    "                        break\n",
    "                    except ValueError:\n",
    "                        day_diff += 1\n",
    "                diff_df = method(\n",
    "                    year_df,\n",
    "                    changepoint=changepoint,\n",
    "                    control_changepoint=control_changepoint,\n",
    "                    topic_level=level,\n",
    "                    window_size=relativedelta(days=window_size),\n",
    "                    relative=relative,\n",
    "                )\n",
    "                \n",
    "                increased = diff_df.select(\"topic\", \"diff\").sort(F.col(\"diff\").desc()).limit(20)\n",
    "                \n",
    "                increased_dataset = dict(\n",
    "                    country = country_desc,\n",
    "                    changepoint = changepoint,\n",
    "                    topics = [json.loads(i) for i in increased.toJSON().collect()],\n",
    "                )\n",
    "                increased_out_path = out_path / country_group.lower() / country.lower() / \"changepoints\" / f\"{method_name}_{to_date2(changepoint)}_increased.json\"\n",
    "                increased_out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                with open(increased_out_path, \"w\") as f:\n",
    "                    json.dump(increased_dataset, f, indent=2, sort_keys=True, default=str)\n",
    "                    \n",
    "                \n",
    "                decreased = diff_df.select(\"topic\", \"diff\").sort(F.col(\"diff\").asc()).limit(20)\n",
    "                \n",
    "                decreased_dataset = dict(\n",
    "                    country = country_desc,\n",
    "                    changepoint = changepoint,\n",
    "                    topics = [json.loads(i) for i in decreased.toJSON().collect()],\n",
    "                )\n",
    "                decreased_out_path = out_path / country_group.lower() / country.lower() / \"changepoints\" / f\"{method_name}_{to_date2(changepoint)}_decreased.json\"\n",
    "                decreased_out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                with open(decreased_out_path, \"w\") as f:\n",
    "                    json.dump(decreased_dataset, f, indent=2, sort_keys=True, default=str)\n",
    "                \n",
    "                print(\"took %.2f seconds\" % (time.time() - start))\n",
    "                \n",
    "                plot_topics(\n",
    "                    increased.limit(10).toPandas(),\n",
    "                    risers=True,\n",
    "                    relative=relative,\n",
    "                    savefig=f\"./figs/results/{country}/attention_shifts/{method_name}_{to_date2(changepoint)}_increase.pdf\",\n",
    "                    title=\"Changepoint %s ($\\pm$ %d days)%s\" % (to_date(changepoint), window_size, method_desc)\n",
    "                )\n",
    "                plot_topics(\n",
    "                    decreased.limit(10).toPandas(),\n",
    "                    risers=False,\n",
    "                    relative=relative,\n",
    "                    savefig=f\"./figs/results/{country}/attention_shifts/{method_name}_{to_date2(changepoint)}_decrease.pdf\",\n",
    "                    title=\"Changepoint %s ($\\pm$ %d days)%s\" % (to_date(changepoint), window_size, method_desc)\n",
    "                )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
