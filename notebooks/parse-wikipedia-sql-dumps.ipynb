{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5e97f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import traceback\n",
    "import pyspark\n",
    "import datetime\n",
    "import csv\n",
    "import time\n",
    "import gzip\n",
    "import gc\n",
    "import sys\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Optional, List\n",
    "import lsde2021.utils as utils\n",
    "import lsde2021.csv as csvutils\n",
    "import lsde2021.download as dl\n",
    "import lsde2021.aggregate as agg\n",
    "from lsde2021.types import PathLike\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, IntegerType\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821e74a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_MEMORY = \"60G\"\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"parse-wikipedia-sql-dumps\") \\\n",
    "    .config(\"spark.executor.memory\", MAX_MEMORY) \\\n",
    "    .config(\"spark.driver.memory\", MAX_MEMORY) \\\n",
    "    .config('spark.driver.maxResultSize', MAX_MEMORY) \\\n",
    "    .config('spark.ui.showConsoleProgress', 'false') \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "csv_loader = spark.read.format(\"csv\").options(header='True', inferSchema='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b41e81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_wikipedia_sql_dump(\n",
    "    sql_input_path: PathLike,\n",
    "    columns: List[str],\n",
    "    dest: Optional[PathLike] = None,\n",
    "    encoding: str = \"utf-8\",\n",
    "    nrows: Optional[int] = None,\n",
    "    log: bool = False,\n",
    "    write: bool = True,\n",
    "    force: bool = False\n",
    ") -> PathLike:\n",
    "    start = time.time()\n",
    "    csv_config = dict(\n",
    "        delimiter=',',\n",
    "        doublequote=False,\n",
    "        escapechar='\\\\',\n",
    "        quotechar=\"'\",\n",
    "        strict=True,\n",
    "    )\n",
    "\n",
    "    input_path = Path(sql_input_path)\n",
    "    output_path = input_path.with_suffix(\".csv\")\n",
    "    if dest is not None:\n",
    "        output_path = dest / output_path.name\n",
    "    \n",
    "    if not force and output_path.exists():\n",
    "        print(f\"using existing {output_path} ...\")\n",
    "        return output_path\n",
    "\n",
    "    with utils.fopen(input_path, mode=\"rt\", encoding=encoding, errors=\"ignore\") as input_file:\n",
    "        output_file = None\n",
    "        if write:\n",
    "            output_file = open(output_path, mode=\"w\", encoding=encoding)\n",
    "        entries = 0\n",
    "        try:\n",
    "            writer = None\n",
    "            if output_file:\n",
    "                writer = csv.writer(output_file, quoting=csv.QUOTE_MINIMAL)\n",
    "            if writer:\n",
    "                # write the header\n",
    "                writer.writerow(columns)\n",
    "            \n",
    "            for line in input_file.readlines():\n",
    "                # Look for INSERT statement and parse it\n",
    "                if line.startswith('INSERT INTO'):\n",
    "                    values = line.partition('` VALUES ')[2]\n",
    "                    assert values\n",
    "                    assert values[0] == '('\n",
    "                    reader = csv.reader([values], **csv_config)\n",
    "                    for ridx, row in enumerate(reader):\n",
    "                        for cidx in range(0, len(row), len(columns)):\n",
    "                            entry = \",\".join(row[cidx:cidx+len(columns)])[1:-1]\n",
    "                            try:\n",
    "                                entry = tuple(*csv.reader([entry],  delimiter=','))\n",
    "                                if log:\n",
    "                                    pprint(entry)\n",
    "                                    sys.stdout.flush()\n",
    "                                if writer:\n",
    "                                    writer.writerow(entry)\n",
    "                                entries += 1\n",
    "                            except Exception as e:\n",
    "                                print(\"entry:\", entry)\n",
    "                                print(\"context:\", row[cidx:cidx+10])\n",
    "                                raise e\n",
    "                            if nrows is not None and entries >= nrows:\n",
    "                                return output_path\n",
    "                        gc.collect()\n",
    "        finally:\n",
    "            if output_file:\n",
    "                output_file.close()\n",
    "                print(f\"wrote {entries} rows to {output_path} in {time.time() - start:.2f} seconds ...\")\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ff230d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with utils.fopen(\"../hdd/wikipedia_sql_dumps/dewiki/20211001/dewiki-20211001-categorylinks.sql.gz\") as f:\n",
    "    print(str(f.read(100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27b9fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(utils.detect_encoding(\n",
    "    \"../hdd/wikipedia_sql_dumps/dewiki/20211001/dewiki-20211001-categorylinks.sql.gz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2d1b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = \"enwiki\"\n",
    "for table, cols in [\n",
    "    (\"page\", csvutils.PAGE_COLUMNS),\n",
    "    (\"categorylinks\", csvutils.CATEGORYLINKS_COLUMNS)\n",
    "]:\n",
    "    parse_wikipedia_sql_dump(\n",
    "        f\"../hdd/wikipedia_sql_dumps/{wiki}/20211001/{wiki}-20211001-{table}.sql.gz\",\n",
    "        columns=cols,\n",
    "        log=True,\n",
    "        nrows=10,\n",
    "        force=True,\n",
    "        write=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a010f830",
   "metadata": {},
   "outputs": [],
   "source": [
    "dest = Path(\"../hdd/wikipedia_sql_dumps\")\n",
    "tables = [\"langlinks\", \"page\", \"category\", \"categorylinks\"]\n",
    "\n",
    "languages = pd.read_csv(\"./data/languages.csv\", index_col=\"code\")\n",
    "languages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594618a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "downloads = list([\n",
    "    (table, dest / \"/\".join(dl.wikimedia_sql_dump_local_file(date, wiki=wiki, table=table)))\n",
    "    for (date, wiki, table), _ in dl.wikimedia_sql_dump_urls(\n",
    "        [datetime.date(2021, 10, 1)], wikis=languages[\"dbname\"], tables=tables\n",
    "    )\n",
    "])\n",
    "pprint(downloads[:3])\n",
    "\n",
    "print(\"downloaded %d of %d\" % (\n",
    "    len(list(dest.rglob(\"**/*sql.gz\"))), len(downloads)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb87a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_csv_columns = dict(\n",
    "    page=csvutils.PAGE_COLUMNS,\n",
    "    category=csvutils.CATEGORY_COLUMNS,\n",
    "    categorylinks=csvutils.CATEGORYLINKS_COLUMNS,\n",
    "    langlinks=csvutils.LANGLINKS_COLUMNS,\n",
    ")\n",
    "\n",
    "def parse_wikipedia_sql_dump_handler(item: Tuple[str, PathLike], force: bool = False) -> PathLike:\n",
    "    table, sql_dump_file = item\n",
    "    csv_columns = table_csv_columns[table]\n",
    "    return parse_wikipedia_sql_dump(\n",
    "        sql_dump_file,\n",
    "        columns=csv_columns,\n",
    "        force=force,\n",
    "    )\n",
    "\n",
    "parsed = sc.parallelize(downloads, numSlices=multiprocessing.cpu_count()).map(\n",
    "    partial(\n",
    "        parse_wikipedia_sql_dump_handler,\n",
    "        force=False,\n",
    "    )\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448469f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the parsed files are sane\n",
    "wiki = \"enwiki\"\n",
    "pages = csv_loader.load(\n",
    "    f\"../hdd/wikipedia_sql_dumps/{wiki}/20211001/{wiki}-20211001-page.sql.csv\"\n",
    ").limit(10_000)\n",
    "pages.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ceff19",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = csv_loader.load(\n",
    "    f\"../hdd/wikipedia_sql_dumps/{wiki}/20211001/{wiki}-20211001-category.sql.csv\"\n",
    ").limit(10_000)\n",
    "print(categories.count(), \"rows\")\n",
    "categories.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f8e245",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorylinks = csv_loader.load(\n",
    "    f\"../hdd/wikipedia_sql_dumps/{wiki}/20211001/{wiki}-20211001-categorylinks.sql.csv\"\n",
    ").limit(10_000)\n",
    "categorylinks.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6398693b",
   "metadata": {},
   "outputs": [],
   "source": [
    "langlinks = csv_loader.load(\n",
    "    f\"../hdd/wikipedia_sql_dumps/{wiki}/20211001/{wiki}-20211001-langlinks.sql.csv\"\n",
    ").limit(10_000)\n",
    "langlinks.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f124e5-1168-4252-8f92-abdb8ff55d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Parse CSV files and convert to correct data dypes using chunking\n",
    "wikis = languages[\"dbname\"]\n",
    "wikis = [w for w in wikis if w != \"enwiki\"]\n",
    "assert \"enwiki\" not in wikis\n",
    "parsed = list([\n",
    "    (table, (dest / \"/\".join(dl.wikimedia_sql_dump_local_file(date, wiki=wiki, table=table))).with_suffix(\".csv\"))\n",
    "    for (date, wiki, table), _ in dl.wikimedia_sql_dump_urls(\n",
    "        [datetime.date(2021, 10, 1)], wikis=wikis, tables=tables\n",
    "    )\n",
    "])\n",
    "pprint(parsed[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc210930-c614-4090-af2e-5033f9c6d2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_to_int(val):\n",
    "    try:\n",
    "        return int(val)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "    \n",
    "def try_to_float(val):\n",
    "    try:\n",
    "        return float(val)\n",
    "    except ValueError:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4817708-615a-429b-82fd-ac552f93d0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_page_sql_dump(parsed_dump_file: PathLike, output_path: PathLike, chunksize: int = 10 ** 4, max_chunks: Optional[int] = None) -> pd.DataFrame:\n",
    "    header = True\n",
    "    with csvutils.read_page_csv(\n",
    "        parsed_dump_file,\n",
    "        engine=\"c\",\n",
    "        dtype=csvutils.RAW_PAGE_DTYPE,\n",
    "        chunksize=chunksize,\n",
    "    ) as reader:\n",
    "        for n, chunk in enumerate(reader):\n",
    "            # print(\"before\")\n",
    "            # print(chunk.head())\n",
    "            chunk[\"page_id\"] = pd.to_numeric(chunk[\"page_id\"], errors='coerce', downcast=\"unsigned\")\n",
    "            chunk[\"page_namespace\"] = pd.to_numeric(chunk[\"page_namespace\"], errors='coerce', downcast=\"unsigned\")\n",
    "            chunk[\"page_title\"] = chunk[\"page_title\"].astype(\"string\")\n",
    "            chunk[\"page_restrictions\"] = chunk[\"page_restrictions\"].astype(\"string\")\n",
    "\n",
    "            chunk[\"page_is_redirect\"] = chunk[\"page_is_redirect\"].astype(\"bool\")\n",
    "            chunk[\"page_is_new\"] = chunk[\"page_is_new\"].astype(\"bool\")\n",
    "\n",
    "            chunk[\"page_random\"] = pd.to_numeric(chunk[\"page_random\"], errors='coerce', downcast=\"unsigned\")\n",
    "            chunk[\"page_touched\"] = pd.to_datetime(chunk[\"page_touched\"], errors='coerce')\n",
    "            chunk[\"page_links_updated\"] = pd.to_datetime(chunk[\"page_links_updated\"], errors='coerce')\n",
    "\n",
    "            chunk[\"page_len\"] = pd.to_numeric(chunk[\"page_len\"], errors='coerce', downcast=\"unsigned\")\n",
    "            chunk[\"page_content_model\"] = chunk[\"page_content_model\"].astype(\"category\")\n",
    "            chunk[\"page_lang\"] = chunk[\"page_lang\"].astype(\"category\")\n",
    "            # print(\"after\")\n",
    "            # print(chunk.head())\n",
    "            \n",
    "            chunk.to_csv(output_path, header=header, mode='a')\n",
    "            header = False\n",
    "            if max_chunks is not None and n >= max_chunks:\n",
    "                break\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c907074-4b1c-4a7a-96b2-879c288ba23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_categorylinks_sql_dump(parsed_dump_file: PathLike, output_path: PathLike, chunksize: int = 10 ** 2, max_chunks: Optional[int] = None) -> pd.DataFrame:\n",
    "    header = True\n",
    "    with csvutils.read_categorylinks_csv(\n",
    "        parsed_dump_file,\n",
    "        engine=\"c\",\n",
    "        dtype=csvutils.RAW_CATEGORYLINKS_DTYPE,\n",
    "        chunksize=chunksize,\n",
    "    ) as reader:\n",
    "        for n, chunk in enumerate(reader):\n",
    "            # print(\"before\")\n",
    "            # print(chunk.head())\n",
    "            chunk[\"page_id\"] = pd.to_numeric(chunk[\"page_id\"].astype(\"string\"), errors='coerce', downcast=\"unsigned\")\n",
    "            chunk[\"category_name\"] = chunk[\"category_name\"].astype(\"string\")\n",
    "            chunk[\"sortkey\"] = chunk[\"sortkey\"].astype(\"category\")\n",
    "            chunk[\"timestamp\"] = pd.to_datetime(chunk[\"timestamp\"], errors='coerce')\n",
    "            chunk[\"sortkey_prefix\"] = chunk[\"sortkey_prefix\"].astype(\"category\")\n",
    "            chunk[\"collation\"] = chunk[\"collation\"].astype(\"category\")\n",
    "            chunk[\"type\"] = chunk[\"type\"].astype(\"category\")\n",
    "            # print(\"after\")\n",
    "            # print(chunk.head())\n",
    "            chunk.to_csv(output_path, header=header, mode='a')\n",
    "            header = False\n",
    "            if max_chunks is not None and n >= max_chunks:\n",
    "                break\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a28609-0db6-4005-9bdf-cee0705d54b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_category_sql_dump(parsed_dump_file: PathLike, output_path: PathLike, chunksize: int = 10 ** 4, max_chunks: Optional[int] = None) -> pd.DataFrame:\n",
    "    header = True\n",
    "    with csvutils.read_category_csv(\n",
    "        parsed_dump_file,\n",
    "        engine=\"c\",\n",
    "        dtype=csvutils.RAW_CATEGORY_DTYPE,\n",
    "        chunksize=chunksize,\n",
    "    ) as reader:\n",
    "        for n, chunk in enumerate(reader):\n",
    "            # print(\"before\")\n",
    "            # print(chunk.head())\n",
    "            chunk[\"cat_id\"] = pd.to_numeric(chunk[\"cat_id\"], errors='coerce', downcast=\"unsigned\")\n",
    "            chunk[\"cat_title\"] = chunk[\"cat_title\"].astype(\"string\")\n",
    "            chunk[\"cat_pages\"] = pd.to_numeric(chunk[\"cat_pages\"], errors='coerce', downcast=\"unsigned\")\n",
    "            chunk[\"cat_subcats\"] = pd.to_numeric(chunk[\"cat_subcats\"], errors='coerce', downcast=\"unsigned\")\n",
    "            chunk[\"cat_files\"] = pd.to_numeric(chunk[\"cat_files\"], errors='coerce', downcast=\"unsigned\")\n",
    "            # print(\"after\")\n",
    "            # print(chunk.head())\n",
    "            \n",
    "            chunk.to_csv(output_path, header=header, mode='a')\n",
    "            header = False\n",
    "            if max_chunks is not None and n >= max_chunks:\n",
    "                break\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17f1013-6084-498b-9d17-7a51f768668b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_langlinks_sql_dump(parsed_dump_file: PathLike, output_path: PathLike, chunksize: int = 10 ** 4, max_chunks: Optional[int] = None) -> pd.DataFrame:\n",
    "    header = True\n",
    "    with csvutils.read_langlinks_csv(\n",
    "        parsed_dump_file,\n",
    "        engine=\"c\",\n",
    "        dtype=csvutils.RAW_LANGLINKS_DTYPE,\n",
    "        chunksize=chunksize,\n",
    "    ) as reader:\n",
    "        for n, chunk in enumerate(reader):\n",
    "            # print(\"before\")\n",
    "            # print(chunk.head())\n",
    "            chunk[\"page_id\"] = pd.to_numeric(chunk[\"page_id\"], errors='coerce', downcast=\"unsigned\")\n",
    "            chunk[\"lang\"] = chunk[\"lang\"].astype(\"category\")\n",
    "            chunk[\"lang_title\"] = chunk[\"lang_title\"].astype(\"string\")\n",
    "            # print(\"after\")\n",
    "            # print(chunk.head())\n",
    "            \n",
    "            chunk.to_csv(output_path, header=header, mode='a')\n",
    "            header = False\n",
    "            if max_chunks is not None and n >= max_chunks:\n",
    "                break\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ce2eb4-fc7f-4b5c-8e78-d55178fc5567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_wikipedia_sql_dump_handler(item: Tuple[str, PathLike], force: bool = False, max_chunks: Optional[int] = None) -> PathLike:\n",
    "    table, parsed_sql_dump_file = item\n",
    "    output_path = utils.strip_extension(parsed_sql_dump_file)\n",
    "    output_path = (output_path.parent / (output_path.stem + \"-converted\")).with_suffix(\".sql.csv\")\n",
    "    print(table, parsed_sql_dump_file, output_path)\n",
    "    \n",
    "    if not force and output_path.exists():\n",
    "        print(f\"using existing {output_path} ...\")\n",
    "        return output_path\n",
    "    \n",
    "    # since we parse in chunks and append to the file, we have to remove it first\n",
    "    output_path.unlink(missing_ok=True)\n",
    "    print(f\"creating {output_path} ...\")\n",
    "    \n",
    "    start = time.time()\n",
    "    if table == \"page\":\n",
    "        prepare_page_sql_dump(parsed_sql_dump_file, output_path, max_chunks=max_chunks)\n",
    "    if table == \"categorylinks\":\n",
    "        prepare_categorylinks_sql_dump(parsed_sql_dump_file, output_path, max_chunks=max_chunks)\n",
    "    if table == \"category\":\n",
    "        prepare_category_sql_dump(parsed_sql_dump_file, output_path, max_chunks=max_chunks)\n",
    "    if table == \"langlinks\":\n",
    "        prepare_langlinks_sql_dump(parsed_sql_dump_file, output_path, max_chunks=max_chunks)\n",
    "    \n",
    "    print(f\"created {output_path} in {time.time() - start:.2f} seconds ...\")\n",
    "    return output_path\n",
    "\n",
    "parallel = multiprocessing.cpu_count()\n",
    "converted = sc.parallelize(parsed, numSlices=parallel).map(\n",
    "    partial(\n",
    "        prepare_wikipedia_sql_dump_handler,\n",
    "        force=False,\n",
    "        # max_chunks=10,\n",
    "    )\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96175adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out the namespaces in more detail\n",
    "all_namespaces = pages.select(\"page_namespace\").distinct()\n",
    "all_namespaces.show()\n",
    "print(all_namespaces.count(), \"page_namespace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe585bf4-6824-47ff-a2fa-160beb37ec70",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e916870e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the german wikipedia artices with no translation entry\n",
    "german_pages = csv_loader.load(\n",
    "    f\"../hdd/wikipedia_sql_dumps/dewiki/20211001/dewiki-20211001-page.sql.csv\"\n",
    ").limit(10_000)\n",
    "\n",
    "page_langlinks = pages.join(\n",
    "    langlinks.select([\n",
    "        F.col(\"lang\").alias(\"id\"),\n",
    "        F.col(\"lang_title\").alias(\"translation_lang\"),\n",
    "        F.col(\"page_id\").alias(\"english_page_id\")\n",
    "    ]), on=\"id\", how=\"outer\")\n",
    "page_langlinks.show()\n",
    "\n",
    "\n",
    "without_translation = page_langlinks.filter(F.col(\"translation_lang\").isNull())\n",
    "\n",
    "print(\"total:\", page_langlinks.count())\n",
    "print(\"without translation:\", without_translation.count())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
