{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28b14d4-39a1-4c83-84e3-d8dfeb33d55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import bz2\n",
    "import csv\n",
    "import io\n",
    "import json\n",
    "import time\n",
    "import urllib\n",
    "import random\n",
    "import requests\n",
    "import concurrent.futures\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import List, Dict\n",
    "import lsde2021.csv as csvutil\n",
    "import lsde2021.utils as utils\n",
    "import lsde2021.download as dl\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10ef6a7-ef3b-42c6-934b-cb988509f1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_MEMORY = \"60G\"\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"parse-wikipedia-sql-dumps\") \\\n",
    "    .config(\"spark.executor.memory\", MAX_MEMORY) \\\n",
    "    .config(\"spark.driver.memory\", MAX_MEMORY) \\\n",
    "    .config('spark.driver.maxResultSize', MAX_MEMORY) \\\n",
    "    .config('spark.dynamicAllocation.maxExecutors', 4) \\\n",
    "    .config('spark.ui.showConsoleProgress', 'false') \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa19333-6a4f-4909-a251-ad76da62854c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = \"enwiki\"\n",
    "pages = spark.read.format(\"parquet\").options(inferSchema='True').load(f\"../nvme/wikipedia_sql_dumps/{wiki}/20211001/{wiki}-20211001-page.sql.parquet\")\n",
    "pages.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0308fe23-7c52-4d53-a155-3515e5cd79bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_page_ids = pages \\\n",
    "    .filter(F.col(\"page_id\").isNotNull() & (F.col(\"page_namespace\") == 0)) \\\n",
    "    .select(\"page_id\") \\\n",
    "    .distinct() \\\n",
    "    .withColumn(\"page_id\", F.col(\"page_id\").cast(T.IntegerType())) \\\n",
    "    .sort('page_id', ascending=True) \\\n",
    "    .rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ddadca-23d3-4666-9332-d1ac05653227",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(en_page_ids))\n",
    "pprint(en_page_ids[:20])\n",
    "assert isinstance(en_page_ids[0], int)\n",
    "\n",
    "ores_topics_dir = Path(\"../nvme/ores_topics\")\n",
    "ores_topics_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(ores_topics_dir / \"en_page_ids_sorted.pkl\", 'wb') as f:\n",
    "    pkl.dump(en_page_ids, f, protocol=pkl.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb4a337-e14b-4096-a102-4d1b57fc12e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_rev_ids(page_ids: List[int]) -> Dict[int, int]:\n",
    "    page_ids_str = '|'.join(map(str, page_ids))\n",
    "    revids = dict()\n",
    "    with requests.get(f\"https://en.wikipedia.org/w/api.php?action=query&prop=revisions&pageids={page_ids_str}&format=json\") as r:\n",
    "        r.raise_for_status()\n",
    "        content = json.loads(r.content)\n",
    "        if \"query\" in content:\n",
    "            if \"pages\" in content[\"query\"]:\n",
    "                pages = content[\"query\"][\"pages\"].items()\n",
    "                for page_id, metadata in pages:\n",
    "                    try:\n",
    "                        if \"revisions\" in metadata and len(metadata[\"revisions\"]) >= 1:\n",
    "                            revisions = metadata[\"revisions\"][0]\n",
    "                            revids[int(page_id)] = int(revisions.get(\"revid\", None))\n",
    "                    except Exception:\n",
    "                        pass\n",
    "        return revids\n",
    "    \n",
    "pprint(get_page_rev_ids([604727, 604728]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f3ec83-2627-48e3-8d5e-1ccccd4fb7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ORESException(ValueError):\n",
    "    def __init__(self, message):\n",
    "        super().__init__(message)\n",
    "\n",
    "def get_ores_articletopics(context: str, models: List[str], rev_ids: List[int]) -> Dict[int, int]:\n",
    "    url = \"https://ores.wikimedia.org/v3/scores/{0}/\".format(urllib.parse.quote(context))\n",
    "\n",
    "    params = {'revids': \"|\".join(str(rid) for rid in rev_ids),\n",
    "              'models': \"|\".join(urllib.parse.quote(model) for model in models)}\n",
    "    \n",
    "    headers = {\"User-Agent\": random.choice(dl.USER_AGENTS)}\n",
    "    with requests.get(url, params=params, headers=headers) as r:\n",
    "        r.raise_for_status()\n",
    "        content = json.loads(r.content)\n",
    "        \n",
    "        if 'error' in content:\n",
    "            raise ORESException(content['error'])\n",
    "        if 'warnings' in content:\n",
    "            for warning in content['warnings']:\n",
    "                print(warning)\n",
    "        \n",
    "        return [content[context]['scores'][str(rev_id)] for rev_id in rev_ids]\n",
    "    \n",
    "get_ores_articletopics(context=\"enwiki\", models=[\"articletopic\"], rev_ids=[\"1050929646\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427a1016-da01-4e83-9052-1eb67a678031",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_prob=0.6\n",
    "max_topics=5\n",
    "retries=10\n",
    "\n",
    "def get_ores_articletopics_for_page_ids (page_ids):\n",
    "    all_rev_ids = dict()\n",
    "    all_topics = dict()\n",
    "    for i in range(0, len(page_ids), 50):\n",
    "        attempts, success = 0, False\n",
    "        while not success and attempts < retries:\n",
    "            attempts += 1\n",
    "            try:\n",
    "                rev_ids = get_page_rev_ids(page_ids[i:i+50]).items()\n",
    "                scores = list(get_ores_articletopics(context=\"enwiki\", models=[\"articletopic\"], rev_ids=[rid for _, rid in rev_ids]))\n",
    "                for (page_id, rev_id), score in zip(rev_ids, scores):\n",
    "                    all_rev_ids[page_id] = rev_id\n",
    "                    if \"articletopic\" in score:\n",
    "                        response = score[\"articletopic\"]\n",
    "                        if \"error\" not in response and \"score\" in response:\n",
    "                            if \"probability\" in response[\"score\"]:\n",
    "                                topic_probs = response[\"score\"][\"probability\"]\n",
    "                                topic_probs = sorted(topic_probs.items(), key=lambda t: t[1], reverse=True)\n",
    "                                topic_probs = [t for t, prob in topic_probs if prob > min_prob]\n",
    "                                all_topics[page_id] = topic_probs[:max_topics]\n",
    "                                success = True\n",
    "                        else:\n",
    "                            print(\"bad response\", response)\n",
    "            except Exception as e:\n",
    "                # raise e\n",
    "                print(\"error\", e)\n",
    "    return all_rev_ids, all_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef578bf4-8bcc-400f-92eb-263c473331ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_parallel = 2\n",
    "start = 0 * 100_000\n",
    "count = 100_000\n",
    "chunk_size = int(np.ceil(count / n_parallel))\n",
    "all_revids = dict()\n",
    "all_topics = dict()\n",
    "procs = []\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=n_parallel) as executor:\n",
    "    for worker_id in range(n_parallel):\n",
    "        worker_page_ids = en_page_ids[start + worker_id * chunk_size: start + (worker_id + 1) * chunk_size]\n",
    "        print(worker_page_ids[:10])\n",
    "        print(\"worker %d got assigned %d page ids\" % (worker_id, len(worker_page_ids)))\n",
    "        procs.append(executor.submit(get_ores_articletopics_for_page_ids, worker_page_ids))\n",
    "\n",
    "# collect the results\n",
    "for i, proc in enumerate(procs):\n",
    "    cur_revids, cur_topics = proc.result()\n",
    "    all_revids.update(cur_revids)\n",
    "    all_topics.update(cur_topics)\n",
    "    print(\"worker %d done\" % i)\n",
    "\n",
    "# save result to pickle\n",
    "revids_file = ores_topics_dir / (\"revids_%d_to_%d.pkl\" % (start, start+count))\n",
    "topics_file = ores_topics_dir / (\"topics_%d_to_%d.pkl\" % (start, start+count))\n",
    "print(revids_file)\n",
    "print(topics_file)\n",
    "\n",
    "with open(revids_file, 'wb') as f:\n",
    "    pkl.dump(all_revids, f, protocol=pkl.HIGHEST_PROTOCOL)\n",
    "with open(topics_file, 'wb') as f:\n",
    "    pkl.dump(all_topics, f, protocol=pkl.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(len(all_revids))\n",
    "# https://ores.wikimedia.org/v3/scores/enwiki?models=articletopic&revids=421063984"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3befa9b4-e8ec-466a-9526-2f380efce6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_with_topics.write.format(\"parquet\").mode(\"overwrite\") \\\n",
    "    .partitionBy(\"ores_topic1\").save(f\"../nvme/wikipedia_sql_dumps/{wiki}/20211001/{wiki}-20211001-page-ores-topics.sql.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
