{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9451169b-b010-435c-a572-07af1d7404fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bz2\n",
    "import csv\n",
    "import io\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import List, Dict\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import lsde2021.csv as csvutils\n",
    "import lsde2021.utils as utils\n",
    "import lsde2021.download as dl\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fecb49-e430-423a-a27d-a5c93f23f67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_MEMORY = \"30G\"\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"parse-wikipedia-sql-dumps\") \\\n",
    "    .config(\"spark.executor.memory\", MAX_MEMORY) \\\n",
    "    .config(\"spark.driver.memory\", MAX_MEMORY) \\\n",
    "    .config('spark.driver.maxResultSize', MAX_MEMORY) \\\n",
    "    .config('spark.ui.showConsoleProgress', 'false') \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff33b465-39c0-4ab1-918a-a8d45e6e29bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = spark.read.format(\"csv\").options(header='True').load(\"./data/languages.csv\")\n",
    "languages = languages.withColumn(\"wiki_code\", F.concat(F.col(\"code\"), F.lit(\".wikipedia\")))\n",
    "languages = languages.select(F.col(\"name\").alias(\"language\"), \"dbname\", \"group\", \"code\", \"wiki_code\")\n",
    "languages.limit(100).show()\n",
    "\n",
    "language_dbnames = languages.select(F.col(\"dbname\")).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "print(language_dbnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256e068f-732e-4a96-8afb-765dd6a29530",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_english_topics_schema = T.StructType([\n",
    "    T.StructField('page_id', T.IntegerType(), False),\n",
    "    T.StructField('topics1', T.ArrayType(T.StringType()), False),\n",
    "    T.StructField('topics2', T.ArrayType(T.StringType()), False),\n",
    "    T.StructField('topics3', T.ArrayType(T.StringType()), False),\n",
    "    T.StructField('topics4', T.ArrayType(T.StringType()), False),\n",
    "])\n",
    "\n",
    "\n",
    "raw_english_topics = spark.read.format(\"parquet\").load(f\"../nvme/en_topics/topics_final.parquet\")\n",
    "raw_english_topics.limit(20).show()\n",
    "print(\"page ids with topics:\", raw_english_topics.select(\"page_id\").distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4de0d35-abee-4c02-957f-cbe5bed5b568",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_schema = T.StructType([\n",
    "    T.StructField(\"page_id\", T.IntegerType(), True),\n",
    "    T.StructField(\"page_namespace\", T.IntegerType(), True),\n",
    "    T.StructField(\"page_title\", T.StringType(), True),\n",
    "    T.StructField(\"page_restrictions\", T.StringType(), True),\n",
    "    T.StructField(\"page_is_redirect\", T.BooleanType(), True),\n",
    "    T.StructField(\"page_is_new\", T.BooleanType(), True),\n",
    "    T.StructField(\"page_random\", T.FloatType(), True),\n",
    "    T.StructField(\"page_touched\", T.TimestampType(), True),\n",
    "    T.StructField(\"page_links_updated\", T.TimestampType(), True),\n",
    "    T.StructField(\"page_latest\", T.StringType(), True),\n",
    "    T.StructField(\"page_len\", T.IntegerType(), True),\n",
    "    T.StructField(\"page_content_model\", T.StringType(), True),\n",
    "    T.StructField(\"page_lang\", T.StringType(), True),\n",
    "])\n",
    "\n",
    "raw_english_pages = spark.read.format(\"parquet\").options(inferSchema='True').option(\"mergeSchema\", \"true\").load(f\"../nvme/wikipedia_sql_dumps/enwiki/20211001/enwiki-20211001-page.sql.parquet\")\n",
    "english_pages = raw_english_pages.withColumn(\"page_is_redirect\", F.col(\"page_is_redirect\").cast(T.BooleanType()))\n",
    "english_pages = english_pages.withColumn(\"page_is_new\", F.col(\"page_is_new\").cast(T.BooleanType()))\n",
    "english_pages = english_pages.withColumn(\"page_random\", F.col(\"page_random\").cast(T.FloatType()))\n",
    "english_pages = english_pages.withColumn(\"page_touched\", F.to_timestamp(\"page_touched\", 'yyyyMMddHHmmss'))\n",
    "english_pages = english_pages.withColumn(\"page_links_updated\", F.to_timestamp(\"page_links_updated\", 'yyyyMMddHHmmss'))\n",
    "english_pages = english_pages.withColumn(\"page_len\", F.col(\"page_len\").cast(T.IntegerType()))\n",
    "english_pages.limit(20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5938f6b8-6b46-42e6-96cd-a429c9728138",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_pages = english_pages.join(raw_english_topics, on=\"page_id\", how=\"outer\")\n",
    "english_pages.limit(20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b188ced3-19ff-4740-987f-5034c094f898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count page ids without a topic\n",
    "print(\"total page ids\", english_pages.select(\"page_id\").distinct().count())\n",
    "english_pages_ns0_non_redirect = english_pages.filter(\n",
    "    (F.col(\"page_is_redirect\") == 0)\n",
    "    & (F.col(\"page_namespace\") == 0)\n",
    ")\n",
    "print(\"total page ids in namespace 0 which are not redirects\", english_pages_ns0_non_redirect.select(\"page_id\").distinct().count())\n",
    "print(\"page ids without topics:\", english_pages.filter(F.col(\"topics1\").isNull()).count())\n",
    "print(\"page ids in namespace 0 without topics:\", english_pages_ns0_non_redirect.filter(F.col(\"topics1\").isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edcf982-ea44-48dd-8999-01dc1d5f724d",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_pages_with_topics = english_pages_ns0_non_redirect.filter(\n",
    "    (F.col(\"topics1\").isNotNull())\n",
    "    & (F.col(\"topics2\").isNotNull())\n",
    "    & (F.col(\"topics3\").isNotNull())\n",
    "    & (F.col(\"topics4\").isNotNull())\n",
    ")\n",
    "print(\"writing %d pages with topics\" % (english_pages_with_topics.count()))\n",
    "# english_pages_with_topics.write.format(\"parquet\").mode(\"overwrite\").save(f\"../nvme/wikipedia_sql_dumps/enwiki/20211001/enwiki-20211001-page-topics-ns0-nonredirect.sql.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60431a1e-8026-4df7-8cee-8aae7034b9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the number of categories in level 1, 2, 3, 4\n",
    "test = spark.read.format(\"parquet\").load(f\"../nvme/wikipedia_sql_dumps/enwiki/20211001/enwiki-20211001-page-topics-ns0-nonredirect.sql.parquet\")\n",
    "test = test.withColumn(\"topics1\", F.explode(\"topics1\"))\n",
    "test = test.withColumn(\"topics2\", F.explode(\"topics2\"))\n",
    "test = test.withColumn(\"topics3\", F.explode(\"topics3\"))\n",
    "test = test.withColumn(\"topics4\", F.explode(\"topics4\"))\n",
    "test = test.select(\"topics1\", \"topics2\", \"topics3\", \"topics4\")\n",
    "print(\"level 1\", test.select(\"topics1\").distinct().count())\n",
    "print(\"level 2\", test.select(\"topics2\").distinct().count())\n",
    "print(\"level 3\", test.select(\"topics3\").distinct().count())\n",
    "print(\"level 4\", test.select(\"topics4\").distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef2fe99-2e67-4a2c-9858-90d43b6c1e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "langlinks_schema = T.StructType([\n",
    "    T.StructField(\"page_id\", T.IntegerType(), True),\n",
    "    T.StructField(\"lang\", T.StringType(), True),\n",
    "    T.StructField(\"lang_title\", T.StringType(), True),\n",
    "])\n",
    "\n",
    "en_langlinks = None\n",
    "for dbname in language_dbnames:\n",
    "    langlinks = spark.read.format(\"parquet\") \\\n",
    "        .schema(langlinks_schema) \\\n",
    "        .load(f\"../nvme/wikipedia_sql_dumps/{dbname}/20211001/{dbname}-20211001-langlinks.sql.parquet\")\n",
    "    langlinks = langlinks.filter(F.col(\"lang\") == \"en\")\n",
    "    langlinks = langlinks.withColumn('dbname', F.lit(dbname))\n",
    "    if en_langlinks is None:\n",
    "        en_langlinks = langlinks\n",
    "    else:\n",
    "        en_langlinks = en_langlinks.union(langlinks)\n",
    "\n",
    "en_langlinks = en_langlinks \\\n",
    "    .filter(F.col(\"lang_title\").isNotNull()) \\\n",
    "    .select(\"page_id\", 'dbname', F.col(\"lang_title\").alias(\"en_title\"))\n",
    "en_langlinks.limit(20).show()\n",
    "en_langlinks.filter(F.col(\"dbname\") == \"enwiki\").limit(20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c899c5-570b-469a-9145-7784b8104181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# en_langlinks.write.format(\"parquet\").mode(\"overwrite\").save(\"../nvme/wikipedia_sql_dumps/en_langlinks.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80caf509-0955-4f7d-a862-12a7815007c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pageview_complete_src = Path(\"../hdd/pageview_complete\")\n",
    "pageview_complete_dest = Path(\"../nvme/pageview_complete_processed\")\n",
    "end_date = datetime.date(2021, 10, 1)\n",
    "\n",
    "daily_pageview_files = []\n",
    "for year in [2018]: # 2019, 2020, 2021]:\n",
    "    daily_range = list(dl.date_range(\n",
    "        datetime.date(year, 1, 1),\n",
    "        datetime.date(year, 12, 31),\n",
    "    ))\n",
    "    \n",
    "    daily_range = [d for d in daily_range if (end_date - d).total_seconds() > 0]\n",
    "    daily_pageview_files += daily_range\n",
    "\n",
    "daily_pageview_files = [datetime.date(2021, 4, 21), datetime.date(2021, 4, 22)]\n",
    "daily_pageview_files = [\n",
    "    (\n",
    "        pageview_complete_src / Path(\"/\".join(dl.wikimedia_pageview_complete_local_file(date, monthly=False))),\n",
    "        pageview_complete_dest / Path(\"/\".join(dl.wikimedia_pageview_complete_local_file(date, monthly=False))).with_suffix(\".parquet\"),\n",
    "    )\n",
    "    for date in daily_pageview_files\n",
    "]\n",
    "pprint(daily_pageview_files[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a929f2c-2e07-4429-b1d2-0284b276c4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = {c: i for i, c in enumerate(list(\"abcdefghijklmnopqrstuvwxyz\"))}\n",
    "assert len(alphabet) == 26\n",
    "\n",
    "def hourly_pageviews_handler(s):\n",
    "    # from 0 to 23, written as 0 = A, 1 = B ... 22 = W, 23 = X, e.g. F1I1\n",
    "    ans = np.zeros(24)\n",
    "    if s is not None and s is not np.nan:\n",
    "        s = re.sub('[\\s+]', '', s)\n",
    "        parts = re.split('(\\d+)',s)\n",
    "        for i in range(0, len(parts)-1, 2):\n",
    "            ans[alphabet[parts[i].lower()]] = parts[i+1]\n",
    "    return ans.astype(int).tolist()\n",
    "\n",
    "def hourly_coding(**coded):\n",
    "    ans = np.zeros(24)\n",
    "    for c, val in coded.items():\n",
    "        try:\n",
    "            ans[alphabet[c.lower()]] = int(val)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return ans.astype(int).tolist()\n",
    "\n",
    "assert hourly_pageviews_handler(\"F234I12\") == hourly_coding(F=234, I=12)\n",
    "\n",
    "hourly_pageviews_udf = F.udf(hourly_pageviews_handler, T.ArrayType(T.IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cb50fa-8ae7-4e02-8088-633e900a5499",
   "metadata": {},
   "outputs": [],
   "source": [
    "pageview_schema = T.StructType([\n",
    "    T.StructField(\"wiki_code\", T.StringType(), True),\n",
    "    T.StructField(\"page_title\",T.StringType(), True),\n",
    "    T.StructField(\"page_id\", T.IntegerType(), True),\n",
    "    T.StructField(\"user_client\", T.StringType(), True),\n",
    "    T.StructField(\"daily_total\", T.IntegerType(), True),\n",
    "    T.StructField(\"hourly_count\", T.StringType(), True),\n",
    "])\n",
    "\n",
    "for daily_file, daily_processed_output_file in daily_pageview_files:\n",
    "    start = time.time()\n",
    "    \n",
    "    if False and daily_processed_output_file.exists():\n",
    "        print(f\"using existing file {daily_processed_output_file}\")\n",
    "        continue\n",
    "    \n",
    "    df = spark.read.format(\"csv\").options(delimiter=\" \", header=\"false\").schema(pageview_schema).load(str(daily_file))\n",
    "    \n",
    "    # combine daily pageviews for different user clients\n",
    "    df = df\\\n",
    "        .filter((F.col(\"page_id\").isNotNull()) & (F.col(\"wiki_code\").isNotNull())) \\\n",
    "        .groupBy([\"page_id\", \"page_title\", \"wiki_code\"]) \\\n",
    "        .agg(F.sum(\"daily_total\").alias(\"daily_total\"))\n",
    "    \n",
    "    # df = df.withColumn(\"hourly_count\", hourly_pageviews_udf(df['hourly_count']))\n",
    "    \n",
    "    df = df.join(languages, on=\"wiki_code\", how=\"inner\")\n",
    "        \n",
    "    # join the english lang title\n",
    "    df = df.join(en_langlinks, on=[\"page_id\", \"dbname\"], how=\"outer\")\n",
    "    \n",
    "    # set en title to be the same for the english pages\n",
    "    df = df.withColumn(\"en_title\", F.when(df.dbname == \"enwiki\", df.page_title).otherwise(df.en_title))\n",
    "    df = df.filter(F.col(\"en_title\").isNotNull())\n",
    "        \n",
    "    # join the english page id using the english page title\n",
    "    df = df.join(\n",
    "        english_pages_with_topics.select(F.col(\"page_title\").alias(\"en_title\"), F.col(\"page_id\").alias(\"en_page_id\"), \"topics1\", \"topics2\", \"topics3\", \"topics4\"),\n",
    "        on=\"en_title\",\n",
    "        how=\"inner\",\n",
    "    )\n",
    "    \n",
    "    df.write.format(\"parquet\").mode(\"overwrite\").partitionBy(\"group\").save(str(daily_processed_output_file))\n",
    "    print(\"wrote %s in %.2f minutes\" % (daily_processed_output_file, (time.time() - start) / 60))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
