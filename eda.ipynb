{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --quiet --no-cache --force git+https://github.com/romnn/lsde2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dd7a58d8-7690-4c3b-8844-69b35efd3e7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pyspark\n",
    "import lsde2021.download as dl\n",
    "import lsde2021.aggregate as agg\n",
    "from lsde2021.types import PathLike\n",
    "from pyspark.sql import SparkSession\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/10/02 17:45:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('EDA').getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.session.SparkSession'>\n"
     ]
    }
   ],
   "source": [
    "date_range = list(dl.datetime_range(\n",
    "    datetime.datetime(2019, 1, 1, hour=0),\n",
    "    datetime.datetime(2019, 1, 3, hour=0)\n",
    "))\n",
    "assert len(date_range) == 2 * 24 + 1\n",
    "# assert len(date_range) == 365 * 24 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-000000.gz(0 + 4) / 4]\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190101-120000.gz\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190101-000000.gz\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-120000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190101-000000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190101-010000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190101-120000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190101-130000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190102-000000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-010000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190102-120000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-130000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190101-010000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190101-020000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190102-010000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-020000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190101-130000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190101-140000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190102-130000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-140000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190101-020000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190101-030000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190102-020000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-030000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190101-140000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190101-150000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190101-030000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190101-040000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190102-030000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-040000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190102-140000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-150000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190101-040000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190101-050000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190101-150000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190101-160000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190102-040000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-050000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190101-050000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190101-060000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190102-150000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-160000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190102-050000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-060000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190101-160000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190101-170000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190101-060000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190101-070000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190102-060000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-070000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190102-160000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-170000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190101-070000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190101-080000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190101-170000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190101-180000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190102-070000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-080000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190101-080000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190101-090000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190102-170000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-180000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190102-080000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-090000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190101-180000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190101-190000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190101-090000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190101-100000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190102-090000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-100000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190102-180000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-190000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190101-100000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190101-110000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190101-190000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190101-200000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190102-100000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-110000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190102-190000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-200000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190101-110000.gz ...\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190101-200000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190101-210000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190102-110000.gz ...\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190102-200000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-210000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190101-210000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190101-220000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190102-210000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-220000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190101-220000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190101-230000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190102-220000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-230000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190101-230000.gz ...\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190102-230000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190103-000000.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.datetime(2019, 1, 1, 0, 0), PosixPath('wikimedia_data/2019/2019-01/pageviews-20190101-000000.gz')), (datetime.datetime(2019, 1, 1, 1, 0), PosixPath('wikimedia_data/2019/2019-01/pageviews-20190101-010000.gz')), (datetime.datetime(2019, 1, 1, 2, 0), PosixPath('wikimedia_data/2019/2019-01/pageviews-20190101-020000.gz'))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190103-000000.gz ...\n",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dest = Path(\"/dbfs/mnt/group29\")\n",
    "dest = Path(\"./wikimedia_data\")\n",
    "# dl.download_handler(list(dl.wikimedia_files(date_range))[10], dest=dest, force=False)\n",
    "downloaded = sc.parallelize(dl.wikimedia_files(date_range)) \\\n",
    "    .map(partial(dl.download_handler, dest=dest, force=False)) \\\n",
    "    .collect()\n",
    "\n",
    "print(downloaded[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "608103b4-d709-4274-bfd2-b9851867189e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "downloaded_files = [p for p in (dest / \"2019/2019-01\").glob(\"**/*.gz\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 49\n",
      "[PosixPath('wikimedia_data/2019/2019-01/pageviews-20190101-000000.gz'), PosixPath('wikimedia_data/2019/2019-01/pageviews-20190101-010000.gz'), PosixPath('wikimedia_data/2019/2019-01/pageviews-20190101-020000.gz'), PosixPath('wikimedia_data/2019/2019-01/pageviews-20190101-030000.gz'), PosixPath('wikimedia_data/2019/2019-01/pageviews-20190101-040000.gz'), PosixPath('wikimedia_data/2019/2019-01/pageviews-20190101-050000.gz'), PosixPath('wikimedia_data/2019/2019-01/pageviews-20190101-060000.gz'), PosixPath('wikimedia_data/2019/2019-01/pageviews-20190101-070000.gz'), PosixPath('wikimedia_data/2019/2019-01/pageviews-20190101-080000.gz'), PosixPath('wikimedia_data/2019/2019-01/pageviews-20190101-090000.gz')]\n"
     ]
    }
   ],
   "source": [
    "print(len(downloaded_files), len(downloaded))\n",
    "assert len(downloaded_files) == len(downloaded)\n",
    "print(downloaded_files[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# todo: aggregate days here and store to parquet\n",
    "date_range = list(dl.datetime_range(\n",
    "    datetime.datetime(2019, 1, 1, hour=0),\n",
    "    datetime.datetime(2019, 1, 2, hour=0),\n",
    "    interval=datetime.timedelta(days=1)\n",
    "))\n",
    "print(len(date_range))\n",
    "assert len(date_range) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/02 19:15:09 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 1510651 ms exceeds timeout 120000 ms\n",
      "21/10/02 19:15:09 WARN SparkContext: Killing executors is not supported by current scheduler.\n",
      "21/10/02 19:21:29 WARN TaskMemoryManager: Failed to allocate a page (134217728 bytes), try again.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote wikimedia_data/daily/2019/2019-1-1.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 275:(138 + 2) / 200][Stage 278:>  (0 + 1) / 1][Stage 281:>  (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, IntegerType\n",
    "import pyspark.sql.functions as F\n",
    "import traceback\n",
    "\n",
    "# see https://stackoverflow.com/questions/51217168/wikipedia-pageviews-analysis\n",
    "# domain_code\n",
    "# page_title\n",
    "# count_views\n",
    "# total_response_size (no longer maintained)\n",
    "\n",
    "def aggregate_daily_pageviews(date: datetime.date, src: PathLike, dest: PathLike) -> PathLike:\n",
    "    schema = StructType([\n",
    "        StructField(\"domain_code\", StringType(), True),\n",
    "        StructField(\"page_title\", StringType(), True),\n",
    "        StructField(\"view_count\", LongType(), True),\n",
    "        StructField(\"total_response_size\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "    csv_loader = spark.read.format(\"csv\").option(\"sep\", ' ')\n",
    "    \n",
    "    daily = None\n",
    "    daily_out = dest / Path(\"/\".join(dl.wikimedia_daily_local_file(date)))\n",
    "    \n",
    "    for hour in range(24):\n",
    "        current = datetime.datetime.combine(date, datetime.time.min) + datetime.timedelta(hours=hour)\n",
    "        file = src / Path(\"/\".join(dl.wikimedia_local_file(current)))\n",
    "        # print(file)\n",
    "        # continue\n",
    "        try:\n",
    "            df = csv_loader.load(str(file), schema=schema)\n",
    "            if daily is None:\n",
    "                daily = df\n",
    "            else:\n",
    "                daily = df \\\n",
    "                    .select(\"domain_code\", \"page_title\", F.col(\"view_count\").alias(\"view_count2\")) \\\n",
    "                    .join(daily, on=[\"domain_code\", \"page_title\"], how=\"outer\") \\\n",
    "                    .fillna(value=0)\n",
    "                daily = daily \\\n",
    "                    .withColumn('view_count_sum', sum([daily[\"view_count\"], daily[\"view_count2\"]])) \\\n",
    "                    .select(\"domain_code\", \"page_title\", F.col(\"view_count_sum\").alias(\"view_count\"))\n",
    "        except Exception as e:\n",
    "            print(f\"failed to load {file}: {e}\")\n",
    "            print(traceback.format_exc())\n",
    "    \n",
    "    if daily:\n",
    "        try:\n",
    "            daily = daily \\\n",
    "                .sort(F.col(\"view_count\").desc()) \\\n",
    "                .repartition(F.col(\"domain_code\"))\n",
    "            daily.show()\n",
    "            daily_out.parent.mkdir(parents=True, exist_ok=True)\n",
    "            daily.write.format(\"parquet\").partitionBy(\"domain_code\").mode(\"overwrite\").save(str(daily_out))\n",
    "            print(f\"wrote {daily_out}\")\n",
    "            # print(date, daily.count())\n",
    "        except Exception as e:\n",
    "            print(f\"failed to save daily data {daily_out}: {e}\")\n",
    "            print(traceback.format_exc())\n",
    "    return daily_out\n",
    "    \n",
    "for date in date_range:\n",
    "    agg.aggregate_daily_pageviews(spark, date.date(), src=dest, dest=dest / \"daily\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5e13a542-1259-44f6-aa14-903ba0b17881",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_loader = spark.read.format(\"csv\") \\\n",
    "            .option(\"sep\", ' ')\n",
    "           #.option(\"header\", \"true\") \\\n",
    "           #.option(\"delimiter\", \"|\") \\\n",
    "           #.option(\"inferschema\", \"true\")\n",
    "for date, file in downloaded:\n",
    "    df = csv_loader.load(str(file))\n",
    "    df.show(1)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0e02e82d-2e17-4a70-9c9e-8462e91ee8f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"domain\", StringType(), True),\n",
    "    StructField(\"pagename\", StringType(), True),\n",
    "    StructField(\"count\", StringType(), True),\n",
    "   StructField(\"responsebytes\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "df = spark.read.option(\"sep\",\" \").csv(\"/mnt/group29/test.gz\", schema=schema)\n",
    "df.show(1)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "eda",
   "notebookOrigID": 1672146355091320,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
