{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --quiet --no-cache --force git+https://github.com/romnn/lsde2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dd7a58d8-7690-4c3b-8844-69b35efd3e7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pyspark\n",
    "import lsde2021.download as dl\n",
    "import lsde2021.aggregate as agg\n",
    "from lsde2021.types import PathLike\n",
    "from pyspark.sql import SparkSession\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_MEMORY = \"60G\"\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"EDA\") \\\n",
    "    .config(\"spark.executor.memory\", MAX_MEMORY) \\\n",
    "    .config(\"spark.driver.memory\", MAX_MEMORY) \\\n",
    "    .config('spark.driver.maxResultSize', MAX_MEMORY) \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in [2018, 2019, 2020, 2021]:\n",
    "    hour_range = list(dl.datetime_range(\n",
    "        datetime.datetime(year, 1, 1, hour=0),\n",
    "        datetime.datetime(year, 1, 8, hour=0)\n",
    "    ))\n",
    "    assert len(hour_range) == 7 * 24 + 1\n",
    "    \n",
    "    dest = Path(\"./wikimedia_data\")\n",
    "    downloaded = sc.parallelize(dl.wikimedia_files(hour_range)) \\\n",
    "        .map(partial(dl.download_handler, dest=dest, force=False)) \\\n",
    "        .collect()\n",
    "\n",
    "    downloaded_files = [p for p in (dest / f\"{year}/{year}-01\").glob(\"**/*.gz\")]\n",
    "    print(downloaded_files[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in [2018, 2019, 2020, 2021]:\n",
    "    daily_date_range = list(dl.datetime_range(\n",
    "        datetime.datetime(year, 1, 1, hour=0),\n",
    "        datetime.datetime(year, 1, 1, hour=0),\n",
    "        interval=datetime.timedelta(days=1)\n",
    "    ))\n",
    "    print(len(daily_date_range))\n",
    "    assert len(daily_date_range) == 1\n",
    "    \n",
    "    # aggregate days here and store to parquet\n",
    "    for date in daily_date_range:\n",
    "        agg.aggregate_daily_pageviews(date.date(), spark=spark, src=dest, dest=dest / \"daily\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "608103b4-d709-4274-bfd2-b9851867189e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, IntegerType\n",
    "import pyspark.sql.functions as F\n",
    "import traceback\n",
    "\n",
    "# see https://stackoverflow.com/questions/51217168/wikipedia-pageviews-analysis\n",
    "# domain_code\n",
    "# page_title\n",
    "# count_views\n",
    "# total_response_size (no longer maintained)\n",
    "\n",
    "def aggregate_daily_pageviews(date: datetime.date, src: PathLike, dest: PathLike) -> PathLike:\n",
    "    schema = StructType([\n",
    "        StructField(\"domain_code\", StringType(), True),\n",
    "        StructField(\"page_title\", StringType(), True),\n",
    "        StructField(\"view_count\", LongType(), True),\n",
    "        StructField(\"total_response_size\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "    csv_loader = spark.read.format(\"csv\").option(\"sep\", ' ')\n",
    "    \n",
    "    daily = None\n",
    "    daily_out = dest / Path(\"/\".join(dl.wikimedia_daily_local_file(date)))\n",
    "    \n",
    "    for hour in range(24):\n",
    "        current = datetime.datetime.combine(date, datetime.time.min) + datetime.timedelta(hours=hour)\n",
    "        file = src / Path(\"/\".join(dl.wikimedia_local_file(current)))\n",
    "        # print(file)\n",
    "        # continue\n",
    "        try:\n",
    "            df = csv_loader.load(str(file), schema=schema)\n",
    "            if daily is None:\n",
    "                daily = df\n",
    "            else:\n",
    "                daily = df \\\n",
    "                    .select(\"domain_code\", \"page_title\", F.col(\"view_count\").alias(\"view_count2\")) \\\n",
    "                    .join(daily, on=[\"domain_code\", \"page_title\"], how=\"outer\") \\\n",
    "                    .fillna(value=0)\n",
    "                daily = daily \\\n",
    "                    .withColumn('view_count_sum', sum([daily[\"view_count\"], daily[\"view_count2\"]])) \\\n",
    "                    .select(\"domain_code\", \"page_title\", F.col(\"view_count_sum\").alias(\"view_count\"))\n",
    "        except Exception as e:\n",
    "            print(f\"failed to load {file}: {e}\")\n",
    "            print(traceback.format_exc())\n",
    "    \n",
    "    if daily:\n",
    "        try:\n",
    "            daily = daily \\\n",
    "                .sort(F.col(\"view_count\").desc()) \\\n",
    "                .repartition(F.col(\"domain_code\"))\n",
    "            daily.show()\n",
    "            daily_out.parent.mkdir(parents=True, exist_ok=True)\n",
    "            daily.write.format(\"parquet\").partitionBy(\"domain_code\").mode(\"overwrite\").save(str(daily_out))\n",
    "            print(f\"wrote {daily_out}\")\n",
    "            # print(date, daily.count())\n",
    "        except Exception as e:\n",
    "            print(f\"failed to save daily data {daily_out}: {e}\")\n",
    "            print(traceback.format_exc())\n",
    "    return daily_out\n",
    "    \n",
    "for date in daily_date_range:\n",
    "    agg.aggregate_daily_pageviews(spark, date.date(), src=dest, dest=dest / \"daily\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import lsde2021.aggregate as agg\n",
    "hourly = sc.parallelize([d.date() for d in daily_date_range][:1]) \\\n",
    "    .map(partial(agg.aggregate_daily_pageviews, src=dest, dest=dest / \"daily\")) \\\n",
    "    .collect()\n",
    "print(len(hourly))\n",
    "print(hourly[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5e13a542-1259-44f6-aa14-903ba0b17881",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_loader = spark.read.format(\"csv\") \\\n",
    "            .option(\"sep\", ' ')\n",
    "           #.option(\"header\", \"true\") \\\n",
    "           #.option(\"delimiter\", \"|\") \\\n",
    "           #.option(\"inferschema\", \"true\")\n",
    "for date, file in downloaded:\n",
    "    df = csv_loader.load(str(file))\n",
    "    df.show(1)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0e02e82d-2e17-4a70-9c9e-8462e91ee8f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"domain\", StringType(), True),\n",
    "    StructField(\"pagename\", StringType(), True),\n",
    "    StructField(\"count\", StringType(), True),\n",
    "   StructField(\"responsebytes\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "df = spark.read.option(\"sep\",\" \").csv(\"/mnt/group29/test.gz\", schema=schema)\n",
    "df.show(1)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "eda",
   "notebookOrigID": 1672146355091320,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
