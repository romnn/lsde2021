{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dd7a58d8-7690-4c3b-8844-69b35efd3e7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import urllib.request\n",
    "from urllib.parse import unquote, urlparse\n",
    "import os\n",
    "import shutil\n",
    "import requests\n",
    "import datetime\n",
    "import traceback\n",
    "from pathlib import Path, PurePosixPath\n",
    "from typing import List, Iterable, Optional, Union\n",
    "\n",
    "PathLike = Union[str, os.PathLike]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('EDA').getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wikimedia_url(date: datetime.datetime) -> str:\n",
    "    year = str(date.year)\n",
    "    month = str(date.month).zfill(2)\n",
    "    day = str(date.day).zfill(2)\n",
    "    hour = str(date.hour).zfill(2)\n",
    "    file = f\"{year}/{year}-{month}/pageviews-{year}{month}{day}-{hour}0000.gz\"\n",
    "    return f\"https://dumps.wikimedia.org/other/pageviews/{file}\"\n",
    "\n",
    "def wikimedia_local_file(date: datetime.datetime) -> str:\n",
    "    parsed_url = urlparse(wikimedia_url(date))\n",
    "    parsed_path = PurePosixPath(unquote(parsed_url.path))\n",
    "    filename_parts = parsed_path.parts[-3:]\n",
    "    return filename_parts\n",
    "\n",
    "for d, url, path in [\n",
    "    (\n",
    "        datetime.datetime(2021, 1, 1, hour=8),\n",
    "        \"https://dumps.wikimedia.org/other/pageviews/2021/2021-01/pageviews-20210101-080000.gz\",\n",
    "        (\"2021\", \"2021-01\", \"pageviews-20210101-080000.gz\"),\n",
    "    ),\n",
    "    (\n",
    "        datetime.datetime(2021, 1, 16, hour=14),\n",
    "        \"https://dumps.wikimedia.org/other/pageviews/2021/2021-01/pageviews-20210116-140000.gz\",\n",
    "        (\"2021\", \"2021-01\", \"pageviews-20210116-140000.gz\"),\n",
    "    ),\n",
    "]:\n",
    "    assert wikimedia_url(d) == url\n",
    "    assert wikimedia_local_file(d) == path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetime_range(\n",
    "    start: datetime.datetime,\n",
    "    end: datetime.datetime,\n",
    "    interval: Optional[datetime.timedelta] = None,\n",
    ") -> Iterable[datetime.datetime]:\n",
    "    iv = interval or datetime.timedelta(hours=1)\n",
    "    current = start\n",
    "    yield current\n",
    "    while current < end:\n",
    "        current += iv\n",
    "        yield current\n",
    "\n",
    "def wikimedia_files(\n",
    "    dates: Iterable[datetime.datetime]\n",
    ") -> Iterable[str]:\n",
    "    return zip(dates, map(wikimedia_url, dates))\n",
    "\n",
    "date_range = list(datetime_range(\n",
    "    datetime.datetime(2021, 1, 1, hour=8),\n",
    "    datetime.datetime(2021, 1, 1, hour=12)\n",
    "))\n",
    "assert len(date_range) == 5 # from 8 to 12 there are 5 hours\n",
    "assert list([url for _, url in wikimedia_files(date_range)])[:2] == [\n",
    "    'https://dumps.wikimedia.org/other/pageviews/2021/2021-01/pageviews-20210101-080000.gz',\n",
    "    'https://dumps.wikimedia.org/other/pageviews/2021/2021-01/pageviews-20210101-090000.gz'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "608103b4-d709-4274-bfd2-b9851867189e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def download_wikimedia_file(url: str, destination: PathLike) -> PathLike:\n",
    "    if Path(destination).exists():\n",
    "        # skip download\n",
    "        return destination\n",
    "    \n",
    "    # make sure the directory exists\n",
    "    Path(destination).parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # download the file\n",
    "    try:\n",
    "        with requests.get(url, allow_redirects=True) as data, open(destination, \"wb\") as out_file:\n",
    "            out_file.write(data.content)\n",
    "    except Exception as e:\n",
    "        print(f\"failed to download {url}: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "    return destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_handler(item, path):\n",
    "    date, url = item\n",
    "    filename = \"/\".join(wikimedia_local_file(date))\n",
    "    destination = path / filename\n",
    "    print(f\"downloading {destination}\")\n",
    "    return date, download_wikimedia_file(url, destination=destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test download locally\n",
    "if False:\n",
    "    for item in wikimedia_files(date_range):\n",
    "        download_handler(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8761 8760\n"
     ]
    }
   ],
   "source": [
    "# test download using spark in parallel\n",
    "date_range = list(datetime_range(\n",
    "    datetime.datetime(2019, 1, 1, hour=0),\n",
    "    datetime.datetime(2020, 1, 1, hour=0)\n",
    "))\n",
    "assert len(date_range) == 365 * 24 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path(\"/dbfs/mnt/group29\")\n",
    "from functools import partial\n",
    "downloaded = sc.parallelize(wikimedia_files(date_range)) \\\n",
    "    .map(partial(download_handler, path=Path(\"./wikimedia_data\"))) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-110000.gz\n",
      "21/10/02 06:30:20 ERROR Executor: Exception in task 2.0 in stage 33.0 (TID 104): Connection reset\n",
      "21/10/02 06:30:20 ERROR Executor: Exception in task 3.0 in stage 33.0 (TID 105): Connection reset\n",
      "21/10/02 06:30:20 ERROR Executor: Exception in task 1.0 in stage 33.0 (TID 103): Connection reset\n",
      "21/10/02 06:30:20 ERROR Executor: Exception in task 0.0 in stage 33.0 (TID 102): Connection reset\n"
     ]
    }
   ],
   "source": [
    "# stop any running spark processes\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparkContext' object has no attribute 'start'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_29/1554079828.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# restart spark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'SparkContext' object has no attribute 'start'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5e13a542-1259-44f6-aa14-903ba0b17881",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---+---+\n",
      "|_c0|      _c1|_c2|_c3|\n",
      "+---+---------+---+---+\n",
      "| aa|Main_Page|  5|  0|\n",
      "+---+---------+---+---+\n",
      "only showing top 1 row\n",
      "\n",
      "+---+---+---+---+\n",
      "|_c0|_c1|_c2|_c3|\n",
      "+---+---+---+---+\n",
      "| aa|  -| 10|  0|\n",
      "+---+---+---+---+\n",
      "only showing top 1 row\n",
      "\n",
      "+---+---------+---+---+\n",
      "|_c0|      _c1|_c2|_c3|\n",
      "+---+---------+---+---+\n",
      "| aa|Main_Page|  4|  0|\n",
      "+---+---------+---+---+\n",
      "only showing top 1 row\n",
      "\n",
      "+---+---+---+---+\n",
      "|_c0|_c1|_c2|_c3|\n",
      "+---+---+---+---+\n",
      "| aa|  -|  3|  0|\n",
      "+---+---+---+---+\n",
      "only showing top 1 row\n",
      "\n",
      "+---+---------+---+---+\n",
      "|_c0|      _c1|_c2|_c3|\n",
      "+---+---------+---+---+\n",
      "| aa|Main_Page| 48|  0|\n",
      "+---+---------+---+---+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_loader = spark.read.format(\"csv\") \\\n",
    "            .option(\"sep\", ' ')\n",
    "           #.option(\"header\", \"true\") \\\n",
    "           #.option(\"delimiter\", \"|\") \\\n",
    "           #.option(\"inferschema\", \"true\")\n",
    "for date, file in downloaded:\n",
    "    df = csv_loader.load(str(file))\n",
    "    df.show(1)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0e02e82d-2e17-4a70-9c9e-8462e91ee8f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+------+--------+-----+-------------+\n",
       "domain|pagename|count|responsebytes|\n",
       "+------+--------+-----+-------------+\n",
       "    aa|       -|    6|            0|\n",
       "+------+--------+-----+-------------+\n",
       "only showing top 1 row\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+------+--------+-----+-------------+\n|domain|pagename|count|responsebytes|\n+------+--------+-----+-------------+\n|    aa|       -|    6|            0|\n+------+--------+-----+-------------+\nonly showing top 1 row\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"domain\", StringType(), True),\n",
    "    StructField(\"pagename\", StringType(), True),\n",
    "    StructField(\"count\", StringType(), True),\n",
    "   StructField(\"responsebytes\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "df = spark.read.option(\"sep\",\" \").csv(\"/mnt/group29/test.gz\", schema=schema)\n",
    "df.show(1)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "eda",
   "notebookOrigID": 1672146355091320,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
