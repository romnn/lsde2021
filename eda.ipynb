{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --quiet --no-cache --force git+https://github.com/romnn/lsde2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dd7a58d8-7690-4c3b-8844-69b35efd3e7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pyspark\n",
    "import lsde2021.download as dl\n",
    "import lsde2021.aggregate as agg\n",
    "from lsde2021.types import PathLike\n",
    "from pyspark.sql import SparkSession\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/10/02 22:05:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "MAX_MEMORY = \"60G\"\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"EDA\") \\\n",
    "    .config(\"spark.executor.memory\", MAX_MEMORY) \\\n",
    "    .config(\"spark.driver.memory\", MAX_MEMORY) \\\n",
    "    .config('spark.driver.maxResultSize', MAX_MEMORY) \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_range = list(dl.datetime_range(\n",
    "    datetime.datetime(2019, 1, 1, hour=0),\n",
    "    datetime.datetime(2019, 1, 3, hour=0)\n",
    "))\n",
    "assert len(date_range) == 2 * 24 + 1\n",
    "# assert len(date_range) == 365 * 24 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-060000.gz + 16) / 16]\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190101-090000.gz\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-150000.gz\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190101-000000.gz\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190101-150000.gz\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-090000.gz\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190101-180000.gz\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190101-030000.gz\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-000000.gz\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-030000.gz\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-180000.gz\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190101-120000.gz\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190101-060000.gz\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-120000.gz\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-210000.gz\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190101-210000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190102-060000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-070000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190101-090000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190101-100000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190101-000000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190101-010000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190102-030000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-040000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190101-060000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190101-070000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190102-000000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-010000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190101-120000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190101-130000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190101-030000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190101-040000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190102-090000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-100000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190101-180000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190101-190000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190102-180000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-190000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190102-150000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-160000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190102-120000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-130000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190101-150000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190101-160000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190102-070000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-080000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190101-210000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190101-220000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190102-210000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-220000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190102-040000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-050000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190101-100000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190101-110000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190101-070000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190101-080000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190101-010000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190101-020000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190101-040000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190101-050000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190102-010000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-020000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190102-100000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-110000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190101-130000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190101-140000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190102-050000.gz ...\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190102-080000.gz ...\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190101-190000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190101-200000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190101-020000.gz ...\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190101-080000.gz ...\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190101-050000.gz ...\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190102-130000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-140000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190102-190000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-200000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190101-160000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190101-170000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190101-220000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190101-230000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190101-110000.gz ...\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190102-160000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-170000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190102-220000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190102-230000.gz\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190102-020000.gz ...\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190102-110000.gz ...\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190101-140000.gz ...\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190101-200000.gz ...\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190102-140000.gz ...\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190101-230000.gz ...\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190101-170000.gz ...\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190102-200000.gz ...\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190102-170000.gz ...\n",
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190102-230000.gz ...\n",
      "downloading wikimedia_data/2019/2019-01/pageviews-20190103-000000.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.datetime(2019, 1, 1, 0, 0), PosixPath('wikimedia_data/2019/2019-01/pageviews-20190101-000000.gz')), (datetime.datetime(2019, 1, 1, 1, 0), PosixPath('wikimedia_data/2019/2019-01/pageviews-20190101-010000.gz')), (datetime.datetime(2019, 1, 1, 2, 0), PosixPath('wikimedia_data/2019/2019-01/pageviews-20190101-020000.gz'))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "using existing file wikimedia_data/2019/2019-01/pageviews-20190103-000000.gz ...\n",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dest = Path(\"/dbfs/mnt/group29\")\n",
    "dest = Path(\"./wikimedia_data\")\n",
    "# dl.download_handler(list(dl.wikimedia_files(date_range))[10], dest=dest, force=False)\n",
    "downloaded = sc.parallelize(dl.wikimedia_files(date_range)) \\\n",
    "    .map(partial(dl.download_handler, dest=dest, force=False)) \\\n",
    "    .collect()\n",
    "\n",
    "print(downloaded[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "608103b4-d709-4274-bfd2-b9851867189e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "downloaded_files = [p for p in (dest / \"2019/2019-01\").glob(\"**/*.gz\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 49\n",
      "[PosixPath('wikimedia_data/2019/2019-01/pageviews-20190102-000000.gz'), PosixPath('wikimedia_data/2019/2019-01/pageviews-20190101-140000.gz'), PosixPath('wikimedia_data/2019/2019-01/pageviews-20190101-060000.gz'), PosixPath('wikimedia_data/2019/2019-01/pageviews-20190102-100000.gz'), PosixPath('wikimedia_data/2019/2019-01/pageviews-20190101-190000.gz'), PosixPath('wikimedia_data/2019/2019-01/pageviews-20190101-090000.gz'), PosixPath('wikimedia_data/2019/2019-01/pageviews-20190101-200000.gz'), PosixPath('wikimedia_data/2019/2019-01/pageviews-20190102-020000.gz'), PosixPath('wikimedia_data/2019/2019-01/pageviews-20190101-110000.gz'), PosixPath('wikimedia_data/2019/2019-01/pageviews-20190102-230000.gz')]\n"
     ]
    }
   ],
   "source": [
    "print(len(downloaded_files), len(downloaded))\n",
    "assert len(downloaded_files) == len(downloaded)\n",
    "print(downloaded_files[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# todo: aggregate days here and store to parquet\n",
    "daily_date_range = list(dl.datetime_range(\n",
    "    datetime.datetime(2019, 1, 1, hour=0),\n",
    "    datetime.datetime(2019, 1, 1, hour=0),\n",
    "    interval=datetime.timedelta(days=1)\n",
    "))\n",
    "print(len(daily_date_range))\n",
    "assert len(daily_date_range) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote wikimedia_data/daily/2019/2019-1-1.parquet\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, IntegerType\n",
    "import pyspark.sql.functions as F\n",
    "import traceback\n",
    "\n",
    "# see https://stackoverflow.com/questions/51217168/wikipedia-pageviews-analysis\n",
    "# domain_code\n",
    "# page_title\n",
    "# count_views\n",
    "# total_response_size (no longer maintained)\n",
    "\n",
    "def aggregate_daily_pageviews(date: datetime.date, src: PathLike, dest: PathLike) -> PathLike:\n",
    "    schema = StructType([\n",
    "        StructField(\"domain_code\", StringType(), True),\n",
    "        StructField(\"page_title\", StringType(), True),\n",
    "        StructField(\"view_count\", LongType(), True),\n",
    "        StructField(\"total_response_size\", IntegerType(), True)\n",
    "    ])\n",
    "\n",
    "    csv_loader = spark.read.format(\"csv\").option(\"sep\", ' ')\n",
    "    \n",
    "    daily = None\n",
    "    daily_out = dest / Path(\"/\".join(dl.wikimedia_daily_local_file(date)))\n",
    "    \n",
    "    for hour in range(24):\n",
    "        current = datetime.datetime.combine(date, datetime.time.min) + datetime.timedelta(hours=hour)\n",
    "        file = src / Path(\"/\".join(dl.wikimedia_local_file(current)))\n",
    "        # print(file)\n",
    "        # continue\n",
    "        try:\n",
    "            df = csv_loader.load(str(file), schema=schema)\n",
    "            if daily is None:\n",
    "                daily = df\n",
    "            else:\n",
    "                daily = df \\\n",
    "                    .select(\"domain_code\", \"page_title\", F.col(\"view_count\").alias(\"view_count2\")) \\\n",
    "                    .join(daily, on=[\"domain_code\", \"page_title\"], how=\"outer\") \\\n",
    "                    .fillna(value=0)\n",
    "                daily = daily \\\n",
    "                    .withColumn('view_count_sum', sum([daily[\"view_count\"], daily[\"view_count2\"]])) \\\n",
    "                    .select(\"domain_code\", \"page_title\", F.col(\"view_count_sum\").alias(\"view_count\"))\n",
    "        except Exception as e:\n",
    "            print(f\"failed to load {file}: {e}\")\n",
    "            print(traceback.format_exc())\n",
    "    \n",
    "    if daily:\n",
    "        try:\n",
    "            daily = daily \\\n",
    "                .sort(F.col(\"view_count\").desc()) \\\n",
    "                .repartition(F.col(\"domain_code\"))\n",
    "            daily.show()\n",
    "            daily_out.parent.mkdir(parents=True, exist_ok=True)\n",
    "            daily.write.format(\"parquet\").partitionBy(\"domain_code\").mode(\"overwrite\").save(str(daily_out))\n",
    "            print(f\"wrote {daily_out}\")\n",
    "            # print(date, daily.count())\n",
    "        except Exception as e:\n",
    "            print(f\"failed to save daily data {daily_out}: {e}\")\n",
    "            print(traceback.format_exc())\n",
    "    return daily_out\n",
    "    \n",
    "for date in daily_date_range:\n",
    "    agg.aggregate_daily_pageviews(spark, date.date(), src=dest, dest=dest / \"daily\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/02 20:08:18 ERROR FileFormatWriter: Aborting job 9ce22dc4-ccf1-488f-bf3f-343ddfb3647a.\n",
      "org.apache.spark.SparkException: Job 41 cancelled because SparkContext was shut down\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1085)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1083)\n",
      "\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1083)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:2463)\n",
      "\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2369)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2069)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1419)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2069)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.stop(JavaSparkContext.scala:550)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n",
      "\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:304)\n",
      "\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:171)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:267)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:155)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:149)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:166)\n",
      "\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:132)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:118)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:118)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:151)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:149)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:166)\n",
      "\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.doExecute(basicPhysicalOperators.scala:92)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n",
      "\tat org.apache.spark.sql.execution.SortExec.doExecute(SortExec.scala:112)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:184)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/10/02 20:08:18 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@73afbb8b rejected from java.util.concurrent.ThreadPoolExecutor@2d830ea9[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 27852]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:769)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:745)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/10/02 20:08:18 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@54990e26 rejected from java.util.concurrent.ThreadPoolExecutor@5fa09d7b[Shutting down, pool size = 3, active threads = 3, queued tasks = 0, completed tasks = 27853]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.executor.Executor.launchTask(Executor.scala:270)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/10/02 20:08:18 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@36e4b3e4 rejected from java.util.concurrent.ThreadPoolExecutor@2d830ea9[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 27852]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:769)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:745)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/10/02 20:08:18 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@7777062a rejected from java.util.concurrent.ThreadPoolExecutor@5fa09d7b[Shutting down, pool size = 2, active threads = 2, queued tasks = 0, completed tasks = 27854]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.executor.Executor.launchTask(Executor.scala:270)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/10/02 20:08:18 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@6c1bcff0 rejected from java.util.concurrent.ThreadPoolExecutor@2d830ea9[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 27852]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:769)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:745)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/10/02 20:08:18 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@6a460ac4 rejected from java.util.concurrent.ThreadPoolExecutor@5fa09d7b[Shutting down, pool size = 2, active threads = 2, queued tasks = 0, completed tasks = 27854]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.executor.Executor.launchTask(Executor.scala:270)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/10/02 20:08:18 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$anon$3@2854c735 rejected from java.util.concurrent.ThreadPoolExecutor@2d830ea9[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 27852]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueSuccessfulTask(TaskResultGetter.scala:61)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:769)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:745)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/10/02 20:08:18 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@402ffb0f rejected from java.util.concurrent.ThreadPoolExecutor@5fa09d7b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 27856]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2055)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:825)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1355)\n",
      "\tat org.apache.spark.executor.Executor.launchTask(Executor.scala:270)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:93)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:91)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:91)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:74)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "# sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5e13a542-1259-44f6-aa14-903ba0b17881",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_loader = spark.read.format(\"csv\") \\\n",
    "            .option(\"sep\", ' ')\n",
    "           #.option(\"header\", \"true\") \\\n",
    "           #.option(\"delimiter\", \"|\") \\\n",
    "           #.option(\"inferschema\", \"true\")\n",
    "for date, file in downloaded:\n",
    "    df = csv_loader.load(str(file))\n",
    "    df.show(1)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0e02e82d-2e17-4a70-9c9e-8462e91ee8f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"domain\", StringType(), True),\n",
    "    StructField(\"pagename\", StringType(), True),\n",
    "    StructField(\"count\", StringType(), True),\n",
    "   StructField(\"responsebytes\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "df = spark.read.option(\"sep\",\" \").csv(\"/mnt/group29/test.gz\", schema=schema)\n",
    "df.show(1)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "eda",
   "notebookOrigID": 1672146355091320,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
