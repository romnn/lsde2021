{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bd2314-ad10-46d5-bf6e-0c5b4aed1705",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bz2\n",
    "import csv\n",
    "import io\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import List, Dict\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import lsde2021.csv as csvutils\n",
    "import lsde2021.utils as utils\n",
    "import lsde2021.download as dl\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3df94a6-9826-4781-80b4-0f5c1f542eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_MEMORY = \"30G\"\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"parse-wikipedia-sql-dumps\") \\\n",
    "    .config(\"spark.executor.memory\", MAX_MEMORY) \\\n",
    "    .config(\"spark.driver.memory\", MAX_MEMORY) \\\n",
    "    .config('spark.driver.maxResultSize', MAX_MEMORY) \\\n",
    "    .config('spark.ui.showConsoleProgress', 'false') \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "csv_loader = spark.read.format(\"csv\").options(header='True', inferSchema='True')\n",
    "parquet_reader = spark.read.format(\"parquet\").options(inferSchema='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe512810-339f-412b-9bba-3826d3686e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "pageview_complete_processed_src = Path(\"../nvme/pageview_complete_processed\")\n",
    "pageview_complete_per_topic_dest = Path(\"../nvme/pageview_complete_per_topic\")\n",
    "end_date = datetime.date(2021, 10, 1)\n",
    "\n",
    "daily_pageview_files = []\n",
    "for year in [2021]: # 2019, 2020, 2021]:\n",
    "    daily_range = list(dl.date_range(\n",
    "        datetime.date(year, 1, 1),\n",
    "        datetime.date(year, 12, 31),\n",
    "    ))\n",
    "    \n",
    "    daily_range = [d for d in daily_range if (end_date - d).total_seconds() > 0]\n",
    "    daily_pageview_files += daily_range\n",
    "\n",
    "daily_pageview_files = [datetime.date(2021, 4, 21), datetime.date(2021, 4, 22)]\n",
    "daily_pageview_files = [\n",
    "    (\n",
    "        pageview_complete_processed_src / Path(\"/\".join(dl.wikimedia_pageview_complete_local_file(date, monthly=False))).with_suffix(\".parquet\"),\n",
    "        pageview_complete_per_topic_dest / Path(\"/\".join(dl.wikimedia_pageview_complete_local_file(date, monthly=False))).with_suffix(\".parquet\"),\n",
    "    )\n",
    "    for date in daily_pageview_files\n",
    "]\n",
    "pprint(daily_pageview_files[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8008709c-c370-4306-a83f-5aecf71799b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pageview_processed_schema = T.StructType([\n",
    "    T.StructField(\"page_id\",T.IntegerType(), True),\n",
    "    T.StructField(\"dbname\", T.StringType(), True),\n",
    "    T.StructField(\"wiki_code\", T.StringType(), True),\n",
    "    T.StructField(\"page_title\", T.StringType(), True),\n",
    "    T.StructField(\"daily_total\", T.LongType(), True),\n",
    "    T.StructField(\"language\", T.StringType(), True),\n",
    "    T.StructField(\"code\", T.StringType(), True),\n",
    "    T.StructField(\"en_page_id\", T.IntegerType(), True),\n",
    "    # T.StructField(\"en_title\", T.StringType(), True),\n",
    "    T.StructField(\"topics1\", T.ArrayType(T.StringType()), True),\n",
    "    T.StructField(\"topics2\", T.ArrayType(T.StringType()), True),\n",
    "    T.StructField(\"topics3\", T.ArrayType(T.StringType()), True),\n",
    "    T.StructField(\"topics4\", T.ArrayType(T.StringType()), True),\n",
    "    T.StructField(\"group\", T.StringType(), True),\n",
    "])\n",
    "\n",
    "for daily_processed_file, daily_processed_per_topics_output_file in daily_pageview_files:\n",
    "    start = time.time()\n",
    "    # print(daily_processed_file, daily_processed_per_topics_output_file)\n",
    "    # break\n",
    "    \n",
    "    if daily_processed_per_topics_output_file.exists():\n",
    "        print(f\"using existing {daily_processed_per_topics_output_file}\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        df = spark.read.format(\"parquet\").schema(pageview_processed_schema).load(str(daily_processed_file))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(f\"skipping {daily_processed_file} ...\")\n",
    "        continue\n",
    "    \n",
    "    group_cols = [\"topic\", \"dbname\", \"wiki_code\", \"group\", \"language\"]\n",
    "    \n",
    "    topic1_counts = df.select(*group_cols, \"daily_total\", F.explode(\"topics1\").alias(\"topic\"))\n",
    "    topic1_counts = topic1_counts.groupBy(group_cols).agg(F.sum(\"daily_total\").alias(\"level1_daily_total\"), F.count(F.lit(1)).alias(\"level1_page_count\"))\n",
    "    \n",
    "    topic2_counts = df.select(*group_cols, \"daily_total\", F.explode(\"topics2\").alias(\"topic\"))\n",
    "    topic2_counts = topic2_counts.groupBy(group_cols).agg(F.sum(\"daily_total\").alias(\"level2_daily_total\"), F.count(F.lit(1)).alias(\"level2_page_count\"))\n",
    "    \n",
    "    topic3_counts = df.select(*group_cols, \"daily_total\", F.explode(\"topics3\").alias(\"topic\"))\n",
    "    topic3_counts = topic3_counts.groupBy(group_cols).agg(F.sum(\"daily_total\").alias(\"level3_daily_total\"), F.count(F.lit(1)).alias(\"level3_page_count\"))\n",
    "    \n",
    "    topic4_counts = df.select(*group_cols, \"daily_total\", F.explode(\"topics4\").alias(\"topic\"))\n",
    "    topic4_counts = topic4_counts.groupBy(group_cols).agg(F.sum(\"daily_total\").alias(\"level4_daily_total\"), F.count(F.lit(1)).alias(\"level4_page_count\"))\n",
    "    \n",
    "    topic_counts = topic1_counts\n",
    "    topic_counts = topic_counts.join(topic2_counts, on=group_cols, how=\"outer\")\n",
    "    topic_counts = topic_counts.join(topic3_counts, on=group_cols, how=\"outer\")\n",
    "    topic_counts = topic_counts.join(topic4_counts, on=group_cols, how=\"outer\")\n",
    "    \n",
    "    levels_daily_counts = [\"level1_daily_total\", \"level2_daily_total\", \"level3_daily_total\", \"level4_daily_total\"]\n",
    "    topic_counts = topic_counts.withColumn('num_levels', sum(topic_counts[col].isNotNull().cast('int') for col in levels_daily_counts))\n",
    "    topic_counts = topic_counts.filter((F.col(\"num_levels\") > 0) & (F.col(\"group\").isNotNull()))\n",
    "    \n",
    "    # test for pizza\n",
    "    # topic_counts.filter(F.lower(\"topic\") == \"pizza\").limit(100).show()\n",
    "    \n",
    "    # write out to parquet file, partitioned by the country code\n",
    "    topic_counts.write.format(\"parquet\").mode(\"overwrite\").partitionBy(\"group\").save(str(daily_processed_per_topics_output_file))\n",
    "    print(\"wrote %s in %.2f minutes\" % (daily_processed_per_topics_output_file, (time.time() - start) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23e2935-f96b-4fd7-8486-6596dc9b38fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
