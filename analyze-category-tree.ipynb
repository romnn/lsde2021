{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cffa67e-678f-4bc0-99d6-69cd54258145",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import bz2\n",
    "import csv\n",
    "import io\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "import concurrent.futures\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from functools import partial, reduce\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import List, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "import lsde2021.csv as csvutil\n",
    "import lsde2021.utils as utils\n",
    "from lsde2021.lang import singularize, pluralize\n",
    "import lsde2021.download as dl\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50d6c52-0d85-4b72-936d-530a84dc2877",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_MEMORY = \"30G\"\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"parse-wikipedia-sql-dumps\") \\\n",
    "    .config(\"spark.executor.memory\", MAX_MEMORY) \\\n",
    "    .config(\"spark.driver.memory\", MAX_MEMORY) \\\n",
    "    .config('spark.driver.maxResultSize', MAX_MEMORY) \\\n",
    "    .config('spark.ui.showConsoleProgress', 'false') \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "csv_loader = spark.read.format(\"csv\").options(header='True', inferSchema='True')\n",
    "parquet_reader = spark.read.format(\"parquet\").options(inferSchema='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f39ab0-7032-4143-9a8b-998666e831a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join categories with english wiki page table\n",
    "wiki = \"enwiki\"\n",
    "raw_pages = parquet_reader.load(str(f\"../nvme/wikipedia_sql_dumps/{wiki}/20211001/{wiki}-20211001-page-category-count.sql.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f448c1-a9c8-4a1a-bd5b-b78a9e73f240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first have a closer look at some of the categories and how they look like so we can split them eventually\n",
    "example_categories = raw_pages.select(\"category_name\").limit(1_000).rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62445b16-fbaf-42f2-8a89-ad3546fc7f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(example_categories[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61347da9-6b06-4c64-9647-aca4a8138161",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = nx.read_gpickle(f\"../nvme/en-category-tree.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac33d1c-86a3-4e56-b2a1-00dbce59d86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out all hidden categories by removing nodes that have an edge to the hidden category\n",
    "hidden_category = raw_pages \\\n",
    "    .filter((F.col(\"category_name\") == \"Hidden_categories\") & (F.col(\"page_namespace\") == 14))\n",
    "hidden_category.limit(100).show()\n",
    "\n",
    "hidden_category = hidden_category \\\n",
    "    .groupBy(\"category_page_id\") \\\n",
    "    .count()\n",
    "hidden_category.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93882e21-ba20-4972-98f9-9df0a85a45bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_category_node = 15961454\n",
    "\n",
    "hidden_sub_categories = list(nx.bfs_tree(graph, reverse=True, source=15961454, depth_limit=1))\n",
    "\n",
    "# 30259 for depth_limit=1\n",
    "# 6_838_612 for depth_limit=2\n",
    "print(len(hidden_sub_categories))\n",
    "pprint([graph.nodes[n] for n in hidden_sub_categories[-25:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3772bd-e77b-4973-a5e7-ac006600517f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove hidden topics and their edges from the graph\n",
    "print(\"edges before: %d | nodes before: %d\" % (len(graph.edges), len(graph.nodes)))\n",
    "graph.remove_nodes_from(hidden_sub_categories)\n",
    "print(\"edges after: %d | nodes after: %d\" % (len(graph.edges), len(graph.nodes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfc1eab-6f7e-4a7d-b8a9-eca6762a9531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the graph for reuse\n",
    "nx.write_gpickle(graph, f\"../nvme/en-category-tree-without-hidden.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252f723d-adb2-4cc0-a5aa-4e810e0319b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example case: find the COVID19 wikipedia article\n",
    "covid_article = raw_pages.filter(F.col(\"page_title\") == \"COVID-19\").limit(100)\n",
    "covid_article.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ae395d-4418-40e8-9159-bb9e4bbfba44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the content category\n",
    "root_category = raw_pages.filter((F.col(\"category_name\") == \"Content\") & (F.col(\"page_namespace\") == 14)).limit(100)\n",
    "root_category.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cc9557-7c37-4c98-9ba4-6cef92a1e5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find sinks in the graph (there should only be one)\n",
    "sinks = [node for node in graph.nodes if graph.out_degree(node) == 0 and graph.in_degree(node) > 0]\n",
    "print(len(sinks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fd7c75-2162-450a-a891-bd3d04259d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint([graph.nodes[n][\"title\"] for n in sinks[:20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874c2d3a-3e5b-470a-bc01-960fddd83904",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric = re.compile(r'^([\\s\\d]+)$')\n",
    "\n",
    "patterns = [\n",
    "    (re.compile(r\"^\\d+th-century_(\\w+)_in_the_(\\w+)$\"), []),\n",
    "    (re.compile(r\"^\\d+th-century_(\\w+)_in_(\\w+)$\"), []),\n",
    "    \n",
    "    (re.compile(r\"^\\d+s_in_the_(\\w+)$\"), []),\n",
    "    (re.compile(r\"^\\d+s_in_(\\w+)$\"), []),\n",
    "    (re.compile(r\"^\\d+_in_the_(\\w+)$\"), []),\n",
    "    (re.compile(r\"^\\d+_in_(\\w+)$\"), []),\n",
    "    \n",
    "    (re.compile(r\"^(\\w+)_based_in_(\\w+)_by_subject$\"), []),\n",
    "    \n",
    "    (re.compile(r\"^(\\w+)_established_in_the_(\\w+)$\"), []),\n",
    "    (re.compile(r\"^(\\w+)_established_in_(\\w+)$\"), []),\n",
    "    \n",
    "    (re.compile(r\"^(\\w+)_in_the_(\\w+)$\"), []),\n",
    "    (re.compile(r\"^(\\w+)_in_(\\w+)$\"), []),\n",
    "    \n",
    "    (re.compile(r\"^(\\w+)_and_the_(\\w+)$\"), []),\n",
    "    (re.compile(r\"^(\\w+)_and_(\\w+)$\"), []),\n",
    "    \n",
    "    (re.compile(r\"^(\\w+)_of_the_(\\w+)_by_country$\"), []),\n",
    "    (re.compile(r\"^(\\w+)_of_(\\w+)_by_country$\"), []),\n",
    "    (re.compile(r\"^(\\w+)_of_the_(\\w+)$\"), []),\n",
    "    (re.compile(r\"^(\\w+)_of_(\\w+)$\"), []),\n",
    "    \n",
    "    (re.compile(r\"^(\\w+)_by_country$\"), []),\n",
    "    (re.compile(r\"^(\\w+)_by_region$\"), []),\n",
    "    (re.compile(r\"^(\\w+)_by_location$\"), []),\n",
    "    (re.compile(r\"^(\\w+)_by_field$\"), []),\n",
    "    (re.compile(r\"^(\\w+)_by_location$\"), []),\n",
    "    (re.compile(r\"^(\\w+)_by_type$\"), []),\n",
    "    \n",
    "    (re.compile(r\"^\\d+_(\\w+)_by_legal_status$\"), []),\n",
    "    (re.compile(r\"^\\d+_(\\w+)_by_year$\"), []),\n",
    "    (re.compile(r\"^\\d+_(\\w+)_by_date$\"), []),\n",
    "    (re.compile(r\"^\\d+_(\\w+)_by_year_and_country$\"), []),\n",
    "    (re.compile(r\"^\\d+_(\\w+)_by_country_and_year$\"), []),\n",
    "    (re.compile(r\"^\\d+_(\\w+)_by_country$\"), []),\n",
    "    (re.compile(r\"^\\d+_(\\w+)_by_continent$\"), []),\n",
    "    (re.compile(r\"^\\d+_(\\w+)_by_decade$\"), []),\n",
    "    (re.compile(r\"^\\d+_(\\w+)_by_date$\"), []),\n",
    "    (re.compile(r\"^\\d+_(\\w+)_by_(\\w+)$\"), []),\n",
    "    \n",
    "    \n",
    "    (re.compile(r\"^(\\w+)_by_legal_status$\"), []),\n",
    "    (re.compile(r\"^(\\w+)_by_year$\"), []),\n",
    "    (re.compile(r\"^(\\w+)_by_date$\"), []),\n",
    "    (re.compile(r\"^(\\w+)_by_year_and_country$\"), []),\n",
    "    (re.compile(r\"^(\\w+)_by_country_and_year$\"), []),\n",
    "    (re.compile(r\"^(\\w+)_by_country$\"), []),\n",
    "    (re.compile(r\"^(\\w+)_by_continent$\"), []),\n",
    "    (re.compile(r\"^(\\w+)_by_decade$\"), []),\n",
    "    (re.compile(r\"^(\\w+)_by_date$\"), []),\n",
    "    (re.compile(r\"^(\\w+)_by_(\\w+)$\"), []),\n",
    "    \n",
    "    (re.compile(r\"^\\d+_(\\w+)$\"), []),\n",
    "]\n",
    "\n",
    "test_str = 'Companies_by_date'\n",
    "for pattern, extra_words in patterns:\n",
    "    match = pattern.fullmatch(test_str)\n",
    "    if match:\n",
    "        print(list(match.groups()))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5203763-0a87-4837-8ab1-0bf4fe22a65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['a', 'about', 'above', 'across', 'after', 'afterwards']\n",
    "stopwords += ['again', 'against', 'all', 'almost', 'alone', 'along']\n",
    "stopwords += ['already', 'also', 'although', 'always', 'am', 'among']\n",
    "stopwords += ['amongst', 'amoungst', 'amount', 'an', 'and', 'another']\n",
    "stopwords += ['any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere']\n",
    "stopwords += ['are', 'around', 'as', 'at', 'back', 'be', 'became']\n",
    "stopwords += ['because', 'become', 'becomes', 'becoming', 'been']\n",
    "stopwords += ['before', 'beforehand', 'behind', 'being', 'below']\n",
    "stopwords += ['beside', 'besides', 'between', 'beyond', 'bill', 'both']\n",
    "stopwords += ['bottom', 'but', 'by', 'call', 'can', 'cannot', 'cant']\n",
    "stopwords += ['co', 'computer', 'con', 'could', 'couldnt', 'cry', 'de']\n",
    "stopwords += ['describe', 'detail', 'did', 'do', 'done', 'down', 'due']\n",
    "stopwords += ['during', 'each', 'eg', 'eight', 'either', 'eleven', 'else']\n",
    "stopwords += ['elsewhere', 'empty', 'enough', 'etc', 'even', 'ever']\n",
    "stopwords += ['every', 'everyone', 'everything', 'everywhere', 'except']\n",
    "stopwords += ['few', 'fifteen', 'fifty', 'fill', 'find', 'fire', 'first']\n",
    "stopwords += ['five', 'for', 'former', 'formerly', 'forty', 'found']\n",
    "stopwords += ['four', 'from', 'front', 'full', 'further', 'get', 'give']\n",
    "stopwords += ['go', 'had', 'has', 'hasnt', 'have', 'he', 'hence', 'her']\n",
    "stopwords += ['here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers']\n",
    "stopwords += ['herself', 'him', 'himself', 'his', 'how', 'however']\n",
    "stopwords += ['hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed']\n",
    "stopwords += ['interest', 'into', 'is', 'it', 'its', 'itself', 'keep']\n",
    "stopwords += ['last', 'latter', 'latterly', 'least', 'less', 'ltd', 'made']\n",
    "stopwords += ['many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine']\n",
    "stopwords += ['more', 'moreover', 'most', 'mostly', 'move', 'much']\n",
    "stopwords += ['must', 'my', 'myself', 'name', 'namely', 'neither', 'never']\n",
    "stopwords += ['nevertheless', 'next', 'nine', 'no', 'nobody', 'none']\n",
    "stopwords += ['noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'of']\n",
    "stopwords += ['off', 'often', 'on','once', 'one', 'only', 'onto', 'or']\n",
    "stopwords += ['other', 'others', 'otherwise', 'our', 'ours', 'ourselves']\n",
    "stopwords += ['out', 'over', 'own', 'part', 'per', 'perhaps', 'please']\n",
    "stopwords += ['put', 'rather', 're', 's', 'same', 'see', 'seem', 'seemed']\n",
    "stopwords += ['seeming', 'seems', 'serious', 'several', 'she', 'should']\n",
    "stopwords += ['show', 'side', 'since', 'sincere', 'six', 'sixty', 'so']\n",
    "stopwords += ['some', 'somehow', 'someone', 'something', 'sometime']\n",
    "stopwords += ['sometimes', 'somewhere', 'still', 'such', 'system', 'take']\n",
    "stopwords += ['ten', 'than', 'that', 'the', 'their', 'them', 'themselves']\n",
    "stopwords += ['then', 'thence', 'there', 'thereafter', 'thereby']\n",
    "stopwords += ['therefore', 'therein', 'thereupon', 'these', 'they']\n",
    "stopwords += ['thick', 'thin', 'third', 'this', 'those', 'though', 'three']\n",
    "stopwords += ['three', 'through', 'throughout', 'thru', 'thus', 'to']\n",
    "stopwords += ['together', 'too', 'top', 'toward', 'towards', 'twelve']\n",
    "stopwords += ['twenty', 'two', 'un', 'under', 'until', 'up', 'upon']\n",
    "stopwords += ['us', 'very', 'via', 'was', 'we', 'well', 'were', 'what']\n",
    "stopwords += ['whatever', 'when', 'whence', 'whenever', 'where']\n",
    "stopwords += ['whereafter', 'whereas', 'whereby', 'wherein', 'whereupon']\n",
    "stopwords += ['wherever', 'whether', 'which', 'while', 'whither', 'who']\n",
    "stopwords += ['whoever', 'whole', 'whom', 'whose', 'why', 'will', 'with']\n",
    "stopwords += ['within', 'without', 'would', 'yet', 'you', 'your']\n",
    "stopwords += ['yours', 'yourself', 'yourselves']\n",
    "\n",
    "EXCLUDE = set(stopwords).union({\"by\",\"or\",\"and\",\"with\",\"the\",\"of\",\"in\",\"without\",\"a\",\"on\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91341a60-e20c-4155-ab58-085902624d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def unique(l, key):\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    return [x for x in l if not (key(x) in seen or seen_add(key(x)))]\n",
    "\n",
    "def union(dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "def is_uppercase(s: str):\n",
    "    return s[0].isupper()\n",
    "\n",
    "def split_by_pattern(s: str) -> List[str]:\n",
    "    for pattern, extra_words in patterns:\n",
    "        match = re.fullmatch(pattern, s)\n",
    "        if match:\n",
    "            return list(match.groups()), True\n",
    "    return [s], False\n",
    "\n",
    "def split(s: str, split_unmatched=False, singularize=False, pluralize=False, recursive=False):\n",
    "    # first, test for common patterns\n",
    "    splitted, matched = split_by_pattern(s)\n",
    "    # print(splitted)\n",
    "    \n",
    "    # split recursively\n",
    "    rec_splitted = flatten([split_by_pattern(ss)[0] for ss in splitted])\n",
    "    # print(rec_splitted)\n",
    "    while recursive and set(splitted) != set(rec_splitted):\n",
    "        splitted = rec_splitted[:]\n",
    "        # print(splitted)\n",
    "        rec_splitted = flatten([split_by_pattern(ss)[0] for ss in splitted])\n",
    "            \n",
    "    if not matched:\n",
    "        if split_unmatched:\n",
    "            # if no pattern is found, split and remove stopwords\n",
    "            splitted += re.split(' |,|_', s)\n",
    "        else:\n",
    "            splitted = [s]\n",
    "    \n",
    "    splitted = set([sp.replace(\"_\", \" \") for sp in splitted if numeric.match(sp) is None])\n",
    "    # print(s, splitted, matched)\n",
    "    \n",
    "    if singularize and pluralize:\n",
    "        splitted = set([singularize(sp) for sp in splitted]).union(set([pluralize(sp) for sp in splitted]))\n",
    "    elif singularize:\n",
    "        splitted = set([singularize(sp) for sp in splitted])\n",
    "    elif pluralize:\n",
    "        splitted = set([pluralize(sp) for sp in splitted])\n",
    "    splitted = splitted - EXCLUDE\n",
    "    return splitted\n",
    "\n",
    "def split_all(s: str):\n",
    "    return split(s, split_unmatched=True)\n",
    "\n",
    "def bfs_tree(g, node, depth_limit=None):\n",
    "    ans = []\n",
    "    visited = set()\n",
    "    level = [(node, 0)]\n",
    "    while len(level) > 0:\n",
    "        for v, depth in level:\n",
    "            ans.append((v, depth))\n",
    "            visited.add(v)\n",
    "        next_level = set()\n",
    "        for v, depth in level:\n",
    "            for w in g.neighbors(v):\n",
    "                if w not in visited:\n",
    "                    next_level.add((w, depth + 1))\n",
    "        level = next_level\n",
    "    return ans\n",
    "\n",
    "def freq_bfs_tree(g, node, depth_limit=None):\n",
    "    ans = []\n",
    "    counts = dict()\n",
    "    visited = set()\n",
    "    level = [(node, 0)]\n",
    "    while len(level) > 0:\n",
    "        for v, depth in level:\n",
    "            ans.append((v, depth))\n",
    "            visited.add(v)\n",
    "            counts[v] = 1\n",
    "        next_level = set()\n",
    "        for v, depth in level:\n",
    "            for w in g.neighbors(v):\n",
    "                if w in visited:\n",
    "                    counts[v] += 1\n",
    "                elif depth_limit is None or depth + 1 <= depth_limit:\n",
    "                    next_level.add((w, depth + 1))\n",
    "        level = next_level\n",
    "    \n",
    "    levels = dict()\n",
    "    for n, depth in ans:\n",
    "        if depth not in levels:\n",
    "            levels[depth] = []\n",
    "        levels[depth].append((n, counts[n]))\n",
    "    \n",
    "    levels = {depth: sorted(nodes, key=lambda x: x[1], reverse=True) for depth, nodes in levels.items()}\n",
    "    return levels\n",
    "    # return [(n, depth, counts[n]) for n, depth in ans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85e013f-29ff-47e2-a6ff-24f873799116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_topics(node, g, depth_limit: int = 4, max_categories: int = 5) -> Dict[int, List[int]]:\n",
    "    categories = freq_bfs_tree(g, node, depth_limit=depth_limit)\n",
    "    if False:\n",
    "        pprint({\n",
    "            depth: [(g.nodes[n][\"title\"], n, count) for n, count in nodes]\n",
    "            for depth, nodes in categories.items() if depth > 0\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        depth: unique(flatten([\n",
    "            [w.capitalize() for w in split(g.nodes[n][\"title\"], recursive=True)]\n",
    "            for n, count in nodes\n",
    "        ]), key=lambda x: x[0])[:max_categories] for depth, nodes in categories.items() if depth > 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bff8e22-7c0e-486b-8248-93f2d257aa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "depth_limit = 4\n",
    "n_categories = 5\n",
    "page_id = 63030231 # covid 19\n",
    "# page_id = 11867 # germany\n",
    "# page_id = 24365 # porsche\n",
    "\n",
    "pprint(find_topics(page_id, g=graph, depth_limit=4, max_categories=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344d2567-52dd-4ec5-bf10-c5642fe65f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_page_ids =  list(enumerate(sorted(raw_pages.filter(F.col(\"page_namespace\") == 0).select(\"page_id\").distinct().rdd.flatMap(lambda x: x).collect())))\n",
    "print(len(all_page_ids))\n",
    "with open(\"../nvme/en_topics/all_page_ids.pkl\", 'wb') as f:\n",
    "    pkl.dump(all_page_ids, f, protocol=pkl.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1723990-d377-4f15-b7ff-e682da38bcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict()\n",
    "savepoint = dict()\n",
    "save_every = 5_000\n",
    "\n",
    "bk_dir = Path(\"../nvme/en_topics/savepoints\")\n",
    "bk_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "start = time.time()\n",
    "for i, page_id in tqdm(all_page_ids, bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}'):\n",
    "    results[page_id] = find_topics(page_id, g=graph, depth_limit=4, max_categories=5)\n",
    "    \n",
    "    savepoint[page_id] = results[page_id] # .copy()\n",
    "    if i >= save_every and i % save_every == 0:\n",
    "        with open(bk_dir / f\"page_topics_{i-save_every}_{i}.pkl\", 'wb') as f:\n",
    "            pkl.dump(savepoint, f, protocol=pkl.HIGHEST_PROTOCOL)\n",
    "        savepoint = dict()\n",
    "\n",
    "print(len(results))\n",
    "print(\"took %.2f hours\" % ((time.time() - start)/(60*60)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e523d5-ceca-42bc-af80-71bfd6dc5d81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61682d15-7d65-46b5-8260-268c4c70a877",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_parallel = multiprocessing.cpu_count()\n",
    "n_parallel = 1\n",
    "\n",
    "test_page_ids = all_page_ids[:1_000_000]\n",
    "\n",
    "# results = dict()\n",
    "results = []\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=n_parallel) as executor:\n",
    "    for i, test enumerate(executor.map(partial(find_topics, g=graph, depth_limit=4, max_categories=5), test_page_ids)):\n",
    "        results.append(test)\n",
    "    \n",
    "        \n",
    "        # results[page_id] = topics\n",
    "    #for page_id, topics in zip(\n",
    "    #    test_page_ids,\n",
    "    #    executor.map(partial(find_topics, g=graph, depth_limit=4, max_categories=5), test_page_ids)\n",
    "    #):\n",
    "    #    results[page_id] = topics\n",
    "\n",
    "print(len(results))\n",
    "# pprint(list(results.items())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168bc9c2-11dd-4b7c-ba83-297f8ecfb233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_topics_worker(page_ids, g, depth_limit, max_categories):\n",
    "    return {\n",
    "        page_id: find_topics(page_id, g=g, depth_limit=depth_limit, max_categories=max_categories)\n",
    "        for page_id in page_ids\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a517e9-fd75-42ec-bf5d-d0683e406942",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_parallel = 1 # multiprocessing.cpu_count()\n",
    "chunk_size = int(np.ceil(len(all_page_ids[:100]) / n_parallel))\n",
    "tasks = []\n",
    "start = time.time()\n",
    "\n",
    "# initializer=set_global, initargs=(graph,)\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=n_parallel) as executor:\n",
    "    # executor.map(partial(find_topics, g=graph, depth_limit=4, max_categories=5), worker_page_ids)\n",
    "    for worker_id in range(n_parallel):\n",
    "        worker_page_ids = all_page_ids[worker_id * chunk_size: (worker_id + 1) * chunk_size]\n",
    "        print(worker_page_ids[:5])\n",
    "        print(\"worker %d got assigned %d page ids\" % (worker_id, len(worker_page_ids)))\n",
    "        tasks.append(executor.submit(partial(find_topics_worker, g=graph, depth_limit=4, max_categories=5), worker_page_ids))\n",
    "\n",
    "# collect the results\n",
    "results = dict()\n",
    "for worker_id, task in enumerate(tasks):\n",
    "    results.update(task.result())\n",
    "    # print(job)\n",
    "    #for page_id, topics in zip(*job):\n",
    "    #    results[page_id] = topics   \n",
    "    # print(r)\n",
    "    # cur_revids, cur_topics = proc.result()\n",
    "    # all_revids.update(cur_revids)\n",
    "    # all_topics.update(cur_topics)\n",
    "    print(\"worker %d done\" % worker_id)\n",
    "\n",
    "print(\"took %.2f hours\" % ((time.time() - start)/(60*60)))\n",
    "print(len(results))\n",
    "pprint(list(results.items())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ef2b0b-3290-495d-a2da-3e05f5e996ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo build a udf for looking up topics and join with the page table\n",
    "# find_topics_udf = F.udf(partial(find_topics, g=graph, depth_limit=4, max_categories=5), T.ArrayType(T.StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657b0c11-8db7-4b72-a2bd-315e03c4bb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_pages \\\n",
    "    .filter(F.col(\"page_title\") == \"COVID-19\") \\\n",
    "    .withColumn(\"ores_topics\", find_topics_udf(raw_pages['page_id'])) \\\n",
    "    .limit(10) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5bf39a-1db0-406b-8670-9d08ca011399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604417cc-288d-4f69-a36f-c9682201eccb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a7428e-8285-4656-8efe-30c3530482fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6f725f-1a1c-4f40-ab5c-2673fb8519d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8531145-130a-4e1b-a279-64d6b9bfdc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deprecated\n",
    "def find_topics_old(g, node: int, max_level: int = 4, topic_count = 5) -> Dict[int, List[int]]:\n",
    "    start = time.time()\n",
    "    distances = range(0, max_level+1)\n",
    "    topics = {dist: set() for dist in distances}\n",
    "    primary_topic_nodes = {\n",
    "        dist: [n for n in nx.descendants_at_distance(g, node, distance=dist)]\n",
    "        for dist in distances\n",
    "    }\n",
    "    primary_topics = {\n",
    "        dist: flatten([split(graph.nodes[n][\"title\"]) for n in nodes])\n",
    "        # dist: ([graph.nodes[n][\"title\"] for n in nodes])\n",
    "        for dist, nodes in primary_topic_nodes.items()\n",
    "    }\n",
    "    # pprint(primary_topics)\n",
    "    \n",
    "    # we make the following assumptions:\n",
    "    # - single word low level topics are usually the most fitting\n",
    "    # - splitting multi word low level topics and filtering those that are also high level categories is a good high level category\n",
    "    # - among the high level categories, we can choose the ones with the highest frequency that are also reachable from other pages\n",
    "    \n",
    "    # pprint(primary_topics)\n",
    "    low_level_topics = [title for title in primary_topics[1]]\n",
    "    flattened_low_level_topics = set(flatten(low_level_topics))\n",
    "    \n",
    "    common_parent_topics = {\n",
    "        dist: {\n",
    "            graph.nodes[node][\"title\"]: set(flatten([split(graph.nodes[n][\"title\"]) for n in nx.bfs_tree(g, node, depth_limit=2)]))\n",
    "            for node in nodes\n",
    "        }\n",
    "        for dist, nodes in primary_topic_nodes.items()\n",
    "    }\n",
    "    \n",
    "    common_parent_topics_counts = {\n",
    "        dist: Counter(flatten([\n",
    "            [tt.lower() for tt in t]\n",
    "            for t in parent_topics.values()\n",
    "        ]))\n",
    "        for dist, parent_topics in common_parent_topics.items()\n",
    "    }\n",
    "    \n",
    "    common_parent_topics_scores = {\n",
    "        dist: sorted([\n",
    "            (title, np.mean([0.0] + [{**common_parent_topics_counts[0], **common_parent_topics_counts[1]}.get(tt.lower(), 0) for tt in t]))\n",
    "            for title, t in parent_topics.items()\n",
    "        ], key=lambda x: x[1], reverse=True)\n",
    "        for dist, parent_topics in common_parent_topics.items()\n",
    "    }\n",
    "    \n",
    "    # choose single words with uppercase\n",
    "    # for the others, join pluralize and singularize and check if we find the high level topics\n",
    "    \n",
    "    # pprint(low_level_topics)\n",
    "    # pprint(flattened_low_level_topics)\n",
    "    pprint(primary_topics[1])\n",
    "    pprint(common_parent_topics_scores[1])\n",
    "    \n",
    "    # add single word low level categories\n",
    "    # single_word_topics = [list(topics)[0] for topics in low_level_topics if len(topics) == 1]\n",
    "    # and len(split(topic)) == 1\n",
    "    single_word_topics = set([topic for topic in flattened_low_level_topics if is_uppercase(topic)])\n",
    "    topics[1] = topics[1].union(single_word_topics)\n",
    "    \n",
    "    for split_part_topic in flattened_low_level_topics - single_word_topics:\n",
    "        print(split_part_topic.lower())\n",
    "        for dist in distances:\n",
    "            if dist < 2:\n",
    "                continue\n",
    "            # print([t.lower() for t in primary_topics[dist]])\n",
    "            for t in primary_topics[dist]:\n",
    "                if t.lower() == split_part_topic.lower():\n",
    "                    topics[dist].add(t) # use t as it might be uppercased\n",
    "            # if split_part_topic.lower() in [t.lower() for t in primary_topics[dist]]:\n",
    "            #     topics[dist].add(split_part_topic)\n",
    "    \n",
    "    # [:int(topic_count/2)]\n",
    "    # check \n",
    "    \n",
    "    print(\"took %.2f seconds\" % (time.time() - start))\n",
    "    # pprint(topics)\n",
    "    return topics\n",
    "\n",
    "# pprint(find_topics(graph, 63030231)) # covid 19\n",
    "# pprint(find_topics(graph, 62304)) # pfizer\n",
    "pprint(find_topics(graph, 1092923)) # google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af5d718-0302-444c-a226-b669e1244c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deprecated\n",
    "# todo: find other pages from the splitted words\n",
    "    # covid_article = raw_pages.filter(F.col(\"page_title\") == \"COVID-19\").limit(100)\n",
    "    similar_articles = []\n",
    "    for low_level_topic in flattened_low_level_topics:\n",
    "        similar_articles.append(raw_pages.filter(F.lower(\"page_title\").contains(low_level_topic.lower())).limit(100))\n",
    "    similar_articles = union(similar_articles)\n",
    "    \n",
    "    # similar_articles = raw_pages.filter(F.lower(\"page_title\").isin(flattened_low_level_topics))\n",
    "    \n",
    "    print(similar_articles.count())\n",
    "    # similar_articles.show()\n",
    "    \n",
    "    # find the categories for similar pages\n",
    "    other_page_topics = {dist: dict() for dist in distances}\n",
    "    for similar_page_id in similar_articles.select(\"page_id\").distinct().rdd.flatMap(lambda x: x).collect(): # .rdd.toLocalIterator():\n",
    "        # print(similar_page_id)\n",
    "        # similar_page_id = similar_page_row[\"page_id\"]\n",
    "        if similar_page_id not in g.nodes:\n",
    "            continue\n",
    "        for dist in distances:\n",
    "            for similar_page_topic in [\n",
    "                (n, graph.nodes[n][\"title\"]) for n in nx.descendants_at_distance(g, similar_page_id, distance=dist)\n",
    "            ]:\n",
    "                # print(similar_page_topic)\n",
    "                if similar_page_topic not in other_page_topics[dist]:\n",
    "                    other_page_topics[dist][similar_page_topic] = 0\n",
    "                other_page_topics[dist][similar_page_topic] += 1\n",
    "    \n",
    "    other_page_topic_counts = {\n",
    "        dist: sorted([(n, title, count) for (n, title), count in other_page_topics[dist].items()], key=lambda x: x[2], reverse=True)\n",
    "        for dist in distances\n",
    "    }\n",
    "    top5_other_page_topic_counts = {\n",
    "        dist: topics[:10]\n",
    "        for dist, topics in other_page_topic_counts.items()\n",
    "    }\n",
    "    pprint(top5_other_page_topic_counts)\n",
    "    # for dist in distances:\n",
    "    #     (n, title, count) for (n, title), count in other_page_topics[dist].items()\n",
    "    # pprint(other_page_topics)\n",
    "    # other_page = {\n",
    "    #     dist: [graph.nodes[n] for n in nx.descendants_at_distance(g, node, distance=dist)]\n",
    "    #     for dist in range(1, max_level+1)\n",
    "    # }\n",
    "    \n",
    "    # pprint(primary_topics)\n",
    "    print(\"took %.2f seconds\" % (time.time() - start))\n",
    "    return topics\n",
    "    # for dist in range(1, 5):\n",
    "    #     high_level_categories = [graph.nodes[n] for n in nx.descendants_at_distance(graph, 63030231, distance=dist)]\n",
    "    #     print(\"==== d %d\" % dist)\n",
    "    #     pprint(high_level_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2991f6-4263-4ec6-9bee-95b4234e4a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deprecated\n",
    "depth_limit = 4\n",
    "n_categories = 5\n",
    "page_id = 63030231 # covid 19\n",
    "page_id = 11867 # germany\n",
    "page_id = 24365 # porsche\n",
    "\n",
    "\n",
    "all_covid_categories1 = list(nx.dfs_tree(graph, page_id, depth_limit=depth_limit))\n",
    "# all_covid_categories2 = list(bfs_tree(graph, 63030231, depth_limit=depth_limit))\n",
    "# print(len(all_covid_categories1), len(all_covid_categories2))\n",
    "# assert len(all_covid_categories1) == len(all_covid_categories2)\n",
    "all_covid_categories2 = bfs_tree2(graph, page_id, depth_limit=depth_limit)\n",
    "\n",
    "# print(\"highest level root categories\")\n",
    "# pprint([(graph.nodes[n][\"title\"], depth, count) for n, depth, count in all_covid_categories2 if graph.out_degree(n) == 0])\n",
    "\n",
    "# max_depth = np.max([depth for _, depth, count in all_covid_categories2])\n",
    "# print(\"max depth\", max_depth)\n",
    "\n",
    "# print(\"highest level root categories\")\n",
    "# pprint([(graph.nodes[n][\"title\"], depth, count) for n, depth, count in all_covid_categories2 if 30 <= depth <= 60])\n",
    "\n",
    "# print(\"lower level categories\")\n",
    "# pprint([(graph.nodes[n][\"title\"], depth, count) for n, depth, count in all_covid_categories2[:100]])\n",
    "# print(covid_page_node) # node count really refers to how many categories does a page have assigned to it\n",
    "\n",
    "# test = {depth: [] for depth in range(1, depth_limit)}\n",
    "# for n, depth, count in all_covid_categories2:\n",
    "#     if depth < 1:\n",
    "#         continue\n",
    "#     test[depth].append((n, count))\n",
    "\n",
    "# pprint({depth: [(graph.nodes[n][\"title\"], n, count) for n, count in nodes] for depth, nodes in all_covid_categories2.items() if depth > 0})\n",
    "\n",
    "pprint({\n",
    "    depth: flatten([\n",
    "        [(w.capitalize(), n) for w in split(graph.nodes[n][\"title\"])]\n",
    "        for n, count in nodes\n",
    "    ])[:n_categories] for depth, nodes in all_covid_categories2.items() if depth > 0\n",
    "})\n",
    "# pprint({depth: (graph.nodes[n][\"title\"], depth, count) for n, depth, count in all_covid_categories2[:100]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
