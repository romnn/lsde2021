{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9f3f13-fe27-4e6a-b1cb-c63cc84635ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pip install colour ruptures\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bz2\n",
    "import csv\n",
    "import io\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from matplotlib import ticker\n",
    "from colour import Color\n",
    "import ruptures as rpt\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import List, Dict\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import lsde2021.csv as csvutils\n",
    "import lsde2021.utils as utils\n",
    "import lsde2021.download as dl\n",
    "import lsde2021.changepoints as cp\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.rcParams[\"figure.figsize\"] = (20,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6372447b-1db1-467d-8c3e-f3856400812c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_MEMORY = \"30G\"\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"parse-wikipedia-sql-dumps\") \\\n",
    "    .config(\"spark.executor.memory\", MAX_MEMORY) \\\n",
    "    .config(\"spark.driver.memory\", MAX_MEMORY) \\\n",
    "    .config('spark.driver.maxResultSize', MAX_MEMORY) \\\n",
    "    .config('spark.ui.showConsoleProgress', 'false') \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7204bb-4e73-41c2-b3cb-9c4a9f9dbebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stringency = spark.read.format(\"parquet\").load(f\"../nvme/oxcgrt-covid-policy-tracker/OxCGRT_withnotes.parquet\")\n",
    "languages = spark.read.format(\"parquet\").load(f\"./data/languages.parquet\")\n",
    "# languages.show()\n",
    "# stringency.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68f7029-4421-4da9-99f5-b89c40f739c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_groups = list(reversed(languages.select(\"group\").distinct().rdd.flatMap(lambda x: x).collect()))\n",
    "print(language_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ef0b37-d0fb-4707-9435-2b1582f0e7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_stringency, changepoints = cp.get_change_points(stringency, \"Germany\")\n",
    "country_stringency.show()\n",
    "cp.plot_changepoints(\n",
    "    country_stringency.select(\"Date\", \"StringencyIndex\").toPandas().set_index(\"Date\"),\n",
    "    changepoints,\n",
    "    title=\"Changepoints based on stringency index for {Germany}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e69fa32-7bae-4d8c-8832-ea4c74cac5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_stringency(\n",
    "    s,\n",
    "    pageviews,\n",
    "    label=\"Page views\",\n",
    "    xlabel=\"time\",\n",
    "    ylabel1=\"stringency index\",\n",
    "    ylabel2=\"absolute page views\",\n",
    "    title=None,\n",
    "    smoothing = 50,\n",
    "    fontsize=15,\n",
    "    figsize=(12, 7),\n",
    "    savefig=None\n",
    "):\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax2 = ax.twinx()\n",
    "    \n",
    "    ax.tick_params(axis='both', which='major', labelsize=fontsize * 3/4)\n",
    "    ax.tick_params(axis='both', which='minor', labelsize=fontsize * 3/4)\n",
    "    \n",
    "    ax.set_xlabel(xlabel, labelpad=20, size=fontsize)\n",
    "    ax.set_ylabel(ylabel1, labelpad=20, size=fontsize)\n",
    "    ax2.set_ylabel(ylabel2, labelpad=20, size=fontsize)\n",
    "    \n",
    "    assert len(pageviews) > smoothing\n",
    "    \n",
    "    ax.plot(s.index[:len(pageviews)-smoothing], s[\"StringencyIndex\"][:len(pageviews)-smoothing], label=\"Stringency Index\", color=\"gray\", linestyle='--', linewidth=2)\n",
    "    ax2.plot(pageviews.index[:-smoothing], utils.smoothing_window(pageviews[\"page_views\"], radius=smoothing), label=label, color=\"blue\", linestyle='-', linewidth=3)\n",
    "\n",
    "    plt.legend(fontsize=fontsize)\n",
    "    if title:\n",
    "        plt.title(title, fontsize=fontsize, pad=20)\n",
    "    plt.tight_layout()\n",
    "    if savefig:\n",
    "        Path(savefig).parent.mkdir(parents=True, exist_ok=True)\n",
    "        plt.savefig(savefig)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec33467-eb25-4123-826e-6782fe9d7423",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = [\"disambiguation\", \"wikipedia\", \"main topic articles\", \"stubs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a9f5ec-c074-4fde-9687-aadc4f2a1e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def did(year_df, changepoint, control_changepoint, topic_level=4, window_size=relativedelta(days=10), relative=False):\n",
    "    # control_changepoint = changepoint.replace(year=2019)\n",
    "    control_start, control_end = control_changepoint - window_size, control_changepoint + window_size\n",
    "    target_start, target_end = changepoint - window_size, changepoint + window_size\n",
    "\n",
    "    # control_days = list(dl.date_range(control_start, control_end))\n",
    "    # target_days = list(dl.date_range(target_start, target_end))\n",
    "    # print(target_start, target_end)\n",
    "    # print(control_start, control_end)\n",
    "    \n",
    "    # exlude known bad topics\n",
    "    year_df = year_df.filter(~F.lower(\"topic\").rlike(\"|\".join([\"(\" + pat + \")\" for pat in exclude])))\n",
    "    \n",
    "    pre_target_mean = year_df \\\n",
    "        .filter((F.lit(target_start) <= F.col(\"date\")) & (F.col(\"date\") <= F.lit(changepoint))) \\\n",
    "        .fillna(0) \\\n",
    "        .groupBy(\"topic\", \"dbname\", \"wiki_code\", \"language\") \\\n",
    "        .agg(F.mean(f\"level{topic_level}_daily_total\").alias(\"pre_target_mean\"))\n",
    "\n",
    "    pre_control_mean = year_df \\\n",
    "        .filter((F.lit(control_start) <= F.col(\"date\")) & (F.col(\"date\") <= F.lit(control_changepoint))) \\\n",
    "        .fillna(0) \\\n",
    "        .groupBy(\"topic\", \"dbname\", \"wiki_code\", \"language\") \\\n",
    "        .agg(F.mean(f\"level{topic_level}_daily_total\").alias(\"pre_control_mean\"))\n",
    "\n",
    "    post_target_mean = year_df \\\n",
    "        .filter((F.lit(changepoint) <= F.col(\"date\")) & (F.col(\"date\") <= F.lit(target_end))) \\\n",
    "        .fillna(0) \\\n",
    "        .groupBy(f\"topic\", \"dbname\", \"wiki_code\", \"language\") \\\n",
    "        .agg(F.mean(f\"level{topic_level}_daily_total\").alias(\"post_target_mean\"))\n",
    "\n",
    "    post_control_mean = year_df \\\n",
    "        .filter((F.lit(control_changepoint) <= F.col(\"date\")) & (F.col(\"date\") <= F.lit(control_end))) \\\n",
    "        .fillna(0) \\\n",
    "        .groupBy(f\"topic\", \"dbname\", \"wiki_code\", \"language\") \\\n",
    "        .agg(F.mean(f\"level{level}_daily_total\").alias(\"post_control_mean\")) \\\n",
    "\n",
    "    diff = pre_target_mean.join(pre_control_mean, on=[\"topic\", \"dbname\", \"wiki_code\", \"language\"], how=\"inner\")\n",
    "    diff = diff.join(post_target_mean, on=[\"topic\", \"dbname\", \"wiki_code\", \"language\"], how=\"inner\")\n",
    "    diff = diff.join(post_control_mean, on=[\"topic\", \"dbname\", \"wiki_code\", \"language\"], how=\"inner\")\n",
    "\n",
    "    diff = diff.withColumn('control_diff', ( F.col(\"pre_control_mean\") - F.col(\"post_control_mean\") ) )\n",
    "    diff = diff.withColumn('target_diff', ( F.col(\"post_target_mean\") - (F.col(\"pre_target_mean\") + F.col(\"control_diff\")) ) )\n",
    "    diff = diff.withColumn('rel_diff', ( F.col(\"post_target_mean\") / (F.col(\"pre_target_mean\") + F.col(\"control_diff\")) ) )\n",
    "    \n",
    "    round_digits = 10 if relative else 0\n",
    "    diff_key = \"rel_diff\" if relative else \"target_diff\"\n",
    "    # .sort(F.col(diff_key).desc())\n",
    "    diff = diff.select(\n",
    "        \"topic\",\n",
    "        # \"dbname\", \"wiki_code\", \"language\",\n",
    "        F.round(diff_key, round_digits).alias(\"diff\"),\n",
    "        F.round(\"pre_target_mean\", round_digits).alias(\"pre_target_mean\"),\n",
    "        F.round(\"pre_control_mean\", round_digits).alias(\"pre_control_mean\"),\n",
    "        F.round(\"post_target_mean\", round_digits).alias(\"post_target_mean\"),\n",
    "        F.round(\"post_control_mean\", round_digits).alias(\"post_control_mean\"),\n",
    "    )\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637b14e1-08c2-4874-9c91-0f102648cd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff(year_df, changepoint, control_changepoint, topic_level=4, window_size=relativedelta(days=10), relative=False):\n",
    "    pre_start, pre_end = changepoint - window_size, changepoint\n",
    "    post_start, post_end = changepoint, changepoint + window_size\n",
    "    print(pre_start, pre_end, post_start, post_end)\n",
    "    \n",
    "    # exlude known bad topics\n",
    "    year_df = year_df.filter(~F.lower(\"topic\").rlike(\"|\".join([\"(\" + pat + \")\" for pat in exclude])))\n",
    "    \n",
    "    pre_mean = year_df \\\n",
    "        .filter((F.lit(pre_start) <= F.col(\"date\")) & (F.col(\"date\") <= F.lit(pre_end))) \\\n",
    "        .fillna(0) \\\n",
    "        .groupBy(\"topic\", \"dbname\", \"wiki_code\", \"language\") \\\n",
    "        .agg(F.mean(f\"level{topic_level}_daily_total\").alias(\"pre_mean\"))\n",
    "\n",
    "    post_mean = year_df \\\n",
    "        .filter((F.lit(post_start) <= F.col(\"date\")) & (F.col(\"date\") <= F.lit(post_end))) \\\n",
    "        .fillna(0) \\\n",
    "        .groupBy(\"topic\", \"dbname\", \"wiki_code\", \"language\") \\\n",
    "        .agg(F.mean(f\"level{topic_level}_daily_total\").alias(\"post_mean\"))\n",
    "\n",
    "    diff = pre_mean.join(post_mean, on=[\"topic\", \"dbname\", \"wiki_code\", \"language\"], how=\"inner\")\n",
    "    \n",
    "    diff = diff.withColumn('rel_diff', ( F.col(\"post_mean\") / F.col(\"pre_mean\") ) )\n",
    "    diff = diff.withColumn('abs_diff', ( F.col(\"post_mean\") - F.col(\"pre_mean\") ) )\n",
    "    \n",
    "    round_digits = 10 if relative else 0\n",
    "    diff_key = \"rel_diff\" if relative else \"abs_diff\"\n",
    "    # .sort(F.col(diff_key).desc())\n",
    "    diff = diff.select(\n",
    "        \"topic\",\n",
    "        # \"dbname\", \"wiki_code\", \"language\",\n",
    "        F.round(diff_key, round_digits).alias(\"diff\"),\n",
    "        F.round(\"rel_diff\", round_digits).alias(\"rel_diff\"),\n",
    "        F.round(\"abs_diff\", round_digits).alias(\"abs_diff\"),\n",
    "    )\n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1e7987-ae3c-46f4-9113-6ea3dcbbfccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_topics(topics, fontsize=17, figsize=(12, 7), relative=False, risers=True, title=None, savefig=None):\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    \n",
    "    (pre_start, pre_end) = (Color(\"blue\"), Color(\"purple\"))\n",
    "    (post_start, post_end) = (Color(\"red\"), Color(\"orange\"))\n",
    "\n",
    "    y_pos = np.arange(len(topics))\n",
    "    \n",
    "    # print(y_pos)\n",
    "    # print(topics[\"topic\"].tolist())\n",
    "    \n",
    "    # print(len(colors), len(y_pos), len(topics))\n",
    "    # print(len(pre_start.range_to(pre_end,len(topics))))\n",
    "    # print(len(topics))\n",
    "    colors = [c.hex for c in pre_start.range_to(pre_end, len(topics))]\n",
    "    if risers:\n",
    "        colors = [c.hex for c in post_start.range_to(post_end,len(topics))]\n",
    "\n",
    "    ax.barh(0 + 2 * y_pos, topics[\"diff\"], color=colors, align='center')\n",
    "    # ax.barh(1 + 2 * y_pos, top_n[\"post_control_mean\"], color=post_colors, alpha=0.2, align='center')\n",
    "\n",
    "    ax.tick_params(axis=\"both\", labelsize=fontsize, which=\"both\", bottom=\"off\", top=\"off\", labelbottom=\"on\", left=\"off\", right=\"off\", labelleft=\"on\")\n",
    "\n",
    "    vals = ax.get_xticks()\n",
    "    for tick in vals:\n",
    "        ax.axvline(x=tick, linestyle='dashed', alpha=0.4, color='#eeeeee', zorder=1)\n",
    "\n",
    "    ax.set_xlabel(\"%s difference page views\" % (\"Relative\" if relative else \"Absolute\"), labelpad=20, size=fontsize)\n",
    "\n",
    "    # ax.set_ylabel(\"Start Station\", labelpad=20, size=font_size)\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    ax.yaxis.set_major_locator(ticker.FixedLocator(0.0 + 2 * y_pos))\n",
    "    ax.yaxis.set_major_formatter(ticker.FixedFormatter(topics[\"topic\"]))\n",
    "    ax.set_yticklabels(topics[\"topic\"])\n",
    "    \n",
    "    if title:\n",
    "        plt.title(title, fontsize=fontsize, pad=20)\n",
    "    plt.tight_layout()\n",
    "    if savefig:\n",
    "        Path(savefig).parent.mkdir(parents=True, exist_ok=True)\n",
    "        plt.savefig(savefig)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185ed393-fb3f-4b14-a222-3a79e26f5947",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 15\n",
    "level = 4\n",
    "relative = False\n",
    "\n",
    "def to_date2(d):\n",
    "    d = d.timetuple()\n",
    "    return f\"{d.tm_year}-{d.tm_mon}-{d.tm_mday}\"\n",
    "\n",
    "def to_date(d):\n",
    "    d = d.timetuple()\n",
    "    return f\"{d.tm_mday}/{d.tm_mon}/{d.tm_year}\"\n",
    "\n",
    "for country_group in language_groups:\n",
    "    for country in cp.COUNTRIES[country_group]:\n",
    "        try:\n",
    "            country_stringency, changepoints = cp.get_change_points(stringency, country)\n",
    "        except rpt.exceptions.BadSegmentationParameters as e:\n",
    "            print(e)\n",
    "            changepoints = []\n",
    "            \n",
    "        # include set of known global changepoints\n",
    "        changepoints = [\n",
    "            datetime.date(2020, 1, 11) # first death in china\n",
    "        ] + changepoints\n",
    "        \n",
    "        print(f\"found {len(changepoints)} changepoints for {country} ({country_group})\")\n",
    "        \n",
    "        country_desc = dict(\n",
    "            group=country_group,\n",
    "            name=country_stringency.select(\"CountryName\").first().CountryName,\n",
    "            code=country_stringency.select(\"CountryCode\").first().CountryCode,\n",
    "        )\n",
    "        \n",
    "        stringency_dataset = dict(\n",
    "            country = country_desc,\n",
    "            stringency = [json.loads(s) for s in country_stringency.select(\"Date\", \"StringencyIndex\", \"Notes\").toJSON().collect()],\n",
    "            changepoints = changepoints,\n",
    "        )\n",
    "        \n",
    "        out_path = Path(\"./website/public/data\")\n",
    "        out_src_path = Path(\"./website/src/data\")\n",
    "        \n",
    "        stringency_out_path = out_path / country_group.lower() / country.lower() / \"stringency_changepoints.json\"\n",
    "        stringency_out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        with open(stringency_out_path, \"w\") as f:\n",
    "            json.dump(stringency_dataset, f, indent=2, sort_keys=True, default=str)\n",
    "        \n",
    "        cp.plot_changepoints(\n",
    "            country_stringency.select(\"Date\", \"StringencyIndex\").toPandas().set_index(\"Date\"),\n",
    "            changepoints,\n",
    "            title=f\"Change-points based on stringency index for {country}\",\n",
    "            savefig=f\"./figs/results/{country}/changepoints.pdf\")\n",
    "        plt.show()\n",
    "        \n",
    "        continue\n",
    "\n",
    "        country_stringency_2020 = country_stringency.filter(\n",
    "            (F.lit(datetime.date(2020,1,1)) <= F.col(\"Date\"))\n",
    "            & (F.col(\"Date\") <= F.lit(datetime.date(2021,1,1)))\n",
    "        ).toPandas().set_index(\"Date\")\n",
    "\n",
    "        print(f\"collecting samples for {country}\")\n",
    "        year_df = spark.read.format(\"parquet\").load(f\"../nvme/country_pageviews/pageviews_{country_group}.parquet\")\n",
    "        year_df = year_df.filter(F.col(f\"level{level}_daily_total\").isNotNull())\n",
    "\n",
    "        total_traffic = year_df \\\n",
    "            .select(\"date\", f\"level{level}_daily_total\") \\\n",
    "            .groupBy(\"date\") \\\n",
    "            .agg(F.sum(f\"level{level}_daily_total\").alias(\"page_views\")) \\\n",
    "            .sort(F.col(\"date\").asc())\n",
    "        \n",
    "        # total_traffic.show()\n",
    "        \n",
    "        plot_with_stringency(\n",
    "            country_stringency.toPandas().set_index(\"Date\"),\n",
    "            total_traffic.toPandas().set_index(\"date\"),\n",
    "            label=\"Total\",\n",
    "            title=f\"Total page views for {country}\",\n",
    "            savefig=f\"./figs/results/{country}/total_pageviews.pdf\"\n",
    "        )\n",
    "        plt.show()\n",
    "        \n",
    "        total_traffic_dataset = dict(\n",
    "            country = country_desc,\n",
    "            traffic = [json.loads(s) for s in total_traffic.select(\"date\", F.col(\"page_views\").alias(\"views\")).toJSON().collect()],\n",
    "        )\n",
    "        total_traffic_out_path = out_path / country_group.lower() / country.lower() / \"total.json\"\n",
    "        total_traffic_out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        with open(total_traffic_out_path, \"w\") as f:\n",
    "            json.dump(total_traffic_dataset, f, indent=2, sort_keys=True, default=str)\n",
    "\n",
    "        # for topic in [\"medicine\", \"sports\", \"television\", \"computer games\", \"mathematics\", \"homeoffice\", \"home-office\"]:\n",
    "        # for topic in [\"games\", \"mathematics\", \"medicine\", \"sports\", \"television\"]:\n",
    "        \n",
    "        # year_df.show()\n",
    "        # year_df.filter(F.lower(F.col(\"topic\")).contains(\"covid\")).show()\n",
    "        \n",
    "        selected_topics = [\"medicine\", \"music\", \"entertainment\", \"sports\", \"television\", \"gaming\", \"culture\", \"party\", \"science\", \"books\"]\n",
    "        selected_topics_out_path = out_src_path / \"selected_topics.json\"\n",
    "        selected_topics_out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        with open(selected_topics_out_path, \"w\") as f:\n",
    "            json.dump(selected_topics, f, indent=2, sort_keys=True, default=str)\n",
    "        \n",
    "        \n",
    "        for topic in selected_topics:\n",
    "            # continue\n",
    "            # .filter((F.lit(control_changepoint) <= F.col(\"date\")) & (F.col(\"date\") <= F.lit(control_end))) \\\n",
    "            # mean_topic_page_views = year_df \\\n",
    "            #     .select(\"date\", f\"level{level}_daily_total\") \\\n",
    "            #     .groupBy(\"date\") \\\n",
    "            #     .agg(F.sum(f\"level{level}_daily_total\").alias(\"sum_page_views\")) \\\n",
    "                # .toPandas().set_index(\"date\")\n",
    "\n",
    "            # absolute\n",
    "                \n",
    "            topic_page_views = year_df \\\n",
    "                .filter(F.lower(F.col(\"topic\")) == topic.lower()) \\\n",
    "                .select(\"date\", f\"level{level}_daily_total\") \\\n",
    "                .groupBy(\"date\") \\\n",
    "                .agg(F.sum(f\"level{level}_daily_total\").alias(\"page_views\"))\n",
    "            \n",
    "            try:\n",
    "                plot_with_stringency(\n",
    "                    country_stringency_2020,\n",
    "                    # topic_page_views.filter((F.lit(datetime.date(2020,1,1)) <= F.col(\"date\"))).sort(F.col(\"date\").asc()).toPandas().set_index(\"date\"),\n",
    "                    topic_page_views.sort(F.col(\"date\").asc()).toPandas().set_index(\"date\"),\n",
    "                    label=f\"{topic.capitalize()}\",\n",
    "                    title=f\"Page views for topic \\\"{topic.capitalize()}\\\" in {country}\",\n",
    "                    savefig=f\"./figs/results/{country}/topics/{country}_{topic}.pdf\"\n",
    "                )\n",
    "                plt.show()\n",
    "                \n",
    "                topic_dataset = dict(\n",
    "                    country = country_desc,\n",
    "                    traffic = [json.loads(s) for s in topic_page_views.select(\"date\", F.col(\"page_views\").alias(\"views\")).toJSON().collect()],\n",
    "                )\n",
    "                topic_out_path = out_path / country_group.lower() / country.lower() / \"topics\" / f\"{topic.lower()}.json\"\n",
    "                topic_out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                with open(topic_out_path, \"w\") as f:\n",
    "                    json.dump(topic_dataset, f, indent=2, sort_keys=True, default=str)\n",
    "                \n",
    "            except AssertionError:\n",
    "                print(\"topic\", topic, \"failed\")\n",
    "            \n",
    "            # normalized (relative)\n",
    "            \n",
    "            topic_page_views_normalized = topic_page_views.join(\n",
    "                    total_traffic.select(\"date\", F.col(\"page_views\").alias(\"total_page_views\")),\n",
    "                    on=\"date\",\n",
    "                    how=\"inner\"\n",
    "                ) \\\n",
    "                .withColumn(\"normalized_page_views\", F.col(\"page_views\") / F.col(\"total_page_views\")) \\\n",
    "                .select(\"date\", F.col(\"normalized_page_views\").alias(\"page_views\"))\n",
    "            \n",
    "            # topic_page_views_normalized.show()\n",
    "            # print(country_stringency.filter((F.lit(datetime.date(2020,1,1)) <= F.col(\"Date\")) & (F.col(\"Date\") <= F.lit(datetime.date(2021,1,1)))).limit(10).toPandas().set_index(\"Date\").head())\n",
    "            # break\n",
    "            # topic_page_views = topic_page_views \\\n",
    "            #     .join(mean_topic_page_views, on=\"date\", how=\"inner\")\n",
    "            # topic_page_views = topic_page_views \\\n",
    "            #     .join(country_stringency.select(F.col(\"Date\").alias(\"date\"), \"StringencyIndex\"), on=\"date\", how=\"inner\") \\\n",
    "            #     .withColumn(\"page_views\", F.col(\"page_views\") / F.col(\"sum_page_views\") ) \\\n",
    "            #     .toPandas().set_index(\"date\")\n",
    "            # \n",
    "            # topic_page_views.show()\n",
    "            \n",
    "            try:\n",
    "                plot_with_stringency(\n",
    "                    country_stringency_2020,\n",
    "                    # topic_page_views.filter((F.lit(datetime.date(2020,1,1)) <= F.col(\"date\"))).sort(F.col(\"date\").asc()).toPandas().set_index(\"date\"),\n",
    "                    topic_page_views_normalized.sort(F.col(\"date\").asc()).toPandas().set_index(\"date\"),\n",
    "                    label=f\"{topic.capitalize()}\",\n",
    "                    ylabel2=\"page views / total page views\",\n",
    "                    title=f\"Normalized page views for topic \\\"{topic.capitalize()}\\\" in {country}\",\n",
    "                    savefig=f\"./figs/results/{country}/topics/{country}_{topic}_normalized.pdf\"\n",
    "                )\n",
    "                plt.show()\n",
    "                \n",
    "                topic_normalized_dataset = dict(\n",
    "                    country = country_desc,\n",
    "                    traffic = [json.loads(s) for s in topic_page_views_normalized.select(\"date\", F.col(\"page_views\").alias(\"views\")).toJSON().collect()],\n",
    "                )\n",
    "                topic_normalized_out_path = out_path / country_group.lower() / country.lower() / \"topics\" / f\"{topic.lower()}_normalized.json\"\n",
    "                topic_normalized_out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                with open(topic_normalized_out_path, \"w\") as f:\n",
    "                    json.dump(topic_normalized_dataset, f, indent=2, sort_keys=True, default=str)\n",
    "            \n",
    "            except AssertionError:\n",
    "                print(\"topic\", topic, \"failed\")\n",
    "\n",
    "        # for each changepoint, get risers and decreasers\n",
    "        for changepoint in changepoints:\n",
    "            for (method_name, method, method_desc) in [(\"did\", did, \" using DiD regression\"), (\"diff\", diff, \" using absolute difference\")]:\n",
    "                start = time.time()\n",
    "                control_changepoint = changepoint\n",
    "                day_diff = 0\n",
    "                while True:\n",
    "                    try:\n",
    "                        control_changepoint = changepoint.replace(year=2019, day=changepoint.day - day_diff)\n",
    "                        break\n",
    "                    except ValueError:\n",
    "                        day_diff += 1\n",
    "                diff_df = method(\n",
    "                    year_df,\n",
    "                    changepoint=changepoint,\n",
    "                    control_changepoint=control_changepoint,\n",
    "                    topic_level=level,\n",
    "                    window_size=relativedelta(days=window_size),\n",
    "                    relative=relative,\n",
    "                )\n",
    "                \n",
    "                increased = diff_df.select(\"topic\", \"diff\").sort(F.col(\"diff\").desc()).limit(20)\n",
    "                # increased.show()\n",
    "                \n",
    "                increased_dataset = dict(\n",
    "                    country = country_desc,\n",
    "                    changepoint = changepoint,\n",
    "                    topics = [json.loads(i) for i in increased.toJSON().collect()],\n",
    "                )\n",
    "                increased_out_path = out_path / country_group.lower() / country.lower() / \"changepoints\" / f\"{method_name}_{to_date2(changepoint)}_increased.json\"\n",
    "                increased_out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                with open(increased_out_path, \"w\") as f:\n",
    "                    json.dump(increased_dataset, f, indent=2, sort_keys=True, default=str)\n",
    "                    \n",
    "                \n",
    "                decreased = diff_df.select(\"topic\", \"diff\").sort(F.col(\"diff\").asc()).limit(20)\n",
    "                # decreased.show()\n",
    "                \n",
    "                decreased_dataset = dict(\n",
    "                    country = country_desc,\n",
    "                    changepoint = changepoint,\n",
    "                    topics = [json.loads(i) for i in decreased.toJSON().collect()],\n",
    "                )\n",
    "                decreased_out_path = out_path / country_group.lower() / country.lower() / \"changepoints\" / f\"{method_name}_{to_date2(changepoint)}_decreased.json\"\n",
    "                decreased_out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                with open(decreased_out_path, \"w\") as f:\n",
    "                    json.dump(decreased_dataset, f, indent=2, sort_keys=True, default=str)\n",
    "                \n",
    "                print(\"took %.2f seconds\" % (time.time() - start))\n",
    "                \n",
    "                plot_topics(\n",
    "                    increased.limit(10).toPandas(),\n",
    "                    risers=True,\n",
    "                    relative=relative,\n",
    "                    savefig=f\"./figs/results/{country}/attention_shifts/{method_name}_{to_date2(changepoint)}_increase.pdf\",\n",
    "                    title=\"Changepoint %s ($\\pm$ %d days)%s\" % (to_date(changepoint), window_size, method_desc)\n",
    "                )\n",
    "                plot_topics(\n",
    "                    decreased.limit(10).toPandas(),\n",
    "                    risers=False,\n",
    "                    relative=relative,\n",
    "                    savefig=f\"./figs/results/{country}/attention_shifts/{method_name}_{to_date2(changepoint)}_decrease.pdf\",\n",
    "                    title=\"Changepoint %s ($\\pm$ %d days)%s\" % (to_date(changepoint), window_size, method_desc)\n",
    "                )\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f18f25b-103a-44dc-9962-a352c04dd9fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb5a097-2af1-4f98-98fa-de7b482966b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19ae867-8b72-414a-8ecb-5edb6d47abcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d3a270-63f6-4366-8dfd-5e4f432a0ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pageview_complete_per_topic = Path(\"../nvme/pageview_complete_per_topic\")\n",
    "\n",
    "group = \"de\"\n",
    "group = \"nl\"\n",
    "# group = \"en\"\n",
    "\n",
    "year = 2020\n",
    "all_days = list(dl.date_range(\n",
    "    datetime.date(year, 1, 1),\n",
    "    datetime.date(year, 12, 31),\n",
    "))\n",
    "# pprint(all_days)\n",
    "\n",
    "year_df = None\n",
    "for day in tqdm(all_days):\n",
    "    data = pageview_complete_per_topic / Path(\"/\".join(dl.wikimedia_pageview_complete_local_file(day, monthly=False))).with_suffix(\".parquet\")\n",
    "    df = spark.read.format(\"parquet\").load(str(data / f\"group={group}\"))\n",
    "    df = df.filter(F.col(\"level4_daily_total\").isNotNull())\n",
    "    df = df.withColumn(\"date\", F.lit(day))\n",
    "    \n",
    "    if year_df is None:\n",
    "        year_df = df\n",
    "    else:\n",
    "        year_df = year_df.union(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ec8fb0-5a88-483a-afd6-6290fa298d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "pageview_complete_per_topic = Path(\"../nvme/pageview_complete_per_topic\")\n",
    "\n",
    "changepoint = datetime.date(2020, 1, 15)\n",
    "control_changepoint = changepoint.replace(year=2019)\n",
    "\n",
    "print(changepoint, control_changepoint)\n",
    "\n",
    "window_size = relativedelta(days=10)\n",
    "\n",
    "def to_date(d):\n",
    "    d = pre_day.timetuple()\n",
    "    return f\"{d.tm_year}-{d.tm_mon}-{d.tm_mday}\"\n",
    "    \n",
    "control_start, control_end = control_changepoint - window_size, control_changepoint + window_size\n",
    "target_start, target_end = changepoint - window_size, changepoint + window_size\n",
    "\n",
    "control_days = list(dl.date_range(control_start, control_end))\n",
    "target_days = list(dl.date_range(target_start, target_end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab900a1-5353-46ec-9191-5a7f81626397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diff in diff\n",
    "level = 4\n",
    "# level = 1\n",
    "\n",
    "pre_target_mean = year_df \\\n",
    "    .filter((F.lit(target_start) <= F.col(\"date\")) & (F.col(\"date\") <= F.lit(changepoint))) \\\n",
    "    .fillna(0) \\\n",
    "    .groupBy(\"topic\", \"dbname\", \"wiki_code\", \"language\") \\\n",
    "    .agg(F.mean(f\"level{level}_daily_total\").alias(\"pre_target_mean\")) \\\n",
    "    # .withColumn(\"pre_target_mean\", F.log(\"pre_target_mean\"))\n",
    "\n",
    "pre_control_mean = year_df \\\n",
    "    .filter((F.lit(control_start) <= F.col(\"date\")) & (F.col(\"date\") <= F.lit(control_changepoint))) \\\n",
    "    .fillna(0) \\\n",
    "    .groupBy(\"topic\", \"dbname\", \"wiki_code\", \"language\") \\\n",
    "    .agg(F.mean(f\"level{level}_daily_total\").alias(\"pre_control_mean\")) \\\n",
    "    # .withColumn(\"pre_control_mean\", F.log(\"pre_control_mean\"))\n",
    "\n",
    "post_target_mean = year_df \\\n",
    "    .filter((F.lit(changepoint) <= F.col(\"date\")) & (F.col(\"date\") <= F.lit(target_end))) \\\n",
    "    .fillna(0) \\\n",
    "    .groupBy(f\"topic\", \"dbname\", \"wiki_code\", \"language\") \\\n",
    "    .agg(F.mean(f\"level{level}_daily_total\").alias(\"post_target_mean\")) \\\n",
    "    # .withColumn(\"post_target_mean\", F.log(\"post_target_mean\"))\n",
    "\n",
    "post_control_mean = year_df \\\n",
    "    .filter((F.lit(control_changepoint) <= F.col(\"date\")) & (F.col(\"date\") <= F.lit(control_end))) \\\n",
    "    .fillna(0) \\\n",
    "    .groupBy(f\"topic\", \"dbname\", \"wiki_code\", \"language\") \\\n",
    "    .agg(F.mean(f\"level{level}_daily_total\").alias(\"post_control_mean\")) \\\n",
    "    # .withColumn(\"post_control_mean\", F.log(\"post_control_mean\"))\n",
    "\n",
    "diff = pre_target_mean.join(pre_control_mean, on=[\"topic\", \"dbname\", \"wiki_code\", \"language\"], how=\"inner\")\n",
    "diff = diff.join(post_target_mean, on=[\"topic\", \"dbname\", \"wiki_code\", \"language\"], how=\"inner\")\n",
    "diff = diff.join(post_control_mean, on=[\"topic\", \"dbname\", \"wiki_code\", \"language\"], how=\"inner\")\n",
    "\n",
    "diff = diff.withColumn('diff', ( F.col(\"post_target_mean\") / F.col(\"pre_target_mean\") ) )\n",
    "diff = diff.withColumn('control_diff', ( F.col(\"pre_control_mean\") - F.col(\"post_control_mean\") ) )\n",
    "diff = diff.withColumn('target_diff', ( F.col(\"post_target_mean\") - (F.col(\"pre_target_mean\") + F.col(\"control_diff\")) ) )\n",
    "\n",
    "diff.sort(F.col(\"target_diff\").desc()).select(\n",
    "    \"topic\", \"dbname\", \"wiki_code\", \"language\",\n",
    "    F.round(\"target_diff\", 0).alias(\"target_diff\"),\n",
    "    F.round(\"pre_target_mean\", 0).alias(\"pre_target_mean\"),\n",
    "    F.round(\"pre_control_mean\", 0).alias(\"pre_control_mean\"),\n",
    "    F.round(\"post_target_mean\", 0).alias(\"post_target_mean\"),\n",
    "    F.round(\"post_control_mean\", 0).alias(\"post_control_mean\"),\n",
    ").show()\n",
    "\n",
    "diff.sort(F.col(\"diff\").desc()).select(\n",
    "    \"topic\", \"dbname\", \"wiki_code\", \"language\",\n",
    "    F.round(\"diff\", 0).alias(\"diff\"),\n",
    "    F.round(\"pre_target_mean\", 0).alias(\"pre_target_mean\"),\n",
    "    # F.round(\"pre_control_mean\", 0).alias(\"pre_control_mean\"),\n",
    "    F.round(\"post_target_mean\", 0).alias(\"post_target_mean\"),\n",
    "    # F.round(\"post_control_mean\", 0).alias(\"post_control_mean\"),\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c203f3e-7911-4d1f-a136-68360a7772bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_diffs = diff.sort(F.col(\"target_diff\").desc()).select(\n",
    "    \"topic\", # \"dbname\", \"wiki_code\", \"language\",\n",
    "    F.round(\"target_diff\", 0).alias(\"target_diff\"),\n",
    "    F.round(\"pre_target_mean\", 0).alias(\"pre_target_mean\"),\n",
    "    F.round(\"pre_control_mean\", 0).alias(\"pre_control_mean\"),\n",
    "    F.round(\"post_target_mean\", 0).alias(\"post_target_mean\"),\n",
    "    F.round(\"post_control_mean\", 0).alias(\"post_control_mean\"),\n",
    ").toPandas()\n",
    "# print(topic_diffs.head())\n",
    "# print(topic_diffs.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e106174-01af-4ec7-8c96-ec885548c757",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_stringency = stringency.filter(F.col(\"CountryName\") == countries[group]).select(\"Date\", \"StringencyIndex\").toPandas().set_index(\"Date\")\n",
    "\n",
    "# for each country\n",
    "\n",
    "# plot the changepoints\n",
    "\n",
    "# for each changepoint, generate the diff in diff changes\n",
    "\n",
    "# plot the total per stringency\n",
    "\n",
    "# plot the topic attention for \"medicine\", \"sports\", \"television\", \"computer games\", \"mathematics\", \"homeoffice\"\n",
    "\n",
    "topic = \"Football\"\n",
    "topic = \"Medicine\"\n",
    "single_topic_pageview = year_df \\\n",
    "    .filter(F.lower(F.col(\"topic\")) == topic.lower()) \\\n",
    "    .select(\"date\", f\"level{level}_daily_total\") \\\n",
    "    .groupBy(\"date\") \\\n",
    "    .agg(F.sum(f\"level{level}_daily_total\").alias(\"page_views\")) \\\n",
    "    .toPandas().set_index(\"date\")\n",
    "\n",
    "country_stringency.head()\n",
    "single_topic_pageview.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb6dfb8-9652-492c-8060-e08f51c824c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_attention = year_df \\\n",
    "    .select(\"date\", f\"level{level}_daily_total\") \\\n",
    "    .groupBy(\"date\") \\\n",
    "    .agg(F.sum(f\"level{level}_daily_total\").alias(\"page_views\")) \\\n",
    "    .toPandas().set_index(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4935d174-96fb-4bcb-ba40-ea13c08cb897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothing_window(vals, radius=50):\n",
    "    cumvals = np.array(vals).cumsum()\n",
    "    return (cumvals[radius:] - cumvals[:-radius]) / radius\n",
    "\n",
    "def plot_with_stringency(s, pageviews, label=\"Pageviews\", title=None, smoothing = 50, fontsize=15, figsize=(12, 7)):\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax2 = ax.twinx()\n",
    "    \n",
    "    ax.tick_params(axis='both', which='major', labelsize=fontsize * 3/4)\n",
    "    ax.tick_params(axis='both', which='minor', labelsize=fontsize * 3/4)\n",
    "    \n",
    "    ax.set_xlabel(\"time\", labelpad=20, size=fontsize)\n",
    "    ax.set_ylabel(\"stringency index\", labelpad=20, size=fontsize)\n",
    "    ax2.set_ylabel(\"absolute page views\", labelpad=20, size=fontsize)\n",
    "    \n",
    "    ax.plot(s.index[:len(pageviews)-smoothing], s[\"StringencyIndex\"][:len(pageviews)-smoothing], label=\"Stringency Index\", color=\"gray\", linestyle='--', linewidth=2)\n",
    "    ax2.plot(pageviews.index[:-smoothing], smoothing_window(pageviews[\"page_views\"], radius=smoothing), label=label, color=\"blue\", linestyle='-', linewidth=3)\n",
    "\n",
    "    plt.legend(fontsize=fontsize)\n",
    "    if title:\n",
    "        plt.title(title, fontsize=fontsize, pad=20)\n",
    "    plt.show()\n",
    "    \n",
    "# f\"Stringency index for {countries[group]}\"\n",
    "plot_with_stringency(country_stringency, single_topic_pageview, label=f\"{topic} pageviews\")\n",
    "plot_with_stringency(country_stringency, total_attention, label=\"Total pageviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11632b82-302e-4011-a379-d2a5493387ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import ticker\n",
    "from colour import Color\n",
    "\n",
    "def plot_topics(topics, fontsize=17, figsize=(12, 7), risers=True):\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    \n",
    "    (pre_start, pre_end) = (Color(\"blue\"), Color(\"purple\"))\n",
    "    (post_start, post_end) = (Color(\"red\"), Color(\"orange\"))\n",
    "\n",
    "    y_pos = np.arange(len(topics))\n",
    "    \n",
    "    colors = [c.hex for c in pre_start.range_to(pre_end,len(topics))]\n",
    "    if risers:\n",
    "        colors = [c.hex for c in post_start.range_to(post_end,len(topics))]\n",
    "\n",
    "    ax.barh(0 + 2 * y_pos, topics[\"diff\"], color=colors, align='center')\n",
    "    # ax.barh(1 + 2 * y_pos, top_n[\"post_control_mean\"], color=post_colors, alpha=0.2, align='center')\n",
    "\n",
    "    ax.tick_params(axis=\"both\", labelsize=fontsize, which=\"both\", bottom=\"off\", top=\"off\", labelbottom=\"on\", left=\"off\", right=\"off\", labelleft=\"on\")\n",
    "\n",
    "    vals = ax.get_xticks()\n",
    "    for tick in vals:\n",
    "        ax.axvline(x=tick, linestyle='dashed', alpha=0.4, color='#eeeeee', zorder=1)\n",
    "\n",
    "    ax.set_xlabel(\"Absolute difference page views\", labelpad=20, size=fontsize)\n",
    "\n",
    "    # ax.set_ylabel(\"Start Station\", labelpad=20, size=font_size)\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    ax.yaxis.set_major_locator(ticker.FixedLocator(0.5 + 2 * y_pos))\n",
    "    ax.yaxis.set_major_formatter(ticker.FixedFormatter(topics[\"topic\"]))\n",
    "    ax.set_yticklabels(topics[\"topic\"])\n",
    "    plt.show()\n",
    "    \n",
    "plot_topics(topic_diffs.iloc[:10], risers=True)\n",
    "plot_topics(topic_diffs.iloc[-10:], risers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8bd3b7-3114-40f0-a14b-41a107ce2aef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d585e4-c95a-433b-bce7-e1da10668a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "pageview_complete_per_topic = Path(\"../nvme/pageview_complete_per_topic\")\n",
    "group = \"nl\"\n",
    "\n",
    "days_2019 = list(dl.date_range(control_start, control_end))\n",
    "target_days = list(dl.date_range(target_start, target_end))\n",
    "all_days = list(dl.date_range(datetime.date(2019, 1, 1), datetime.date(2019, 2, 1))) + list(dl.date_range(datetime.date(2020, 1, 1), datetime.date(2020, 2, 1)))\n",
    "\n",
    "changepoint_df = None\n",
    "for day in all_days:\n",
    "    data = pageview_complete_per_topic / Path(\"/\".join(dl.wikimedia_pageview_complete_local_file(day, monthly=False))).with_suffix(\".parquet\")\n",
    "    df = spark.read.format(\"parquet\").load(str(data / f\"group={group}\"))\n",
    "    df = df.filter(F.col(\"level4_daily_total\").isNotNull())\n",
    "    df = df.select(\"topic\", \"dbname\", \"wiki_code\", \"language\", \"level1_daily_total\", \"level2_daily_total\", \"level3_daily_total\", \"level4_daily_total\")\n",
    "    df = df.withColumn(\"date\", F.lit(day))\n",
    "    \n",
    "    if changepoint_df is None:\n",
    "        changepoint_df = df\n",
    "    else:\n",
    "        changepoint_df = changepoint_df.union(df)\n",
    "        pass\n",
    "    \n",
    "# changepoint_df.limit(10).show()\n",
    "changepoint_df.write.csv(\"../nvme/yannick-nl.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda5a2b2-d0ed-4d5a-ad60-064ad7556a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_df = pre_df.groupBy(\"topic\", \"dbname\", \"wiki_code\", \"language\").agg(F.sum(\"level4_daily_total\"))\n",
    "# changepoint_df.show()\n",
    "# changepoint_df.printSchema()\n",
    "# start simple, compute the mean pre and post changepoint and take largest relative difference\n",
    "print(target_start, changepoint)\n",
    "level = 1\n",
    "\n",
    "pre_pageviews = changepoint_df \\\n",
    "    .filter((F.lit(target_start) <= F.col(\"date\")) & (F.col(\"date\") <= F.lit(changepoint))) \\\n",
    "    .groupBy(\"topic\", \"dbname\", \"wiki_code\", \"language\") \\\n",
    "    .agg(F.mean(f\"level{level}_daily_total\").alias(\"mean_pageviews_pre\"))\n",
    "# pre_pageviews.select(\"date\").distinct().show()\n",
    "# pre_pageviews.show()\n",
    "\n",
    "post_pageviews = changepoint_df \\\n",
    "    .filter((F.lit(changepoint) <= F.col(\"date\")) & (F.col(\"date\") <= F.lit(target_end))) \\\n",
    "    .groupBy(f\"topic\", \"dbname\", \"wiki_code\", \"language\") \\\n",
    "    .agg(F.mean(f\"level{level}_daily_total\").alias(\"mean_pageviews_post\"))\n",
    "# post_pageviews.select(\"date\").distinct().show()\n",
    "# post_pageviews.show()\n",
    "\n",
    "diff = pre_pageviews.join(post_pageviews, on=[\"topic\", \"dbname\", \"wiki_code\", \"language\"], how=\"inner\")\n",
    "diff = diff.withColumn('diff', ( F.col(\"mean_pageviews_post\") / F.col(\"mean_pageviews_pre\") ) )\n",
    "# diff = diff.sort(F.col(\"diff\").desc())\n",
    "# diff.show()\n",
    "\n",
    "diff.filter(F.col(\"mean_pageviews_post\") > 1_000).sort(F.col(\"diff\").desc()).show()\n",
    "# diff.sort(F.col(\"mean_pageviews_post\").desc()).show()\n",
    "# dd1 = dd1.withColumn('Result', ( dd1['A'] - dd1['B'] ) / dd1['A'] )\n",
    "# \n",
    "# diff = pre_pageviews\n",
    "\n",
    "# .select(\"date\").distinct().show()\n",
    "# print(changepoint_df.select(\"topic\").distinct().count())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
