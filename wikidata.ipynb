{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ebc39d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pyspark\n",
    "import lsde2021.download as dl\n",
    "from lsde2021.types import PathLike\n",
    "from pyspark.sql import SparkSession\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "import re\n",
    "import hashlib\n",
    "from pprint import pprint\n",
    "import datetime\n",
    "import logging\n",
    "import logging.config\n",
    "import requests\n",
    "from io import StringIO, BytesIO\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import Union, List, Tuple, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e806b898",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/10/05 22:55:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "WIKI_DL_URL = 'https://dumps.wikimedia.org'\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MAX_MEMORY = \"60G\"\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"EDA\") \\\n",
    "    .config(\"spark.executor.memory\", MAX_MEMORY) \\\n",
    "    .config(\"spark.driver.memory\", MAX_MEMORY) \\\n",
    "    .config('spark.driver.maxResultSize', MAX_MEMORY) \\\n",
    "    .config('spark.ui.showConsoleProgress', 'false') \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4883db82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikipedia_dump_url(lang: str, date: str) -> str:\n",
    "    return '{}/{}wiki/{}'.format(WIKI_DL_URL, lang, date)\n",
    "\n",
    "def get_wikipedia_md5_checksum_url(lang: str, date: str) -> str:\n",
    "    return '{}/{}wiki/{}/{}wiki-{}-md5sums.txt'.format(WIKI_DL_URL, lang, date, lang, date)\n",
    "\n",
    "def get_wikipedia_multi_pattern(lang: str, date: str) -> str:\n",
    "    return r'^.*({}wiki-{}-pages-articles[0-9]+.xml.*bz2$)'.format(lang, date)\n",
    "\n",
    "def get_wikipedia_single_pattern(lang: str, date: str) -> str:\n",
    "    return r'^.*({}wiki-{}-pages-articles+.xml.*bz2$)'.format(lang, date)\n",
    "\n",
    "def get_wiki_archive_url(lang: str, date: str, href: str) -> str:\n",
    "    if Path(href).is_absolute():\n",
    "        return '{}/{}'.format(WIKI_DL_URL, href)\n",
    "    wiki_dump_url = get_wikipedia_dump_url(lang, date)\n",
    "    return '{}/{}'.format(wiki_dump_url, href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66e3c5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def md5_checksum(path: PathLike) -> str:\n",
    "    hash_md5 = hashlib.md5()\n",
    "    with open(path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            hash_md5.update(chunk)\n",
    "    return hash_md5.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dc0eae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_wiki_arxiv_hrefs(lang: str, date: str) -> List[Tuple[str, str]]:\n",
    "    wiki_dump_url = get_wikipedia_dump_url(lang, date)\n",
    "    wiki_dump_checksum_url = get_wikipedia_md5_checksum_url(lang, date)\n",
    "    wiki_archive_hrefs = []\n",
    "    try:\n",
    "        logger.info('collecting archive from {}'.format(wiki_dump_url))\n",
    "        \n",
    "        query_response = requests.get(wiki_dump_url, allow_redirects=True)\n",
    "        checksum_response = requests.get(wiki_dump_checksum_url, allow_redirects=True)\n",
    "        # checksum_response.content\n",
    "        # checksums_file = StringIO(str(checksum_response.content))\n",
    "        checksums_file = BytesIO(checksum_response.content)\n",
    "        checksums = pd.read_csv(\n",
    "            checksums_file,\n",
    "            header=None,\n",
    "            names=[\"checksum\", \"url\"],\n",
    "            index_col=\"url\",\n",
    "            delimiter=r\"\\s+\"\n",
    "        )\n",
    "        # valid_checksum_hrefs = checksums.index.tolist()\n",
    "        # pprint(valid_checksum_hrefs)\n",
    "        # assert \"dewiki-20211001-site_stats.sql.gz\" in checksums.index.tolist()\n",
    "        # print(checksums.loc[\"dewiki-20211001-site_stats.sql.gz\"])\n",
    "        \n",
    "        soup = BeautifulSoup(query_response.content, 'html.parser')\n",
    "        for link in soup.find_all('a'):\n",
    "            # we want multi\n",
    "            # print(link)\n",
    "            pattern = get_wikipedia_multi_pattern(lang, date)\n",
    "            href = link.get('href')\n",
    "            # print(pattern, href)\n",
    "            if re.match(pattern, href):\n",
    "                # print(\"match\")\n",
    "                # href_checksum = None\n",
    "                # if Path(href).name in valid_checksum_hrefs:\n",
    "                #     href_checksum = checksums.loc[Path(href).name]\n",
    "                # print(checksums.loc[Path(href).name][\"checksum\"])\n",
    "                wiki_archive_hrefs.append((checksums.loc[Path(href).name][\"checksum\"], href))\n",
    "            # print(\"no match\")\n",
    "        if len(wiki_archive_hrefs) < 1:\n",
    "            logger.info('no multi archives found. trying for single archive ...')\n",
    "            # if archive is too small, check for single arxiv\n",
    "            for link in soup.find_all('a'):\n",
    "                pattern = get_wikipedia_single_pattern(lang, date)\n",
    "                href = link.get('href')\n",
    "                if re.match(pattern, href):\n",
    "                    wiki_archive_hrefs.append((checksums.loc[Path(href).name][\"checksum\"], href))\n",
    "        if not wiki_archive_hrefs:\n",
    "            logger.warning('no wikipedia archive found')\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        logger.error('HTTP error ({}) using lang = \\'{}\\' and date = \\'{}\\'. '\n",
    "                     'could not retrieve any wikipedia data at {}'\n",
    "                     .format(e.response.status_code, lang, date, wiki_dump_url))\n",
    "        raise e\n",
    "    return wiki_archive_hrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31aa3039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_wikipedia(\n",
    "    sc: pyspark.sql.session.SparkSession,\n",
    "    lang: str,\n",
    "    date: Union[datetime.date, str],\n",
    "    dest: str,\n",
    "    force: bool = False,\n",
    "    max_concurrency: int = 4,\n",
    ") -> None:\n",
    "    if isinstance(date, datetime.date):\n",
    "        _date = \"%04d%02d%02d\" % (date.year, date.month, date.day)\n",
    "    else:\n",
    "        _date = date\n",
    "    \n",
    "    wiki_arxiv_hrefs = collect_wiki_arxiv_hrefs(lang, _date)\n",
    "    \n",
    "    def download_handler(wiki_archive: Tuple[str, str]) -> PathLike:\n",
    "        wiki_archive_checksum, wiki_archive_href = wiki_archive\n",
    "        wiki_archive_url = get_wiki_archive_url(lang, _date, wiki_archive_href)\n",
    "        destination = dest / lang / Path(wiki_archive_href).name\n",
    "        \n",
    "        # print(wiki_archive_href)\n",
    "        # check if the file already exists and is not corrupted\n",
    "        if not force and destination.exists():\n",
    "            checksum = md5_checksum(destination)\n",
    "            if checksum == wiki_archive_checksum:\n",
    "                logger.info('using existing file {}'.format(destination))\n",
    "                return destination\n",
    "            \n",
    "        logger.info('downloading {}'.format(wiki_archive_url))\n",
    "            \n",
    "        def validate_file(downloaded_file: PathLike) -> bool:\n",
    "            checksum = md5_checksum(downloaded_file)\n",
    "            return checksum == wiki_archive_checksum\n",
    "        \n",
    "        return dl.download_file(wiki_archive_url, destination=destination, validate_file_func=validate_file)\n",
    "    \n",
    "    downloaded = sc.parallelize(wiki_arxiv_hrefs, numSlices=max_concurrency) \\\n",
    "        .map(partial(download_handler)) \\\n",
    "        .collect()\n",
    "    return downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8b5afe4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('wikimedia_data/wikipedia/de/dewiki-20211001-pages-articles1.xml-p1p297012.bz2'),\n",
       " PosixPath('wikimedia_data/wikipedia/de/dewiki-20211001-pages-articles2.xml-p297013p1262093.bz2'),\n",
       " PosixPath('wikimedia_data/wikipedia/de/dewiki-20211001-pages-articles3.xml-p1262094p2762093.bz2'),\n",
       " PosixPath('wikimedia_data/wikipedia/de/dewiki-20211001-pages-articles3.xml-p2762094p3376257.bz2'),\n",
       " PosixPath('wikimedia_data/wikipedia/de/dewiki-20211001-pages-articles4.xml-p3376258p4876257.bz2'),\n",
       " PosixPath('wikimedia_data/wikipedia/de/dewiki-20211001-pages-articles4.xml-p4876258p6115464.bz2'),\n",
       " PosixPath('wikimedia_data/wikipedia/de/dewiki-20211001-pages-articles5.xml-p6115465p7615464.bz2'),\n",
       " PosixPath('wikimedia_data/wikipedia/de/dewiki-20211001-pages-articles5.xml-p7615465p9115464.bz2'),\n",
       " PosixPath('wikimedia_data/wikipedia/de/dewiki-20211001-pages-articles5.xml-p9115465p9261244.bz2'),\n",
       " PosixPath('wikimedia_data/wikipedia/de/dewiki-20211001-pages-articles6.xml-p9261245p10761244.bz2'),\n",
       " PosixPath('wikimedia_data/wikipedia/de/dewiki-20211001-pages-articles6.xml-p10761245p11939311.bz2')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dest = Path(\"./wikimedia_data/wikipedia\")\n",
    "download_wikipedia(sc, \"de\", datetime.date(2021,10,1), dest=dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aecf1530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabe113c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
