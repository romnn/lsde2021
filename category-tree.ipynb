{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c2743fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import bz2\n",
    "import csv\n",
    "import io\n",
    "import json\n",
    "import random\n",
    "import requests\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import List, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "import lsde2021.csv as csvutil\n",
    "import lsde2021.utils as utils\n",
    "import lsde2021.download as dl\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, IntegerType\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c293c9a2-89a7-4ea1-8a4b-00c571fab774",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/10/24 13:58:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "MAX_MEMORY = \"60G\"\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"parse-wikipedia-sql-dumps\") \\\n",
    "    .config(\"spark.executor.memory\", MAX_MEMORY) \\\n",
    "    .config(\"spark.driver.memory\", MAX_MEMORY) \\\n",
    "    .config('spark.driver.maxResultSize', MAX_MEMORY) \\\n",
    "    .config('spark.ui.showConsoleProgress', 'false') \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "csv_loader = spark.read.format(\"csv\").options(header='True', inferSchema='True')\n",
    "parquet_reader = spark.read.format(\"parquet\").options(inferSchema='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a097c72-dadc-4680-b9a2-dd090997bfe8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: file:/home/jovyan/nvme/wikipedia_sql_dumps/enwiki/20211001/enwiki-20211001-page.sql.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_48/1822087006.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# join categories with english wiki page table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mwiki\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"enwiki\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mraw_pages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparquet_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"../nvme/wikipedia_sql_dumps/{wiki}/20211001/{wiki}-20211001-page.sql.parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mraw_categorylinks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparquet_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"../nvme/wikipedia_sql_dumps/{wiki}/20211001/{wiki}-20211001-categorylinks.sql.parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/home/jovyan/nvme/wikipedia_sql_dumps/enwiki/20211001/enwiki-20211001-page.sql.parquet"
     ]
    }
   ],
   "source": [
    "# join categories with english wiki page table\n",
    "wiki = \"enwiki\"\n",
    "raw_pages = parquet_reader.load(f\"../nvme/wikipedia_sql_dumps/{wiki}/20211001/{wiki}-20211001-page.sql.parquet\")\n",
    "raw_categorylinks = parquet_reader.load(f\"../nvme/wikipedia_sql_dumps/{wiki}/20211001/{wiki}-20211001-categorylinks.sql.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebeecd8-b355-49fd-81a6-4fa459674340",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_pages.limit(10).show()\n",
    "raw_categorylinks.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41b3c21-ea8e-41ed-af93-9133bfc3edc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = raw_pages \\\n",
    "    .filter((F.col(\"page_is_redirect\") == 0)) \\\n",
    "    .filter((F.col(\"page_namespace\") == 0) | (F.col(\"page_namespace\") == 14)) \\\n",
    "    .select(\"page_id\", \"page_namespace\", \"page_title\")\n",
    "\n",
    "categorylinks = raw_categorylinks \\\n",
    "    .select(\"page_id\", \"category_name\")\n",
    "\n",
    "category_pages = pages \\\n",
    "    .filter(F.col(\"page_namespace\") == 14) \\\n",
    "    .select(\n",
    "        F.col(\"page_id\").alias(\"category_page_id\"),\n",
    "        F.col(\"page_title\").alias(\"category_name\"),\n",
    "    )\n",
    "\n",
    "print(pages.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0f93cc-b103-4147-a366-ce156db7c5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the categories of the page\n",
    "# .limit(100_000) \\\n",
    "page_cats = pages \\\n",
    "    .join(categorylinks, on=\"page_id\", how=\"inner\")\n",
    "\n",
    "# find the page_id for the categories\n",
    "page_cats = page_cats \\\n",
    "    .join(category_pages, on=\"category_name\", how=\"left\")\n",
    "\n",
    "page_cats.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459eac06-09d9-4743-ae9d-dd21133c6ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count topic popularity by number of pages\n",
    "duplicate_counts = page_cats \\\n",
    "    .groupby([\"page_id\"]) \\\n",
    "    .count()\n",
    "\n",
    "page_cats = page_cats \\\n",
    "    .join(duplicate_counts, on=\"page_id\", how=\"inner\") \\\n",
    "    .sort('count', ascending=False) \\\n",
    "\n",
    "page_cats.limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047520ba-04ba-4e9e-aa7c-70ab79cfc1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the pages with category\n",
    "page_cats.write.format(\"parquet\").mode(\"overwrite\").save(f\"../nvme/wikipedia_sql_dumps/{wiki}/20211001/{wiki}-20211001-page-category-count.sql.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83d593d-57c3-4b3c-b3cf-45fad7b886d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "graph = nx.DiGraph()\n",
    "\n",
    "max_size = None # 100_000\n",
    "for i, row in enumerate(page_cats.rdd.toLocalIterator()):\n",
    "    if i % ((max_size or 20_000_000) / 10) == 0:\n",
    "        print(\"row\", i)\n",
    "        \n",
    "    node = row[\"page_id\"]\n",
    "    node_count = row[\"count\"]\n",
    "    \n",
    "    category_node = row[\"category_page_id\"]\n",
    "    is_category = False\n",
    "    try:\n",
    "        is_category = int(row[\"page_namespace\"]) == 14\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    valid_node = node is not None and node is not np.nan\n",
    "    valid_category_node = category_node is not None and category_node is not np.nan\n",
    "    # print(node, category_node, is_category)\n",
    "    \n",
    "    # add page node\n",
    "    if valid_node:\n",
    "        if node not in graph.nodes:\n",
    "            graph.add_node(node, is_category=is_category, title=row[\"page_title\"], node_count=node_count)\n",
    "        else:\n",
    "            graph.update(nodes={\n",
    "                node: dict(is_category=is_category, title=row[\"page_title\"], node_count=node_count)\n",
    "            })\n",
    "    \n",
    "    # add category node\n",
    "    if valid_category_node and category_node not in graph.nodes:\n",
    "        graph.add_node(category_node, is_category=True, title=row[\"category_name\"], node_count=0)\n",
    "    \n",
    "    # add the edge between them\n",
    "    if valid_node and valid_category_node:\n",
    "        graph.add_edge(node, category_node)\n",
    "    \n",
    "    if max_size is not None and i >= max_size:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a4a47b-bacd-48ed-a3de-30466a6d5c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the graph for reuse\n",
    "nx.write_gpickle(graph, f\"../nvme/en-category-tree.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5e4924-f8ef-4be2-b67c-f0075e3128d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the graph for reuse\n",
    "nx.write_graphml_lxml(graph, f\"../nvme/en-category-tree.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb9201c-7311-4d0c-84e1-61dc975b36c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first have a closer look at some of the categories and how they look like so we can split them eventually\n",
    "example_categories = page_cats.select(\"category_name\").limit(1_000).rdd.flatMap(lambda x: x).collect()\n",
    "pprint(example_categories[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2945b7-5d15-4cd3-aee9-7827a6720d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns =\n",
    "# NUMBER_births -> People\n",
    "# XX_based_in_PLACE -> Organization\n",
    "# XX_established_in_PLACE -> Music\n",
    "# compositions_by_ARTIST -> Music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b9c7a9-c406-48ef-93d0-4f5a2b6f5614",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b359c6-0f9d-486f-b567-ad05d302df1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = nx.get_node_attributes(graph, 'title')\n",
    "# colors = {node: \"lightblue\" if is_cat else \"orange\" for node, is_cat in nx.get_node_attributes(graph, 'is_category').items()}\n",
    "colors = [\"lightblue\" if is_cat else \"orange\" for node, is_cat in nx.get_node_attributes(graph, 'is_category').items()]\n",
    "# print(colors)\n",
    "# print(labels)\n",
    "\n",
    "plt.figure(figsize=(12,12)) \n",
    "pos = nx.spring_layout(graph)\n",
    "_ = nx.draw_networkx_edges(graph, pos, alpha=0.2)\n",
    "_ = nx.draw_networkx_nodes(graph, pos, label=labels, node_size=1000, node_color=colors)\n",
    "_ = nx.draw_networkx_labels(graph, pos)\n",
    "# nx.draw(graph, labels=labels, node_size=1000, node_color=colors)\n",
    "# [\"lightblue\" if graph.nodes[n]['is_category'] else \"orange\" for n in graph.nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f4af72-3a32-42f4-85a0-0e9f4decb196",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064d0cee-adf9-4a8d-b1ed-f71caca5af2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f1a527-0e04-44dd-804f-32a32896e29b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808fef06-0c49-4f2e-95b4-846b3ec31524",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de73052-8923-4a02-84e1-026895b5823e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(row[\"page_id\"])\n",
    "    # find the categories of the page\n",
    "    page_id = row[\"page_id\"]\n",
    "    page_id = 3306201\n",
    "    page_cats = regular_pages \\\n",
    "        .filter(F.col(\"page_id\") == page_id) \\\n",
    "        .join(categorylinks, on=\"page_id\", how=\"inner\")\n",
    "    \n",
    "    # .find(F.col(\"page_id\") == row[\"page_id\"])\n",
    "    page_cats.limit(10).show()\n",
    "    \n",
    "    # find the category pages for the categories\n",
    "    # F.col(\"page_title\") == F.col(\"category_name\")\n",
    "    page_cats = page_cats.join(category_pages, on=\"category_name\", how=\"left\")\n",
    "    page_cats.limit(10).show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563f9894",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test = csvutil.read_pageview_csv(\n",
    "    \"../hdd/pageview_complete/2020/2020-02/pageviews-20200207-user.bz2\",\n",
    "    engine=\"python\",\n",
    "    skiprows=10_000,\n",
    "    nrows=10_000,\n",
    ")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2751484b-39b1-4daf-b130-897cbfc6a695",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = \"enwiki\"\n",
    "test = Path(f\"../nvme/wikipedia_sql_dumps/{wiki}/20211001/{wiki}-20211001-categorylinks.sql.csv\").resolve().absolute()\n",
    "df = csv_loader.load(str(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef1715b-72dd-4288-9ae2-69d6b4d126f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.withColumn(\"page_id\", df[\"page_id\"].cast(IntegerType()))\n",
    "# df = df.withColumn(\"page_id\", df[\"page_id\"].cast(IntegerType()))\n",
    "df.limit(100).show()\n",
    "df.printSchema()\n",
    "df.select(\"type\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3fab58-5c5d-424b-a810-6b616220843f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(F.col(\"page_id\").isNotNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214be875-51ce-42a2-80f1-f39f0bd5f29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c4bffe-5b04-4b63-a805-e2cd918cc9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "wiki = \"enwiki\"\n",
    "categorylinks = csvutil.read_categorylinks_csv(\n",
    "    f\"../hdd/wikipedia_sql_dumps/{wiki}/20211001/{wiki}-20211001-categorylinks-converted.sql.csv\",\n",
    "    names=None,\n",
    "    engine=\"c\",\n",
    "    header=0,\n",
    "    # low_memory=True,\n",
    "    # skiprows=1,\n",
    "    index_col=False,\n",
    "    nrows=100_000,\n",
    ")\n",
    "# print(categorylinks.shape)\n",
    "# print(categorylinks.head())\n",
    "#categorylinks[\"page_id\"] = pd.to_numeric(categorylinks[\"page_id\"], errors='coerce', downcast=\"unsigned\")\n",
    "#categorylinks[\"category_name\"] = categorylinks[\"category_name\"].astype(\"string\")\n",
    "#categorylinks[\"sortkey\"] = categorylinks[\"sortkey\"].astype(\"category\")\n",
    "# categorylinks[\"timestamp\"] = pd.to_datetime(categorylinks[\"timestamp\"], errors='coerce')\n",
    "#categorylinks[\"sortkey_prefix\"] = categorylinks[\"sortkey_prefix\"].astype(\"category\")\n",
    "#categorylinks[\"collation\"] = categorylinks[\"collation\"].astype(\"category\")\n",
    "#categorylinks[\"type\"] = categorylinks[\"type\"].astype(\"category\")\n",
    "# print(categorylinks.head())\n",
    "categorylinks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2607ae92-1147-41f7-813e-51c61fbc28fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499dec55-7ab1-441b-9568-8e766950a220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca3bd5f-9ddb-4df9-91f2-a5e3e167d502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_page_chunk(chunk: pd.DataFrame) -> pd.DataFrame:\n",
    "    # convert the dtypes\n",
    "    chunk[\"page_id\"] = pd.to_numeric(chunk[\"page_id\"], errors='coerce')\n",
    "    chunk[\"page_namespace\"] = pd.to_numeric(chunk[\"page_namespace\"], errors='coerce')\n",
    "    chunk[\"page_title\"] = chunk[\"page_title\"].astype(\"string\")\n",
    "    chunk[\"page_restrictions\"] = chunk[\"page_restrictions\"].astype(\"category\")\n",
    "\n",
    "    chunk[\"page_is_redirect\"] = chunk[\"page_is_redirect\"].astype(\"bool\")\n",
    "    chunk[\"page_is_new\"] = chunk[\"page_is_new\"].astype(\"bool\")\n",
    "\n",
    "    chunk[\"page_random\"] = pd.to_numeric(chunk[\"page_random\"], errors='coerce')\n",
    "    chunk[\"page_touched\"] = pd.to_datetime(chunk[\"page_touched\"], errors='coerce')\n",
    "    chunk[\"page_links_updated\"] = pd.to_datetime(chunk[\"page_links_updated\"], errors='coerce')\n",
    "\n",
    "    chunk[\"page_len\"] = chunk[\"page_len\"].astype(\"int32\")\n",
    "    chunk[\"page_content_model\"] = chunk[\"page_content_model\"].astype(\"category\")\n",
    "    chunk[\"page_lang\"] = chunk[\"page_lang\"].astype(\"category\")\n",
    "    \n",
    "    # find the revisions\n",
    "    page_ids = chunk[\"page_id\"].unique().tolist()\n",
    "    rev_ids = get_page_rev_ids(page_ids)\n",
    "    # pprint(rev_ids)\n",
    "\n",
    "    # find the articletopic\n",
    "    topics = dict()\n",
    "    scores = session.score(\"enwiki\", [\"articletopic\"], revids=rev_ids)\n",
    "    for (page_id, rev_id), score in zip(rev_ids.items(), scores):\n",
    "        response = score[\"articletopic\"]\n",
    "        if \"error\" not in response:\n",
    "            topic_probs = response[\"score\"][\"probability\"]\n",
    "            topic_probs = sorted(topic_probs.items(), key=lambda t: t[1], reverse=True)\n",
    "            topic_probs = [t for t, prob in topic_probs if prob > 0.6]\n",
    "            topics[page_id] = topic_probs[:5] + [None] * 5\n",
    "            \n",
    "    # add top 5 articetopics to the chunk dataframe\n",
    "    for i in range(5):\n",
    "        # chunk[f\"ores_topic_{i}\"] = np.nan\n",
    "        chunk[f\"ores_topic_{i+1}\"] = chunk[\"page_id\"].apply(lambda pid: topics.get(page_id, [None] * 5)[i])\n",
    "        \n",
    "    # add the original wikipedia category\n",
    "    chunk = pd.merge(chunk, categorylinks, on=\"page_id\", how=\"left\")\n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc6a2b0-75ab-4018-82f1-f9c38f8d574f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# chunksize = 10 ** 3\n",
    "chunksize = 50 # the limit for wikipedia api queries is 50\n",
    "header = True\n",
    "with csvutil.read_page_csv(\n",
    "    f\"../hdd/wikipedia_sql_dumps/{wiki}/20211001/{wiki}-20211001-page.sql.csv\",\n",
    "    engine=\"c\",\n",
    "    low_memory=True,\n",
    "    chunksize=chunksize,\n",
    "    # skiprows=0,\n",
    "    # nrows=10_000_000,\n",
    ") as reader:\n",
    "    for chunk in reader:\n",
    "        processed = process_page_chunk(chunk)\n",
    "        print(processed.head())\n",
    "        # os.path.join(folder, new_folder, \"new_file_\" + filename)\n",
    "        # chunk.to_csv(, header=header, cols=[['TIME','STUFF']], mode='a')\n",
    "        header = False\n",
    "        break\n",
    "# pages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e179cd8-b054-4537-adac-bb7496afa27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e030fb0b-324c-4a55-a870-d31612f2342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = \"enwiki\"\n",
    "pages = csv_loader.load(\n",
    "    f\"../hdd/wikipedia_sql_dumps/{wiki}/20211001/{wiki}-20211001-page.sql.csv\"\n",
    ")\n",
    "# .limit(10_000)\n",
    "pages = pages.toPandas()\n",
    "pages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c586029-a98b-4bdc-b415-e9c8d153f589",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pages.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555506cf-006d-4eaa-8563-1b66b43dec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "revids=[1050929646] # coffee\n",
    "scores = session.score(\"enwiki\", [\"articletopic\"], revids=revids)\n",
    "for revid, score in zip(revids, scores):\n",
    "    print(revid)\n",
    "    topic_probs = score[\"articletopic\"][\"score\"][\"probability\"]\n",
    "    topic_probs = sorted(topic_probs.items(), key=lambda t: t[1])\n",
    "    print(topic_probs\n",
    "print(list(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af04ff5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try out ores\n",
    "# \"\" enwiki damaging\n",
    "json.loads(\"{\\\"rev_id\\\": 456789}\")\n",
    "f = io.StringIO()\n",
    "out = score_revisions.run(\n",
    "    ores_host=\"https://ores.wikimedia.org\",\n",
    "    user_agent=\"\",\n",
    "    context=\"enwiki\",\n",
    "    model_names=[\"damaging\"],\n",
    "    batch_size=1,\n",
    "    parallel_requests=1,\n",
    "    retries=10,\n",
    "    input=['{\"rev_id\": 456789}'],\n",
    "    output=f,\n",
    "    verbose=0)\n",
    "print(out)\n",
    "print(f.read())\n",
    "# \\n{\"rev_id\": 3242342}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bc3443",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = \"frwiki\"\n",
    "dialect = csvutil.sniff_csv_dialect(\n",
    "    f\"../hdd/wikipedia_sql_dumps/{wiki}/20211001/{wiki}-20211001-categorylinks.sql.csv\"\n",
    ")\n",
    "pprint(csvutil.inspect_csv_dialect(dialect))\n",
    "for k, v in csvutil.inspect_csv_dialect(dialect).items():\n",
    "    print(f\"{k} = {repr(v)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0bb76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorylinks = csvutil.read_categorylinks_csv(\n",
    "    f\"../hdd/wikipedia_sql_dumps/{wiki}/20211001/{wiki}-20211001-categorylinks.sql.csv\",\n",
    "    nrows=1_000_000\n",
    ")\n",
    "categorylinks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1095af",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = csvutil.read_category_csv(\n",
    "    f\"../hdd/wikipedia_sql_dumps/{wiki}/20211001/{wiki}-20211001-category.sql.csv\",\n",
    "    nrows=1_000_000\n",
    ")\n",
    "categories.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be77cdfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
